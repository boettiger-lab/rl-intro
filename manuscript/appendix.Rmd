---
title: "Appendix for: Deep Reinforcement Learning for Conservation Decisions"
authors:
  - name: Marcus Lapeyrolerie
    department: Department of Environmental Science, Policy, and Management
    affiliation: University of California, Berkeley
    location: Berkeley, California
  - name: Melissa Chapman
    department: Department of Environmental Science, Policy, and Management
    affiliation: University of California, Berkeley
    location: Berkeley, California
  - name: Kari Norman
    department: Department of Environmental Science, Policy, and Management
    affiliation: University of California, Berkeley
    location: Berkeley, California
  - name: Carl Boettiger
    department: Department of Environmental Science, Policy, and Management
    affiliation: University of California, Berkeley
    location: Berkeley, California
    email: cboettig@berkeley.edu

abstract: |
  Enter the text of your abstract here.
keywords:
  - blah
  - blee
  - bloo
  - these are optional and can be removed
bibliography: references.bib
output: 
  rticles::arxiv_article:
    keep_tex: true
---

```{r knitr, include = FALSE}
knitr::opts_chunk$set(echo=TRUE, message = FALSE, warning = FALSE,
                      fig.width = 7, fig.height = 4)
ggplot2::theme_set(ggplot2::theme_bw())

scale_colour_discrete <- function(...) ggthemes::scale_colour_solarized()
scale_fill_discrete <- function(...) ggthemes::scale_fill_solarized()
pal <- ggthemes::solarized_pal()(8)
txtcolor <- "#586e75"
```


# Deep Reinforcement Learning Frameworks

At the time of writing, several major frameworks exist which provide convenient access to baseline algorithms commonly used in deep reinforcement learning.  




# Deep Reinforcement Learning in R

All though all the necessary tooling for RL is implemented in python, the R language is more familiar to most ecologists.
Fortunately, modern bindings such as the `reticulate` package [@reticulate] make it straight forward to use these tools without ever leaving the R interface.
In this appendix, we detail this "pure R" approach, as well as a "pure python" approach.

In the R based-approach, R functions take responsibility from the user for translating commands into python code before it is executed, an approach commonly referred to as meta-programming.
This still requires a local python installation, which can be installed directly from R using the command `install_miniconda()` from the `reticulate` package in R.
Alternately, users may prefer running the analysis inside a docker container.  
The Rocker Project [@rocker] provides pre-built docker containers which include the necessary R and python environments, as well as CUDA libraries required to take advantage of GPU-based acceleration on suitable architectures.
This can be particularly useful for users running ML algorithms on remote servers, as configuring R, python, and CUDA environments appropriately can be challenging.  


Clone the repository <https://github.com/boettiger-lab/rl-intro>, e.g. using the New Project->From Version Control->Git menu in RStudio.
From the project directory, we can then install all the necessary dependencies using `renv`, which helps ensure a reproducible environment of fixed versions of R packages and python modules.


```{r}
#install.packages("renv")
renv::restore()
```


Once the packages have installed, we are ready to load the necessary libraries.  Note that the `import` function from `reticulate` package acts much like the `library` command in R, though it does not attach the package function to the global namespace.  To make it more convenient to access those functions, we can assign a shorthand name.  

```{r setup, cache = FALSE, message=FALSE}
# R dependencies
library(tidyverse)
library(patchwork)
library(reticulate)

## Python dependencies loaded via R
sb3         <- import ("stable_baselines3")
gym         <- import ("gym")
gym_fishing <- import("gym_fishing")
gym_conservation <- import("gym_conservation")
```

Numerical reproducibility can be challenging in machine learning problems, particularly when using GPU-based acceleration. In addition to setting a random seed in our python environment, we can optionally disable GPU use to improve reproducibility by setting the `CUDA_VISIBLE_DEVICES` to a blank value.  

```{r}
## reproducible settings
np <- import("numpy")
seed <- 24L # integer
np$random$seed(seed)
# Set to "" to force CPU-evaluation if needing perfect reproducibility
Sys.setenv("CUDA_VISIBLE_DEVICES"="")

```

# Finding a known optimal solution using RL


## Sustainable Harvest Quotas

We begin by select an environment from `gym_fishing` by name, passing the optional parameter `sigma` to initialize the environment with multiplicative Gaussian environmental noise of sd of `0.1`.  

```{r }
## initialize the environment
env <- gym$make("fishing-v1", sigma = 0.1)
```



At the heart of each algorithm is a neural network. The type, size and depth of network, choice of activation functions and so forth, are considered "hyper-parameters" of algorithm: 
we must first make a selection for each of these hyper-parameters to define our (untrained) agent.  
Other hyper-parameters determine other aspects of the training regime: such as how the agent chooses to explore possible actions, the learning rate, or how future rewards are discounted (gamma).
Each algorithm may have different hyper-parameters. `stable-baselines3` provides default values for all hyperparameters based on the original papers that introduced the corresponding algorithms,
for instance, the A2C algorithm originally described in @A2C. 
Here, we train an agent using the A2C algorithm using default hyperparameter settings.

```{r}
a2c <- sb3$A2C('MlpPolicy', env, verbose=0L, seed = seed) # L indicates a (Long) integer, not floating point
```


Training is the main computationally intensive process, which can take anywhere from a few minutes to many days, depending on the complexity of the environment and the number of training iterations budgeted.
Therefore, we save the trained agent to disk, and only execute the training method (`learn`) if no previously saved agent is found:

```{r}
if(!file.exists("cache/a2c.zip")) {

  # Train the agent for a fixed number of timesteps
  a2c$learn(total_timesteps=300000L)
  
  # Save our trained agent for future use
  a2c$save("cache/a2c")
}
```


Note that while default hyperparameters provide a useful starting place (particularly when the environment has been suitably normalized, a best-practice we discuss above), better performance can almost always be achieved by _tuning_ the hyperparameter selection. This is discussed further below. 


We can load a trained agent and use `simulate` and `policyfn` methods provided by the `gym_fishing` environment to simulate the process for a fixed number of replicates.

```{r}
# Simulate management under the trained agent
a2c <- sb3$A2C$load("cache/a2c")
```

```{r}

## represent the initial state size in the 'rescaled' state space.
state <- env$get_state( 0.75 )

for(i in 1:100){
  
  out <- a2c$predict(state)
  action <- out[[1]]
  result <- env$step(action)
  state <- result[[1]]

}

```


```{r}
a2c_sims <- env$simulate(a2c, reps = 500L)
```


```{r}
a2c_policy <- env$policyfn(a2c, reps = 50L)
```



## TD3

We train a second agent based on the TD3 algorithm.  
This time, we will declare our choices for the hyper-parameters explicitly.
Note that some of these hyperparameters, such as the `learning_rate` and discount rate, `gamma`, are also hyper-parameters of the A2C algorithm, while others, like the `action_noise` describing how new actions are proposed, are not used in A2C.
Good choices for hyperparameters can be found through a process known as "hyper-parameter tuning", which simply uses standard function optimization approaches to vary the choice of hyper-parameters used in training to determine which maximizes the agent's performance. 
For simplicity, we show only training with specified hyper-parameters here, example scripts for tuning the algorithm to discover these values can be found in <https://github.com/boettiger-lab/rl-toolkit>, and compare to the tuning utilities found in `stable-baselines3-zoo` [@zoo].  

```{r}
# train an agent (model) on one of the environments:

policy_kwargs = list(net_arch=c(400L, 300L)) # "big"
# non-episodic action noise:
noise_std = 0.6656948079225263
n_actions = env$action_space$shape[0]
action_noise = sb3$common$noise$NormalActionNoise(
        mean=np$zeros(n_actions),
        sigma= noise_std * np$ones(n_actions)
        )


td3 = sb3$TD3( 'MlpPolicy', 
               env,  
               verbose=0L, 
               seed = seed,
               policy_kwargs = policy_kwargs,
               learning_rate = 0.0001355522450968401,
               gamma= 0.995, 
               batch_size = 128L,
               buffer_size = 10000L,
               train_freq = 128L, 
               n_episodes_rollout = -1, 
               gradient_steps = 128L,
               action_noise = action_noise
              )
```

Once again, after our agent is defined we are ready to train it. 

```{r }
if(!file.exists("cache/td3.zip")){
  
  td3$learn(total_timesteps=300000L)
  td3$save("cache/td3")
  
}
```

```{r}
# Simulate management under the trained agent
td3 <- sb3$TD3$load("cache/td3")
td3_sims <- env$simulate(td3, reps = 500L)
td3_policy <- env$policyfn(td3, reps = 50L)
```



```{r}
# Simulate under the optimal solution (given the model)
opt <- gym_fishing$models$escapement(env)
opt_sims <- env$simulate(opt, reps = 500L)
opt_policy <- env$policyfn(opt)
```

```{r}
sims_df <- bind_rows(td3_sims, a2c_sims, opt_sims, .id = "model") %>%
  mutate(model = c("TD3", "A2C", "optimal")[as.integer(model)])

policy_df <- bind_rows(td3_policy, a2c_policy, opt_policy, .id = "model") %>%
  mutate(model = c("TD3", "A2C", "optimal")[as.integer(model)])

gamma <- 1 #discount
reward_df <- sims_df %>% 
  group_by(rep, model) %>%
  mutate(cum_reward = cumsum(reward * gamma^time)) %>%
  group_by(time, model) %>%
  summarise(mean_reward = mean(cum_reward), 
            sd = sd(cum_reward), .groups = "drop")

```


```{r}
write_csv(sims_df, "figs/sims_df.csv")
write_csv(policy_df, "figs/policy_df.csv")
write_csv(reward_df, "figs/reward_df.csv")
```

```{r}
ymin <- function(x) last(x[(ntile(x, 20)==1)])
ymax <- function(x) last(x[(ntile(x, 20)==19)])

fig_sims <- 
sims_df %>% 
  group_by(time, model) %>% 
  summarise(ymin = ymin(state),
            ymax = ymax(state),
            state = mean(state), .groups = "drop") %>%
  ggplot(aes(time, state, ymin = ymin, ymax = ymax, fill=model)) + 
  geom_ribbon(alpha= 0.3) + geom_line(aes(col = model))
```


```{r}
fig_policy <- 
  policy_df %>% ggplot(aes(state, action, 
                           group=interaction(rep, model),
                           col = model)) + 
  geom_line(show.legend = FALSE) + 
  coord_cartesian(xlim = c(0, 1.3), ylim=c(0,0.9)) 
```

```{r}

fig_reward <- reward_df %>% 
  ggplot(aes(time, mean_reward)) + 
  geom_ribbon(aes(ymin = mean_reward - 2*sd, 
                  ymax = mean_reward + 2*sd, fill = model),
              alpha=0.25, show.legend = FALSE) + 
  geom_line(aes(col = model), show.legend = FALSE) + 
  ylab("reward")
```

```{r fig1, fig.cap=""}
fig_sims / ( fig_policy + fig_reward)
```




## Ecological tipping points





```{r}
env <- gym$make("conservation-v6")
```


```{r}

noise_std = 0.4805935357322933
action_noise = OrnsteinUhlenbeckActionNoise(mean = np.zeros(1),
                                            sigma = noise_std * np.ones(1))

model = TD3('MlpPolicy', 
            env, 
            verbose = 0, 
            seed = seed,
            "gamma": 0.995,
            "learning_rate":  8.315382409902049e-05,
            "batch_size": 512,
            "buffer_size": 10000,
            "train_freq": 1000,
            "gradient_steps": 1000,
            "n_episodes_rollout": -1,
            "action_noise": action_noise,
            "policy_kwargs": list("net_arch": c(64,64)))

```


```{r}
model$learn(total_timesteps = 1500000)

```



```{r}
# Simulate management under the trained agent
# See https://github.com/boettiger-lab/conservation-agents/blob//conservation/TD3-v6.py

TD3 <- sb3$TD3$load("cache/td3-conservation")
TD3_sims <- env$simulate(TD3, reps = 100L)
TD3_policy <- env$policyfn(TD3, reps = 10L)
```




```{r}
# Simulate under the steady-state solution (given the model)
K = 1.5
alpha = 0.001
opt <- gym_conservation$models$fixed_action(env, fixed_action = alpha * 100 * 2 * K )
opt_sims <- env$simulate(opt, reps = 100L)
opt_policy <- env$policyfn(opt)
```

```{r}
sims_df <- bind_rows(TD3_sims, opt_sims, .id = "model") %>%
  mutate(model = c("TD3", "steady-state")[as.integer(model)])

policy_df <- bind_rows(TD3_policy, opt_policy, .id = "model") %>%
  mutate(model = c("TD3", "steady-state")[as.integer(model)])

gamma <- 1 #discount
reward_df <- sims_df %>% 
  group_by(rep, model) %>%
  mutate(cum_reward = cumsum(reward * gamma^time)) %>%
  group_by(time, model) %>%
  summarise(mean_reward = mean(cum_reward), 
            sd = sd(cum_reward), .groups = "drop")

```


```{r}
write_csv(sims_df, "figs/tipping_sims_df.csv")
write_csv(policy_df, "figs/tipping_policy_df.csv")
write_csv(reward_df, "figs/tipping_reward_df.csv")
```


```{r}
# ensemble statistics
ymin <- function(x) last(x[(ntile(x, 20)==1)])
ymax <- function(x) last(x[(ntile(x, 20)==19)])

fig_sims <- 
  sims_df %>%
   group_by(time, model) %>% 
   summarise(ymin = ymin(state),
            ymax = ymax(state),
            state = mean(state), .groups = "drop") %>%
  ggplot(aes(time, state, ymin = ymin, ymax = ymax, fill=model)) + 
   geom_ribbon(alpha= 0.3) + geom_line(aes(col = model))
```

```{r}
# Some individual replicates, for comparison
sims_df %>%
  filter(rep < 100, time < 500) %>%
  ggplot(aes(time, state, col=model, group=interaction(model, rep))) +
  geom_line(alpha = .1, show.legend = FALSE) + facet_wrap(~model)
```

```{r}
# Some individual replicates, for comparison
is_low <- sims_df %>% filter(model == "TD3") %>% group_by(rep, model) %>% summarize(low = sum(state < .2) > 3) %>% filter(low)
sims_df %>% inner_join(is_low) %>%
  ggplot(aes(time, state,  group=interaction(model, rep))) +
  geom_line(color = pal[3], show.legend = FALSE) + facet_wrap(~rep)
```


```{r}
# Some individual replicates, for comparison
is_low <- sims_df %>% filter(model == "steady-state") %>% group_by(rep, model) %>% summarize(low = sum(state < .2) > 3) %>% filter(low) %>% head(16)
sims_df %>% inner_join(is_low) %>%
  ggplot(aes(time, state, group=interaction(model, rep))) +
  geom_line(color = pal[1], show.legend = FALSE) + facet_wrap(~rep)
```


```{r}
fig_policy <- 
  policy_df %>% ggplot(aes(state, action, 
                           group=interaction(rep, model),
                           col = model)) + 
  geom_line(show.legend = FALSE) 
```

```{r}

fig_reward <- reward_df %>% 
  ggplot(aes(time, mean_reward)) + 
  geom_ribbon(aes(ymin = mean_reward - 2*sd, 
                  ymax = mean_reward + 2*sd, fill = model),
              alpha=0.25, show.legend = FALSE) + 
  geom_line(aes(col = model), show.legend = FALSE) + 
  ylab("reward")
```

```{r fig2, fig.cap=""}
fig_sims / ( fig_policy + fig_reward)
```

----

