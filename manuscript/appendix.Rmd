---
title: Appendix for: Deep Reinforcement Learning for Conservation Decisions
authors:
  - name: Marcus Lapeyrolerie
    department: Department of Environmental Science, Policy, and Management
    affiliation: University of California, Berkeley
    location: Berkeley, California
  - name: Melissa Chapman
    department: Department of Environmental Science, Policy, and Management
    affiliation: University of California, Berkeley
    location: Berkeley, California
  - name: Kari Norman
    department: Department of Environmental Science, Policy, and Management
    affiliation: University of California, Berkeley
    location: Berkeley, California
  - name: Carl Boettiger
    department: Department of Environmental Science, Policy, and Management
    affiliation: University of California, Berkeley
    location: Berkeley, California
    email: cboettig@berkeley.edu

abstract: |
  Enter the text of your abstract here.
keywords:
  - blah
  - blee
  - bloo
  - these are optional and can be removed
bibliography: references.bib
output: 
  rticles::arxiv_article:
    keep_tex: true
---

```{r knitr, include = FALSE}
knitr::opts_chunk$set(echo=FALSE)
```

```{r setup, cache = FALSE}
library(tidyverse)
library(patchwork)
library(reticulate)

np <- import("numpy")
np$random$seed(42L)

# force CPU-evaluation for reproducibility
Sys.setenv("CUDA_VISIBLE_DEVICES"="")

```

## Finding a known optimal solution using RL

fishing gym?

```{r }
## Python dependencies
gym         <- import ("gym")
gym_fishing <- import("gym_fishing")
sb3         <- import ("stable_baselines3")

## initialize the environment
env <- gym$make("fishing-v1", sigma = 0.1)
```

```{r eval=!file.exists("cache/a2c.zip")}
# train an agent (model) on one of the environments:


#Trial 15 finished with value: 7.6994757652282715 and parameters: 
hyper = list('gamma'= 0.995, 'lr'= 0.0001355522450968401, 'batch_size'= 128L, 
         'buffer_size'= 10000L, 'episodic'= FALSE, 'train_freq'= 128L, 
         'noise_type'= 'normal', 'noise_std'= 0.6656948079225263, 
         'net_arch'= 'big')
policy_kwargs = list(net_arch=c(400L, 300L)) # big


#non-episodic:
hyper['gradient_steps'] = hyper['train_freq']
hyper['n_episodes_rollout'] = -1
n_actions = env$action_space$shape[0]
hyper$action_noise = sb3$common$noise$NormalActionNoise(
        mean=np$zeros(n_actions),
        sigma= hyper[['noise_std']] * np$ones(n_actions)
        )


td3 = sb3$TD3('MlpPolicy', env,  verbose=0L, seed = 42L,
            gamma = hyper[['gamma']],
            learning_rate = hyper[['lr']],
            batch_size = hyper[['batch_size']],            
            buffer_size = hyper[['buffer_size']],
            action_noise = hyper[['action_noise']],
            train_freq = hyper[['train_freq']],
            gradient_steps = hyper[['train_freq']],
            n_episodes_rollout = hyper[['n_episodes_rollout']],
            policy_kwargs=policy_kwargs)


td3$learn(total_timesteps=300000L)
td3$save("cache/td3")
```

```{r}
# Simulate management under the trained agent
#td3 <- sb3$A2C$load("cache/td3") ## fixme, fails to load
sims <- env$simulate(td3, reps = 500L)
policy <- env$policyfn(td3, reps = 50L)
```

```{r}
# Simulate under the optimal solution (given the model)
opt <- gym_fishing$models$escapement(env)
opt_sims <- env$simulate(opt, reps = 500L)
opt_policy <- env$policyfn(opt) %>% rename(optimal = action)
```

```{r}
sims_df <- bind_rows(sims, opt_sims, .id = "model") %>%
  mutate(model = c("TD3", "optimal")[as.integer(model)])

policy <- left_join(policy, opt_policy)
```


```{r}
gamma <- 1 #discount
fig_reward <- sims_df %>% 
  group_by(rep, model) %>%
  mutate(cum_reward = cumsum(reward * gamma^time)) %>%
  group_by(time, model) %>%
  summarise(mean_reward = mean(cum_reward), 
            sd = sd(cum_reward), .groups = "drop") %>% 
  ggplot(aes(time, mean_reward)) + 
  geom_ribbon(aes(ymin = mean_reward - 2*sd, 
                  ymax = mean_reward + 2*sd, fill = model),
              alpha=0.25, show.legend = FALSE) + 
  geom_line(aes(col = model)) + ylab("mean cumulative reward")
```

```{r}
ymin <- function(x) last(x[(ntile(x, 20)==1)])
ymax <- function(x) last(x[(ntile(x, 20)==19)])

fig_sims <- 
sims_df %>% 
  group_by(time, model) %>% 
  summarise(ymin = ymin(state),
            ymax = ymax(state),
            state = mean(state), .groups = "drop") %>%
  ggplot(aes(time, state, ymin = ymin, ymax = ymax, fill=model)) + 
  geom_ribbon(alpha= 0.3) + geom_line(aes(col = model))
```


```{r}
fig_policy <- 
  policy %>% ggplot(aes(state, state - action, group=rep)) + 
  geom_point(alpha=.05) + geom_line(aes(y = state - optimal), col = "blue") + 
  coord_cartesian(xlim = c(0, 1.5), ylim=c(0,0.9))
```

```{r fig1, fig.cap=""}
fig_sims / ( fig_policy + fig_reward)
```
```{r}
```
