---
title: "Appendix for: Deep Reinforcement Learning for Conservation Decisions"
authors:
  - name: Marcus Lapeyrolerie
    department: Department of Environmental Science, Policy, and Management
    affiliation: University of California, Berkeley
    location: Berkeley, California
  - name: Melissa Chapman
    department: Department of Environmental Science, Policy, and Management
    affiliation: University of California, Berkeley
    location: Berkeley, California
  - name: Kari Norman
    department: Department of Environmental Science, Policy, and Management
    affiliation: University of California, Berkeley
    location: Berkeley, California
  - name: Carl Boettiger
    department: Department of Environmental Science, Policy, and Management
    affiliation: University of California, Berkeley
    location: Berkeley, California
    email: cboettig@berkeley.edu


keywords:
  - blah
  - blee
  - bloo
  - these are optional and can be removed
bibliography: references.bib

header-includes:
  - \usepackage{amsmath}
  - \usepackage{amssymb}

output: 
  rticles::arxiv_article:
    keep_tex: true
---

```{r knitr, include = FALSE}
knitr::opts_chunk$set(echo=TRUE, message = FALSE, warning = FALSE,
                      fig.width = 7, fig.height = 4, cache = FALSE)
ggplot2::theme_set(ggplot2::theme_bw())

scale_colour_discrete = function(...) ggthemes::scale_colour_solarized()
scale_fill_discrete = function(...) ggthemes::scale_fill_solarized()
pal = ggthemes::solarized_pal()(8)
txtcolor = "#586e75"
```

# Building a simulation environment `gym`

[@openai_gym]


# Deep Reinforcement Learning Frameworks

At the time of writing, several major frameworks exist which provide convenient access to baseline algorithms commonly used in deep reinforcement learning.
While we do not seek to provide a comprehensive review of available frameworks, some familiarity with current frameworks can be helpful.
Like many machine learning libraries, these frameworks are themselves each built around one of two popular machine learning libraries, PyTorch [@torch] or Tensorflow [@tensorflow], and all are based in Python.
Existing frameworks we evaluated include Keras-RL [@keras-rl], Tensorflow Agents [@tensorflow-agents],  OpenAI Spinning Up [@spinningup] OpenAI Baselines, Stable-Baselines2 [@sb2], and Stable-Baselines3 [@sb3]. 
Keras-RL saw widespread early adoption, but is built on Tensorflow version 1.x. It is incompatible with Tensorflow 2.x and not actively maintained.
Tensorflow Agents is developed by the Tensorflow team, a recent and actively developed framework with support for both Tensorflow 1.x and 2.x and good support for low-level customization of RL algorithms.
However, higher-level interfaces and high-level documentation are still relatively limited.
OpenAI's SpinningUp is an education-targeted framework useful for developers wanting to become more familiar with the internal methods of RL algorithms.  
OpenAI's Baselines is primarily Tensorflow 1.x-based implementation of many recently published RL algorithms.
Researchers at Ensta Paris Tech first created Stable Baselines as a fork of the OpenAI implementation, rigorously addressing numerous issues in documentation, testing, and coding style that have helped make their fork see even greater adoption. 
Stable-Baselines3 is the most recent version (Feb 2021), a ground-up rewrite which switches to a PyTorch-based implementation and further strengthens internal checks such as static types.
The examples here all use the Stable-Baselines3 framework, though researchers in this area should expect frameworks to continue to emerge and evolve. 
Grand challenge problems will likely require significant development beyond the current algorithms and capabilities available in existing frameworks.


# Deep Reinforcement Learning in R

Although all the necessary tooling for RL is implemented in Python, the R language is more familiar to most ecologists.
Fortunately, modern bindings such as the `reticulate` package [@reticulate] make it straightforward to use these tools without ever leaving the R interface.
In this appendix, we detail this "pure R" approach, as well as a "pure Python" approach.

In the R based-approach, R functions take responsibility from the user for translating commands into Python code before it is executed, an approach commonly referred to as meta-programming.
This still requires a local Python installation, which can be installed directly from R using the command `install_miniconda()` from the `reticulate` package in R.
Alternately, users may prefer running the analysis inside a docker container.  
The Rocker Project [@rocker] provides pre-built docker containers which include the necessary R and Python environments, as well as CUDA libraries required to take advantage of GPU-based acceleration on suitable architectures.
This can be particularly useful for users running ML algorithms on remote servers, as configuring R, Python, and CUDA environments appropriately can be challenging.  


Clone the repository <https://github.com/boettiger-lab/rl-intro>, e.g. using the New Project->From Version Control->Git menu in RStudio.
From the project directory, we can then install all the necessary dependencies using `renv`, which helps ensure a reproducible environment of fixed versions of R packages and Python modules.


```{r}
#install.packages("renv")
renv::restore()
```


Once the packages have installed, we are ready to load the necessary libraries.  Note that the `import` function from `reticulate` package acts much like the `library` command in R, though it does not attach the package function to the global namespace.  To make it more convenient to access those functions, we can assign a shorthand name.  

```{r setup, cache = FALSE, message=FALSE}
# R dependencies
library(tidyverse)
library(patchwork)
library(reticulate)

## Python dependencies loaded via R
sb3         = import ("stable_baselines3")
gym         = import ("gym")
gym_fishing = import("gym_fishing")
gym_conservation = import("gym_conservation")
```

Numerical reproducibility can be challenging in machine learning problems, particularly when using GPU-based acceleration. In addition to setting a random seed in our Python environment, we can optionally disable GPU use to improve reproducibility by setting the `CUDA_VISIBLE_DEVICES` to a blank value.  

```{r}
## reproducible settings
np = import("numpy")
seed = 24L # integer
np$random$seed(seed)

# Optionally set to "" to force CPU-evaluation if needing perfect reproducibility
Sys.setenv("CUDA_VISIBLE_DEVICES"="")
set.seed(seed)
```

<!-- is this too verbose? --> 
The above code also illustrates a few conventions which may be helpful to bear in mind when using the `reticulate` interface to interact with Python from R: 
it is often necessary for integer values to be explicitly typed as integers by adding a trailing `L` (corresponding to the primitive C type `long` integer).
Python is also more strongly object-oriented than many R packages, where "methods" of an "object" are accessed with the list-subset operator `$` in R (equivalent to the use of `.` in Python).
Lastly, while R can use `<-` or `=` for assignment, Python uses only `=`. For simplicity we will stick with the latter.  
With few exceptions, the R code shown here can be re-written as python code by dropping the `L` and replacing `$` with `.`, see stand-alone python code in the `python/` subdirectory of the repository.


# Finding a known optimal solution using RL


## Sustainable Harvest Quotas

We begin by selecting an environment from `gym_fishing` by name, passing the optional parameter `sigma` to initialize the environment with multiplicative Gaussian environmental noise of sd of `0.1`.  

```{r }
## initialize the environment
env = gym$make("fishing-v1", sigma = 0.1)
```

<!-- Kari: the role neural networks play in RL is not very well explained in the paper, more detail there would probably make this part more intuitive --> 
At the heart of each deep RL algorithm is a neural network or a collection of neural networks. The RL agent uses these networks to approximate a policy function and/or a value function. The policy function is the agent's mapping of states to actions -- i.e. the agent's strategy. Value functions attempt to evaluate the goodness of a state or state-action pair towards achieving the RL objective of maximizing cumulative rewards. 
Confusingly, the parameters of a RL algorithm that detail the overall learning process are called _hyper-parameters_ while the parameters of a RL algorithm that are learned during training are called parameters. For example, the number of layers in a neural network is a hyper-parameter but a weight in the neural net is referred to as a parameter.
Before training an agent, we must first specify the hyper-parameters; the algorithm's parameters are often randomly initialized.
Each algorithm may have different hyper-parameters. `stable-baselines3` provides default values for all hyper-parameters based on the original papers that introduced the corresponding algorithms,
for instance, the A2C algorithm originally described in @A2C.
Here, we train an agent using the A2C algorithm using default hyper-parameter settings with a multi-layer perceptron policy and value network composed of two 64-neuron layers.

```{r}
a2c = sb3$A2C('MlpPolicy', env, verbose=0L, seed = seed) # L indicates a (Long) integer, not floating point
```


Training is the main computationally intensive process, which can take anywhere from a few minutes to many days, depending on the complexity of the environment, the neural network architecture and the number of training iterations budgeted.
Therefore, we save the trained agent to disk, and only execute the training method (`learn`) if no previously saved agent is found:

```{r}
if(!file.exists("cache/a2c.zip")) {

  # Train the agent for a fixed number of timesteps
  a2c$learn(total_timesteps=300000L)
  
  # Save our trained agent for future use
  a2c$save("cache/a2c")
}
```


Note that while default hyper-parameters provide a useful starting place (particularly when the environment has been suitably normalized, a best-practice we discuss above), better performance can almost always be achieved by _tuning_ the hyper-parameter selection. This is discussed further below. 
Having saved our trained agent to disk (`cache/a2c.zip`), we can then re-load this agent for evaluation or to continue training.  Note that a copy of the trained agents are included in the corresponding GitHub repository.

```{r}
# Simulate management under the trained agent
a2c = sb3$A2C$load("cache/a2c")
```


We can supply an observation to our trained agent and it will return it's proposed action using the `predict` method. 
This is all we need to evaluate or employ the agent on a decision-making task.
Recall that state space and action space in the fishing gym have been re-scaled to a (-1, 1) interval.
Note that this is equivalent to a choice of appropriate units -- we can re-scale the interval without loss of generality.
This is often an important step in the design of an RL environment to facilitate successful training.
Following the `gym` standard, the core methods such as `predict` and `step` operate on the re-scaled units,
so it is necessary to first transform the original units into this re-scaled state space.
For example, if we wish to start a simulation with a stock size of 0.75, we can use the helper
method `get_state()`, to determine the corresponding value in the re-scaled state space.  
Unlike `predict` and `step`, `get_state()` is not a standard method of all gym environments -- typically
a user must first inspect the state space of an environment and choose themselves how to re-scale their
problem into that state space.  

```{r}

## represent the initial state size in the 'rescaled' state space.
state = env$get_state( 0.75 )
state
```


With an initial state in hand, we are ready to simulate management using our agent.
The iteration is simple: we use the agent to predict what action we should take given the current state. Then, we take said action and examine the result to determine the future state. 
Because these methods return additional information as well, a little extra sub-setting is required in R:

```{r}
for(i in 1:10){
  
  out = a2c$predict(state)
  action = out[[1]]
  result = env$step(action)
  state = result[[1]]

}

```

For convenience, `gym_fishing` defines the helper routine `simulate` to perform the above iteration `reps` number of times. The `simulate` method returns the state, action, and reward resulting from each time step of each replicate:


```{r}
a2c_sims = env$simulate(a2c, reps = 500L)
```


Because the agent typically attempts to retain the fish population near a specific value, simulations with well-trained agents will usually not explore the full range of possible states.
To get a better idea of how the agent behaves across the full state space, it is common to examine the policy function: to do this, we'll plot the action taken at every possible state.
Some agents are non-deterministic, so we may want to use replicate draws at each state to get a better picture of the agent's behavior.
Note that the `policyfn` method is another custom method and not part of the `gym` standard. The use of `policyfn` will not make sense for agents that consider the history of many previous states in selecting their action, such as agents which utilize recurrent neural network architectures like LSTMs [@LSTM].

```{r}
a2c_policy = env$policyfn(a2c, reps = 50L)
```



## TD3

We train a second agent based on the TD3 algorithm. 
This illustrates some of the salient differences between the different algorithms used in reinforcement learning.
Bear in mind that the best algorithm for any given environment will depend on the details of the environment as well
as the length of training and the selection of hyper-parameters.
This time, we will declare our choices for the hyper-parameters explicitly.
Note that some of these hyper-parameters, such as the `learning_rate` and discount rate, `gamma`, are also hyper-parameters of the A2C algorithm, while others, like the `action_noise` describing how new actions are proposed, are not used in A2C.
Good choices for hyper-parameters can be found through a process known as "hyper-parameter tuning", which uses standard function optimization approaches to vary the choice of hyper-parameters to determine which hyper-parameters maximize the agent's performance. 
For simplicity, we show only training with specified hyper-parameters here, example scripts for tuning the algorithm to discover these values can be found in <https://github.com/boettiger-lab/rl-toolkit>, and compare to the tuning utilities found in `stable-baselines3-zoo` [@zoo].  

```{r}
# train an agent (model) on one of the environments:

policy_kwargs = list(net_arch=c(400L, 300L)) # "big"
# non-episodic action noise:
noise_std = 0.6656948079225263
n_actions = env$action_space$shape[0]
action_noise = sb3$common$noise$NormalActionNoise(
        mean=np$zeros(n_actions),
        sigma= noise_std * np$ones(n_actions)
        )


td3 = sb3$TD3( 'MlpPolicy', 
               env,  
               verbose=0L, 
               seed = seed,
               policy_kwargs = policy_kwargs,
               learning_rate = 0.0001355522450968401,
               gamma= 0.995, 
               batch_size = 128L,
               buffer_size = 10000L,
               train_freq = 128L, 
               n_episodes_rollout = -1, 
               gradient_steps = 128L,
               action_noise = action_noise
              )
```

Once again, after our agent is defined we are ready to train it:

```{r }
if(!file.exists("cache/td3.zip")){
  
  td3$learn(total_timesteps=300000L)
  td3$save("cache/td3")
  
}
```

With our trained agent, we can simulate replicate scenarios or scan across states to determine the implicit policy function:

```{r}
# Simulate management under the trained agent
td3 = sb3$TD3$load("cache/td3")
td3_sims = env$simulate(td3, reps = 500L)
td3_policy = env$policyfn(td3, reps = 50L)
```


We compare the performance of these two RL agents trained using the algorithms A2C and TD3 respectively to the known optimal policy for this particular environment.  
Recall that under the assumptions of the simple model used in the `fishing-v1` environment, we can determine the optimal harvest policy analytically if the model and parameters are known precisely [@Reed1979].
The optimal strategy is a policy of 'constant escapement', designed to keep the remaining stock size (the population that 'escapes' fishing harvest) at the biomass corresponding to a maximum growth rate, i.e. at $B = K/2$ in this model.
`gym_fishing` defines a collection of non-RL agents in the `models` submodule, including the a human agent that merely asks to enter their desired quota manually. 
The `escapement` model implements the provably optimal constant escapement rule.
A third model, `msy`, implements a policy based on "Maximum Sustainable Yield" policy [@Schaefer1954], which is actually more commonly used as a basis for management than constant escapement, despite only being optimal at the steady state under deterministic dynamics.  

```{r}
# Simulate under the optimal solution (given the model)
opt = gym_fishing$models$escapement(env)
opt_sims = env$simulate(opt, reps = 500L)
opt_policy = env$policyfn(opt)
```


Having performed simulations under each method, we gather the results under each management strategy together into a single data frame for simulations (`sims_df`), the policy function (`policy_df`) and the cumulative reward, used to make the plots in the corresponding panels.  
We preserve a copy of each data frames in local `.csv` text files for maximum cross-platform compatibility.

```{r}
sims_df = bind_rows(td3_sims, a2c_sims, opt_sims, .id = "model") %>%
  mutate(model = c("TD3", "A2C", "optimal")[as.integer(model)])

policy_df = bind_rows(td3_policy, a2c_policy, opt_policy, .id = "model") %>%
  mutate(model = c("TD3", "A2C", "optimal")[as.integer(model)])

gamma = 1 #discount
reward_df = sims_df %>% 
  group_by(rep, model) %>%
  mutate(cum_reward = cumsum(reward * gamma^time)) %>%
  group_by(time, model) %>%
  summarise(mean_reward = mean(cum_reward), 
            sd = sd(cum_reward), .groups = "drop")


write_csv(sims_df, "figs/sims_df.csv")
write_csv(policy_df, "figs/policy_df.csv")
write_csv(reward_df, "figs/reward_df.csv")
```


<!-- Remaining is just plotting code, repeated in the main .Rmd.  Should we omit it from the appendix? -->
Standard R methods are used to plot the results summarized across the replicates:

```{r}
ymin = function(x) last(x[(ntile(x, 20)==1)])
ymax = function(x) last(x[(ntile(x, 20)==19)])

fig_sims = 
sims_df %>% 
  group_by(time, model) %>% 
  summarise(ymin = ymin(state),
            ymax = ymax(state),
            state = mean(state), .groups = "drop") %>%
  ggplot(aes(time, state, ymin = ymin, ymax = ymax, fill=model)) + 
  geom_ribbon(alpha= 0.3) + geom_line(aes(col = model))
```


```{r}
fig_policy = 
  policy_df %>% ggplot(aes(state, action, 
                           group=interaction(rep, model),
                           col = model)) + 
  geom_line(show.legend = FALSE) + 
  coord_cartesian(xlim = c(0, 1.3), ylim=c(0,0.9)) 
```

```{r}

fig_reward = reward_df %>% 
  ggplot(aes(time, mean_reward)) + 
  geom_ribbon(aes(ymin = mean_reward - 2*sd, 
                  ymax = mean_reward + 2*sd, fill = model),
              alpha=0.25, show.legend = FALSE) + 
  geom_line(aes(col = model), show.legend = FALSE) + 
  ylab("reward")
```

## Manuscript Figure 1

<!-- Not sure if it's helpful to duplicate figures from the paper in the appendix...-->

```{r fig1}
fig_sims / ( fig_policy + fig_reward)
```


## Applying an RL agent to real data


We use historical data from Argentine Hake to illustrate how an RL agent might be applied in practice.

Historical biomass and catch data for Argentine Hake can be found in the R.A. Myers Legacy Stock Assessment Database [@ramlegacy].
We will use the agent we have just trained using the `gym_fishing` simulator above, 

```{r}
# Simulate management under the trained agent
env = gym$make("fishing-v1", r = 1.0379274,	K = 1.197693, sigma = 0.1121662)
td3 = sb3$TD3$load("cache/td3")

```



```{r}
hake = read_csv("../data/hake.csv")
x0 = hake %>%
 filter(year == min(year)) %>%
  pull(scaled_biomass)
Tmax = 15
years =1986:2000

td3_state = td3_action = numeric(Tmax)
td3_state[1] = x0
td3_action[1] = NA

```


Our hindcast considers the scenario of managing catch quotas using TD3, beginning in 1986. 
We compare both the anticipated harvest and resulting stock biomass to the historically recorded harvest and biomass.
This shows that quotas set by TD3 would have been lower than historical values throughout the late 80s and early 90s, which
saw a sharp decline in estimated biomass of the Argentine Hake.
For the same or higher estimated stock sizes from the historical record, we see that TD3 would recommend a lower quota.
We also see that under TD3 the stock recovers sufficiently to predict higher quotas by the second half of the 90s than were
being set in the now-depleted stock.  
Whether the TD3 quotas would have truly been sufficient to permit recovery in stock sizes shown in the simulation clearly depends on the accuracy of the underlying simulation.
However, the fact that TD3 would set a lower quota than the historical management given the same or even higher estimate of stock size is independent of the Hake simulation.
Combined with the observed historical declines, this provides some evidence that the RL agent could have decreased or avoided the over-harvesting in this case. 


```{r}
env$reset()
## represent the initial state size in the 'rescaled' state space.
state = env$get_state( x0 )

for(i in 2:Tmax){
  
  # RL-recommended harvest action:
  out = td3$predict(state)
  action = out[[1]]
  
  # Record state and resulting action
  td3_state[i] = env$get_fish_population(state)
  td3_action[i] = env$get_quota(action)
  
  # Implement RL-recommended harvest. 
  result = env$step(action)
  state = result[[1]]
  
  #state = env$get_state(hake$scaled_biomass[i+1])
}

df = bind_rows(
  tibble(year = years, state = td3_state, action = td3_action, model = "TD3"),
  tibble(year = years, state = hake$scaled_biomass, action = hake$scaled_catch, model = "historical"))

df %>% ggplot(aes(year, state, col=model)) + geom_line() + geom_point(aes(year, action, col=model))

```




# Ecological tipping points

Our second example utilizes our `gym_conservation` to provide the necessary environment to simulate an ecosystem approaching a tipping point.

## Tipping point model

The tipping point model is based on the consumer-resource model of @May1977, which creates alternative stable states, 
which we subject to log-normal environmental noise:

\[
\mu_{t} = X_t + X_t r \left(1 - \frac{X_t}{K} \right) - \frac{a_t X_t^q}{X_t^q + b^q}
\]
\[
X_{t+1} \sim \text{lognormal}(\mu_t, \sigma)
\]

where we take $r=0.7$, $K=1.2$, $q=3$, $b=0.15$, $a_0 = 0.19$ and $\sigma = 0.2$ 
Slow change over time in the parameter $a_t$ represents a process of environmental degradation, modeled as a constant increment $a_{t+1} = a_t + \alpha$, where we will take $\alpha = 0.001$.
This model supports the dynamics of a fold bifurcation, widely used to model critical transitions in both theory and empirical manipulation in systems from microbes [@Dai2012] to lakes [@Carptenter2011] to the planet biosphere [@Barnosky2014].  



```{r message=FALSE, fig.cap="Bifurcation diagram for tipping point scenario. The ecosystem begins in the desirable 'high' state under an evironmental parameter (e.g. global mean temperature, arbitrary units) of 0.19.  In the absence of conservation action, the environment worsens (e.g. rising mean temperature) as the parameter increases.  This results in only a slow degredation of the stable state, until the parameter crosses the tipping point threshold at about 0.215, where the upper stable branch is anihilated in a fold bifurcation and the system rapidly transitions to lower stable branch, around state of 0.1.   Recovery to the upper branch requires a much greater conservation investment, reducing the parameter all the way to 0.165 where the reverse bifurcation will carry it back to the upper stable branch."}
bifur_df = read_csv("figs/bifur.csv", col_types = "dccd")
bifur_df %>%
  ggplot(aes(parameter, state, lty=equilibrium, group = group)) + 
  geom_line()

```


We assume the benefit provided by the ecosystem state is assumed to be directly proportional to the state itself, $b x_t$.  
We further assume that each year the manager has the option to slow or reverse the environmental degradation by taking action $A_t$, such that under management, the resulting environment in the next time step is given by

\[
a_{t+1} = a_t + \alpha - A_t
\]

We assume the cost associated with that action to be proportional to the square of the action, such that large actions are proportionally more costly than small ones.
Consequently, the utility at time $t$ is given by the sum of costs and benefits:

\[U(X_t, A_t) = b X_t - c A_t ^2\]

where we will take \(b = 1\) and \(c=1\).  
Our implementation in `gym_conservation` allows the user to consider alternative parameter choices, alternative models for the ecological dynamics, and alternate types of actions, such as manipulating the ecosystem state directly rather than manipulating the environmental parameter. 
For some such scenarios the optimal solution is known or can be determined by stochastic dynamic programming, while for others, including the scenario of focus here, the optimal solution is unknown.


For simplicity of illustration, we will train only a TD3-based agent on this scenario.


```{r}
env = gym$make("conservation-v6")
```


```{r}
noise_std = 0.4805935357322933

OU = sb3$common$noise$OrnsteinUhlenbeckActionNoise
action_noise = OU(mean = np$zeros(1L),  sigma = noise_std * np$ones(1L))

model = sb3$TD3('MlpPolicy', 
            env, 
            verbose = 0, 
            seed = 42L,
            "gamma"= 0.995,
            "learning_rate"=  8.315382409902049e-05,
            "batch_size"= 512L,
            "buffer_size"= 10000L,
            "train_freq"= 1000L,
            "gradient_steps"= 1000L,
            "n_episodes_rollout"= -1L,
            "action_noise"= action_noise,
            "policy_kwargs"= list("net_arch"= c(64L,64L)))

```


```{r}
if(!file.exists("cache/td3-conservation.zip")){

  model$learn(total_timesteps=3000000L)
  model$save("cache/td3-conservation")
  
}

```


```{r}
# Simulate management under the trained agent
# See https://github.com/boettiger-lab/conservation-agents/blob//conservation/TD3-v6.py

model = sb3$TD3$load("cache/td3-conservation")
TD3_sims = env$simulate(model, reps = 100L)
TD3_policy = env$policyfn(model, reps = 10L)
```


In general, the optimal solution depends on the ecological dynamics, the benefit of the ecosystem services and the costs associated with a management response.
Because the tipping point problem is non-autonomous, we cannot solve for the optimal policy even given the model and objective (utility) function using Markov Decision Process methods.
However, a simple heuristic solution provides a reasonable starting point for comparison.


As discussed in the main text, we have no existing optimal solution to the tipping point problem, and so rely on a common heuristic strategy instead:
select a fixed level of conservation investment that is sufficient to counter-balance any further side towards the tipping point, preserving it in it's current state.
This is implemented using the `fixed_action` method provided in our `gym_conservation` module, which also implements other heursitic models, including a human agent which requires interactive input to select the action each year.

```{r}
# Simulate under the steady-state solution (given the model)
K = 1.5
alpha = 0.001
opt = gym_conservation$models$fixed_action(env, fixed_action = alpha * 100 * 2 * K )
opt_sims = env$simulate(opt, reps = 100L)
opt_policy = env$policyfn(opt)
```


We gather together the results under the RL agent and steady-state policy as before,

```{r}
sims_df = bind_rows(TD3_sims, opt_sims, .id = "model") %>%
  mutate(model = c("TD3", "steady-state")[as.integer(model)])

policy_df = bind_rows(TD3_policy, opt_policy, .id = "model") %>%
  mutate(model = c("TD3", "steady-state")[as.integer(model)])

gamma = 1 #discount
reward_df = sims_df %>% 
  group_by(rep, model) %>%
  mutate(cum_reward = cumsum(reward * gamma^time)) %>%
  group_by(time, model) %>%
  summarise(mean_reward = mean(cum_reward), 
            sd = sd(cum_reward), .groups = "drop")

```

The resulting three data frames contain the necessary data for each of the subplots in figure 3 of the main text.


```{r}
write_csv(sims_df, "figs/tipping_sims_df.csv")
write_csv(policy_df, "figs/tipping_policy_df.csv")
write_csv(reward_df, "figs/tipping_reward_df.csv")
```


<!-- Remaining is just plotting code, repeated in the main .Rmd.  Should we omit it from the appendix? -->

```{r}
# ensemble statistics
ymin = function(x) last(x[(ntile(x, 20)==1)])
ymax = function(x) last(x[(ntile(x, 20)==19)])

fig_sims = 
  sims_df %>%
   group_by(time, model) %>% 
   summarise(ymin = ymin(state),
            ymax = ymax(state),
            state = mean(state), .groups = "drop") %>%
  ggplot(aes(time, state, ymin = ymin, ymax = ymax, fill=model)) + 
   geom_ribbon(alpha= 0.3) + geom_line(aes(col = model))
```


```{r}
fig_policy = 
  policy_df %>% ggplot(aes(state, action, 
                           group=interaction(rep, model),
                           col = model)) + 
  geom_line(show.legend = FALSE) 
```

```{r}

fig_reward = reward_df %>% 
  ggplot(aes(time, mean_reward)) + 
  geom_ribbon(aes(ymin = mean_reward - 2*sd, 
                  ymax = mean_reward + 2*sd, fill = model),
              alpha=0.25, show.legend = FALSE) + 
  geom_line(aes(col = model), show.legend = FALSE) + 
  ylab("reward")
```

### Manuscript Figure 3

```{r fig3}
fig_sims / ( fig_policy + fig_reward)
```


Because it is difficult to get a feel for the dynamics of individual replicate simulations from ensemble statistics, we select a few example trajectories to examine directly. 
Of the 100 replicate simulations, we pick 4 examples that dip below a state value of 0.2 for over 15 consecutive timesteps, indicating a transition into the lower basin of attraction. 
Comparing the dynamics under the rule of thumb steady-state strategy to that of the RL-trained agent, it is clear that the RL agent does a better job at both avoiding tipping points and promoting the recovery of those selected trajectories that cross into the lower attractor. 



```{r}
# Some individual replicates, for comparison

## First 4 of the TD3 reps falling below .2 for more than 15 steps
is_low = sims_df %>% 
  filter(model == "TD3") %>% 
  group_by(rep, model) %>% 
  summarize(low = sum(state < .2) > 10) %>% 
  filter(low) %>% head(6)

## First 4 such cases:
sims_df %>% inner_join(is_low) %>%
  ggplot(aes(time, state,  group=interaction(model, rep))) +
  geom_line(color = pal[3], show.legend = FALSE) + facet_wrap(~rep)
```


```{r}
# Some individual replicates, for comparison
is_low = sims_df %>% filter(model == "steady-state") %>%
  group_by(rep, model) %>% summarize(low = sum(state < .2) > 10) %>% 
  filter(low) %>% head(6)
sims_df %>% inner_join(is_low) %>%
  ggplot(aes(time, state, group=interaction(model, rep))) +
  geom_line(color = pal[1], show.legend = FALSE) + facet_wrap(~rep)
```

# RL

<!-- Not tightened up yet to make sense in the appendix, just migrating more nuts and bolts RL stuff
     from manuscript here for sake of word count
-->

<!-- These 2 paragraphs might make it back to the manuscript but going to keep them here for now -->
Among model-free algorithms, there is divergence around what functions the agent is attempting to learn in order to achieve the RL objective.
Generally, model-free agents are either learning a policy function, a value function or both.
For context, *value functions* are proxies for how high of a cumulative reward an agent can expect to receive from a given state or state-action pair.
If an agent knows the value function, then the agent can find the optimal action at any state by selecting the action that will maximize the value function in expectation.
The class of algorithms that exclusively try to learn a value function are called *value learning* methods.
*Policy gradient* algorithms exclusively learn the policy.
And, lastly, *actor-critic* algorithms attempt to learn both a policy function and a value function, whereby the value function, which is learned separately from the policy, is used to inform the agent on the goodness of a selected action.
There is a handful of advantages and disadvantages for each of these approaches that we will not go in depth on -- for more, see [@CITE]; yet, the main point of divergence comes from the bias-variance trade-off [@CITE].
Generally, policy gradient algorithms tend to struggle with high variance, value-based learning tends to be biased while actor-critic methods balance these two concerns [@CITE; @CITE; @CITE].

Neural networks become useful in RL when the environment has a large state-action space, which happens frequently with realistic decision-making problems. 
Classic RL algorithms, like policy iteration, value iteration and TD-learning [@suttonbarto], fail when the state-action space is large, e.g. when the possible states and actions can not be represented in a tractable table [@CITE].
Yet, over the last 10 years, researchers have shown that neural network-based RL algorithms can perform well on previously unsolvable environments.
Neural networks have been widely useful in reinforcement learning because neural nets have the property of being general function approximators [@CITE].
Whenever there is a need for an agent to approximate some function, say a policy function, value function or a transition function, neural networks can be used in this capacity.
While there are other function approximators that can be used in RL, e.g. Gaussian processes, neural networks have excelled in this role because of their ability to learn complex, non-linear, high dimensional functions and their ability to adapt given new information [@CITE].
We will not discuss in detail how neural networks work -- see [@CITE] for further background --, but the general process for how neural networks are used in RL is that at each learning step, the agent, say a policy gradient-based agent, will adjust the parameters in its policy neural network so that according to past experiences, the agent will be more likely to engage in highly rewarding behavior in the future. 
This same process occurs in value learning and actor-critic-based deep RL algorithms, except these agents are adjusting either a value network or both a value and policy network respectively.

## Policy Gradients
The most straightforward way to optimize the RL objective is through a policy search, whereby the agent continually updates a policy parameterized by $\theta$, $\pi_\theta$ -- in deep RL, this parameterized policy will be a neural network.
There are both gradient-free and gradient-based policy search methods that have been successful in practice [@CITE; @CITE]; but for the sake of brevity, we will focus on gradient-based methods, which are more commonly encountered. 
In policy gradient methods, algorithms are performing gradient ascent on the expected return, $J(\pi_\theta)$, to find the optimal policy parameters. 
At each gradient step, the following update is performed,

$$
  \theta_{t+1} = \theta_t + \alpha \nabla_\theta J(\pi_\theta).
$$

Supposing that the agent has interacted with the environment and has collected some trajectories denoted by, $\tau_i$, then $\nabla J(\pi_\theta)$ can be estimated as:

$$
  \nabla_\theta J(\pi_\theta) \approx \sum_{i,t} \nabla_\theta \log{\pi_\theta (a_{i,t} \mid s_{i,t})} \Big( \sum_{t'=t}^H \gamma^{t'-t} r(s_{i, t'}, a_{i, t'}) \Big).
$$

See the appendix for how this is derived. 
The intuition behind this approximation is exactly what we want: the agent will increase the probability of repeating the actions for the inputted states proportional to the size of the return. 
In practice, however, the empirical return tends to have high variance, resulting in very noisy gradients that impedes learning [@CITE].
To avoid this issue, most policy gradient algorithms replace the empirical return with a return estimator that has lower variance [@CITE GAE].
A sketch of a simple policy gradient algorithm is shown in ALGORITHM. 

## Value Learning
Another way to approach the RL problem is by estimating a value function, and then using the value function to retrieve a policy. 
Value functions attempt to quantify the goodness of being in a state or taking an action from a state.
Given that the objective of RL is to maximize the cumulative expected reward, "goodness" refers to how high of a cumulative reward the agent can expect to receive.
For example, the state-action-value or Q function finds the expected return from a state and action pair under a policy, $\pi$, 

$$
  Q^\pi(s_t, a_t) = \mathbb{E}_{\tau \sim p_\pi(\tau \mid s_t, a_t)}\Bigg[\sum_{t' = t}^H \gamma^{t' - t} r(s_t, a_t)\Bigg].
$$

The general Q-learning motivation is that if we know the Q function under certain conditions -- e.g. that the state-action space is discrete and that the given policy visits all state-action pairs repeatedly [@suttonbarto 131; @Qproof] --, then we can easily find the optimal action at any state by selecting the action with the highest Q-value: $a_t^* = \underset{a_t}{\text{argmax}} \, Q^\pi (s_t, a_t)$.
In the case that the state-action space can be tractably represented in a table, one could employ a  algorithm, like temporal-difference control -- see Sutton Ch. 6 for details -- which approximates the Q value for every state-action pair [@suttonbarto]. 
Yet, for many decision-making problems, the state-action space cannot be tractably represented in a table; for this case, one effective alternative is to use a neural network to approximate the Q-function, a method called deep Q-learning. 
The general process for deep Q-learning differs from policy gradient methods, since instead of trying to maximize the return directly, the objective is to minimize the Bellman error.
The Bellman error is a measure of how distant a given value function is from that value function which has been re-evaluated with recent environment interaction.
Suppose we attempt to approximate the Q-function with a neural network and thus represent this approximate Q-function as as $Q_\phi$, the Bellman error for one state-action pair would be

$$
  \mathcal{E} = r(s_t, a_t) + \gamma \, \underset{a}{\text{max}} \, Q_\phi(s_{t+1}, a) - Q_\phi(s_t, a_t).
$$

See the appendix/reference for details on how the Bellman Error is derived from eq. [@CITE]. 
The general scheme for approximate value-learning with deep neural networks is to sample trajectories from the environment, evaluate the Bellman Error across these samples -- with the samples indexed by $i$ --, and then perform the following gradient descent:

$$
  \phi_{t+1} = \phi_t - \alpha \nabla_\phi \, \sum_i \mathcal{E}_i^2 
$$

When being evaluated, a deep Q-learning agent will select actions by inputting the current state to the Q network and then identifying the state-action pair with the highest estimated Q value.
The benefit of using neural networks to approximate Q-functions is that neural networks are able to generalize Q functions over unseen state-action pairs, whereas classic methods like TD-learning are not able to generalize [@CITE].
But there a host of problems with getting neural-network-based value-learning agents to work in practice; see @CITE for more insight on these issues.   
The primary concern with these methods is that they tend to suffer from bias which is introduced by fitting a value network to a target that incorporates an estimated value function [@CITE].  

## Actor-Critic
Actor-critic algorithms integrate the main components from both value-learning and policy gradient based methods.
The "actor" attempts to learn the policy, and the "critic" attempts to learn a value function.
While the general algorithm for the critic is exactly the same as mentioned in the value-learning section, the actor differs from vanilla policy gradient algorithms by incorporating the critic in its gradient ascent step.
Instead of estimating the gradient of the return as in eq ..., the gradient of the return is estimated as 

$$
  \nabla_\theta J(\pi_\theta) \approx \sum_{i,t} \nabla_\theta \log{\pi_\theta (a_{i,t} \mid s_{i,t})} \, \hat{Q}(s_{i,t}, a_{i, t}).
$$

where $\hat{Q}$ is a value function estimated by the critic.
The benefit of actor-critic algorithms is that they balance the issues with bias and variance that are common with value-learning and policy gradient methods respectively [@CITE].
Consequently, actor-critic algorithms have achieved a large majority of the state of the art results in model-free deep RL [@CITE].  

## Value Functions 

There are 3 value functions that are commonly used across RL algorithms: the state-value function, $V^\pi(s_t)$, the state-action value function, $Q^\pi(s_t, a_t)$, and the advantage function, $A^\pi(s_t, a_t)$.
The state-value function is defined as the expected return from a given state,

$$
  V^\pi(s_t) = \mathbb{E}_{\tau \sim p_\pi(\tau \mid s_t)}\Bigg[\sum_{t' = t}^H \gamma^{t' - t} r(s_t, a_t)\Bigg]
$$

Meanwhile, the state-action-value or quality function, finds the expected return given a state and action, 

$$
  Q^\pi(s_t, a_t) = \mathbb{E}_{\tau \sim p_\pi(\tau \mid s_t, a_t)}\Bigg[\sum_{t' = t}^H \gamma^{t' - t} r(s_t, a_t)\Bigg]
$$

And lastly, the advantage function characterizes a relative state-action value in that it quantifies how valuable a state-action pair is over a baseline.
The advantage function can then be succinctly defined as 

$$
  A^\pi(s_t, a_t) = Q^\pi(s_t, a_t) - V^\pi(s_t)
$$ 
Learning $Q^\pi$, commonly called the Q-value function, is more powerful than learning the state-value function as Q-values easily prescribe what action to take when in a certain state, $a_t^* = \underset{a_t}{\text{argmax}} \, Q^\pi (s_t, a_t)$.
Conversely, if we know the state-value function, we would have to perform a maximization over an expected value to find the optimal action, $a_t^* = \underset{a_t}{\text{argmax}} \, \mathbb{E}_{s_{t+1} \sim \,T(s_{t+1} \mid s_t, a_t)}[V^\pi(s_{t+1})]$, which is less tractable.
Yet, learning the advantage function is often preferred in practice as advantage estimators favorably have lower variance than state-value and Q-value estimators [cite schulman gae].
----

