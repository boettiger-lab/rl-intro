---
title: "Appendix for: Deep Reinforcement Learning for Conservation Decisions"
authors:
  - name: Marcus Lapeyrolerie
    department: Department of Environmental Science, Policy, and Management
    affiliation: University of California, Berkeley
    location: Berkeley, California
  - name: Melissa Chapman
    department: Department of Environmental Science, Policy, and Management
    affiliation: University of California, Berkeley
    location: Berkeley, California
  - name: Kari Norman
    department: Department of Environmental Science, Policy, and Management
    affiliation: University of California, Berkeley
    location: Berkeley, California
  - name: Carl Boettiger
    department: Department of Environmental Science, Policy, and Management
    affiliation: University of California, Berkeley
    location: Berkeley, California
    email: cboettig@berkeley.edu

abstract: |
  Enter the text of your abstract here.
keywords:
  - blah
  - blee
  - bloo
  - these are optional and can be removed
bibliography: references.bib
output: 
  rticles::arxiv_article:
    keep_tex: true
---

```{r knitr, include = FALSE}
knitr::opts_chunk$set(echo=FALSE, message = FALSE, warning = FALSE,
                      fig.width = 7, fig.height = 4)
ggplot2::theme_set(ggplot2::theme_bw())

scale_colour_discrete <- function(...) ggthemes::scale_colour_solarized()
scale_fill_discrete <- function(...) ggthemes::scale_fill_solarized()
pal <- ggthemes::solarized_pal()(6)
txtcolor <- "#586e75"
```

```{r setup, cache = FALSE, message=FALSE}
library(tidyverse)
library(patchwork)
library(reticulate)

np <- import("numpy")
np$random$seed(42L)

# force CPU-evaluation if needing perfect reproducibility
Sys.setenv("CUDA_VISIBLE_DEVICES"="0")

```

## Finding a known optimal solution using RL

fishing gym?

```{r }
## Python dependencies
gym         <- import ("gym")
gym_fishing <- import("gym_fishing")
sb3         <- import ("stable_baselines3")

## initialize the environment
env <- gym$make("fishing-v1", sigma = 0.1)
```

```{r eval=!file.exists("cache/td3.zip")}
# train an agent (model) on one of the environments:


#Trial 15 finished with value: 7.6994757652282715 and parameters: 
hyper = list('gamma'= 0.995, 'lr'= 0.0001355522450968401, 'batch_size'= 128L, 
         'buffer_size'= 10000L, 'episodic'= FALSE, 'train_freq'= 128L, 
         'noise_type'= 'normal', 'noise_std'= 0.6656948079225263, 
         'net_arch'= 'big')
policy_kwargs = list(net_arch=c(400L, 300L)) # big


#non-episodic:
hyper['gradient_steps'] = hyper['train_freq']
hyper['n_episodes_rollout'] = -1
n_actions = env$action_space$shape[0]
hyper$action_noise = sb3$common$noise$NormalActionNoise(
        mean=np$zeros(n_actions),
        sigma= hyper[['noise_std']] * np$ones(n_actions)
        )


td3 = sb3$TD3('MlpPolicy', env,  verbose=0L, seed = 42L,
            gamma = hyper[['gamma']],
            learning_rate = hyper[['lr']],
            batch_size = hyper[['batch_size']],            
            buffer_size = hyper[['buffer_size']],
            action_noise = hyper[['action_noise']],
            train_freq = hyper[['train_freq']],
            gradient_steps = hyper[['train_freq']],
            n_episodes_rollout = hyper[['n_episodes_rollout']],
            policy_kwargs=policy_kwargs)


td3$learn(total_timesteps=300000L)
td3$save("cache/td3")
```

```{r}
# Simulate management under the trained agent
td3 <- sb3$TD3$load("cache/td3")
td3_sims <- env$simulate(td3, reps = 500L)
td3_policy <- env$policyfn(td3, reps = 50L)
```



```{r eval=!file.exists("cache/a2c.zip")}
# train an agent (model) on one of the environments:
a2c <- sb3$A2C('MlpPolicy', env, verbose=0L, seed = 42L) # Must use L for integers!
a2c$learn(total_timesteps=300000L)
a2c$save("cache/a2c")
```

```{r}
# Simulate management under the trained agent
a2c <- sb3$A2C$load("cache/a2c")
a2c_sims <- env$simulate(a2c, reps = 500L)
a2c_policy <- env$policyfn(a2c, reps = 50L)
```



```{r}
# Simulate under the optimal solution (given the model)
opt <- gym_fishing$models$escapement(env)
opt_sims <- env$simulate(opt, reps = 500L)
opt_policy <- env$policyfn(opt)
```

```{r}
sims_df <- bind_rows(td3_sims, a2c_sims, opt_sims, .id = "model") %>%
  mutate(model = c("TD3", "A2C", "optimal")[as.integer(model)])

policy_df <- bind_rows(td3_policy, a2c_policy, opt_policy, .id = "model") %>%
  mutate(model = c("TD3", "A2C", "optimal")[as.integer(model)])

gamma <- 1 #discount
reward_df <- sims_df %>% 
  group_by(rep, model) %>%
  mutate(cum_reward = cumsum(reward * gamma^time)) %>%
  group_by(time, model) %>%
  summarise(mean_reward = mean(cum_reward), 
            sd = sd(cum_reward), .groups = "drop")

```




```{r}
ymin <- function(x) last(x[(ntile(x, 20)==1)])
ymax <- function(x) last(x[(ntile(x, 20)==19)])

fig_sims <- 
sims_df %>% 
  group_by(time, model) %>% 
  summarise(ymin = ymin(state),
            ymax = ymax(state),
            state = mean(state), .groups = "drop") %>%
  ggplot(aes(time, state, ymin = ymin, ymax = ymax, fill=model)) + 
  geom_ribbon(alpha= 0.3) + geom_line(aes(col = model))
```


```{r}
fig_policy <- 
  policy_df %>% ggplot(aes(state, state - action, 
                           group=interaction(rep, model),
                           col = model)) + 
  geom_line(show.legend = FALSE) + 
  coord_cartesian(xlim = c(0, 1.3), ylim=c(0,0.9)) + 
  ylab("escapement")
```

```{r}

fig_reward <- reward_df %>% 
  ggplot(aes(time, mean_reward)) + 
  geom_ribbon(aes(ymin = mean_reward - 2*sd, 
                  ymax = mean_reward + 2*sd, fill = model),
              alpha=0.25, show.legend = FALSE) + 
  geom_line(aes(col = model), show.legend = FALSE) + 
  ylab("reward")
```

```{r fig1, fig.cap=""}
fig_sims / ( fig_policy + fig_reward)
```
```{r}
```
