---
title: Deep Reinforcement Learning for Conservation Decisions
authors:
  - name: Marcus Lapeyrolerie
    department: Department of Environmental Science, Policy, and Management
    affiliation: University of California, Berkeley
    location: Berkeley, California
  - name: Melissa Chapman
    department: Department of Environmental Science, Policy, and Management
    affiliation: University of California, Berkeley
    location: Berkeley, California
  - name: Kari Norman
    department: Department of Environmental Science, Policy, and Management
    affiliation: University of California, Berkeley
    location: Berkeley, California
  - name: Carl Boettiger
    department: Department of Environmental Science, Policy, and Management
    affiliation: University of California, Berkeley
    location: Berkeley, California
    email: cboettig@berkeley.edu
    
abstract: |
  Enter the text of your abstract here.
keywords:
  - blah
  - blee
  - bloo
  - these are optional and can be removed
bibliography: references.bib
output: 
  rticles::arxiv_article:
    keep_tex: true
---

# Introduction

Deep Reinforcement Learning (RL), a machine learning technique in which a _software agent_ is trained to select actions which best maximize some cumulative reward, could hold significant untapped promise for solving complex ecological and conservation decision-making problems from wildfire management to biodiversity preservation to zoonotic pandemics.
Unlike more widely recognized machine learning techniques for either _supervised_ learning (such as image classification, @species_ex, @satellite_ex) or unsupervised learning, RL focuses on solving _dynamic_ problems which require planning ahead, often in uncertain and changing environments.
Of all machine learning techniques, RL most resembles a general artificial intelligence.
Notable successes using RL include AlphaZero, in which a software agent learned to beat the world's best human players in games of chess, Go, and Shogi [@alphazero],
and major advances in robotics, including decision-making in autonomous vehicles [@robotics_ex]. 
These tasks all require the agent to not only anticipate future states of it's environment, but also respond to those changes to influence future outcomes.


<!-- Millie help on this para? -->
If conservation challenges are typically complex and dynamics, existing approaches are largely simple and static.
The establishment of protected areas (cite MPAs, 30x30, etc) focuses on policies fixed in space and time to address challenges that are fixed in neither.
The endangered species act ...
Ecosystem services ....
Such simplicity is in no small part due to the challenge associated in determining a 
Some reference to @TomDietrich on potential for CS-based applciation.


In contrast to other ML methods, RL does not require massive data sets for training.  
Despite the expansion in data collection from remote sensors, citizen science, and other efforts, ecological data remains very sparse on many necessary details.
Moreover, no amount of data can train supervised or unsupervised ML approaches can predict a future which has no analog in the historical record.
Mechanistic, process-based models remain our most powerful tool to explore scenarios of dynamic ecosystems in changing climates.
Methods from _optimal control theory_ have long been used in conservation and resource economics literature [@] to determine optimal policies given such models [@Polasky2011].
However, control theory approaches are also tightly constrained by the "curse of dimensionality" [@Mangel1980; @Marescot2013], limiting their application to only simple, stylized models.
Deep reinforcement learning offers a way forward. 
Deep RL agents are trained not with mountains of data, but by interacting with simulators.
Most recent advances in Deep RL have illustrated algorithms trained by playing arcade game simulators such as Atari BreakOut, or physics-based simulations of robotics movement [@TD3; @SAC; @PPO; @A2C].
Simulating from complex models is far simpler than solving them.
The richest ecosystem models, such as the global circulation models (GCMs) used to generate future climate scenarios [@something], or individual-based models used to predict forest growth [@something] or wildfire spread [@something], are massive simulations. 
Given sufficient computational power, RL can be trained and evaluated across a broad range of possible realistic scenarios well beyond the reach of classical methods.
In this manner, RL would compliment, rather than replace, existing empirical and theoretical research which would continue to form the basis for those simulations.


If RL has great promise, the approach also imposes great risk.  
Lacking the tractability of classical methods, an RL agent can appear to solve a wide class of scenarios with ease, but may fail unexpectedly under seemingly similar conditions [@henderson2017].  
Despite recent progress, Deep RL is a young field and stability and robustness of its methods remain largely open problems. 
RL approaches are no pancea, and inproper design or testing can easily lead to negative outcomes.
Technical issues are not the only pitfall of AI applications to conservation problems.
Giving algorithms direct influence over conservation policy may also raise ethical and political issues, particularly if those algorithms (or components thereof) are treated as proprietary intellectual property.

In this paper, we introduce the fundamentals of deep reinforcement learning and illustrate with simple examples how these approaches can be applied to important conservation decision-making problems, from overharvesting in global fisheries to ecosystem tipping points to wildfire dynamics.
These examples span a range from problems for which an optimal analytic solution is already known, to problems not previously solved with classical methods, to problems not yet solved by RL. 
We include a detailed appendix with complete examples in both python and R for training RL algorithms, as well as three stand-alone module "gyms" implementing the each of the conservation problems presented here using a standard benchmark [@gym], which we believe can facilitate future contributions both from researchers in RL searching for interesting and important open problems as well as ecologists who may seek to make those simulations more realistic and precise to specific application areas.
We hope this paper serves as a port of entry to researchers and students from either field seeking to engage in work on RL applications to conservation. 
We also hope these examples serve as illustration of both the potential and perils of such applications.


# Sequential Decision Problems and Reinforcement Learning

<!-- meh, not sure we need this? -->

Reinforcement learning focuses primarily on sequential decision problems: tasks in which a decision-maker or "agent" must repeatedly decide what action to take in response to new information about the "state" of the system they seek to manage, in order to maximize some objective (referred to  as the reward or utility).
Any conservation problem that considers the future possible scenarios is a sequential decision problem: even scenarios that face a one-off decision such as whether declare some region as a protected area, or list or de-list a species as endangered, are sequential decision problems if they must consider the timing of their actions.
In contrast, decisions that do not consider the future, such as deciding which of a collection of possible regions should be prioritized based only on the present: how many species currently inhabit a region, how much each region would cost to purchase today, are not sequential problems.
Central to any sequential decision problem is the need to consider possible future states, the need to forecast.
Because the agent's actions will influence that future, agents facing sequential decision problems must further be able to forecast how each possible action available to them will change that forecast.
Consequentially, our lack of knowledge about the underlying models and inability to predict future environments in our rapidly changing world pose a fundamental challenge to conservation decision-making.


This framework raises two conceptual challenges.
First, sequential decision-making problem is distinct from the problem of model estimation.
A sequential decision problem will typically treat the model as given, typically with some uncertainty around the process and possibly the measurmeents and model estimates as well.
Thus, methods for solving sequential decision-making should not be mistaken as alternatives to methods designed for model estimation, such as regression, Bayesian heirarchical modeling, generalized additive models (GAMs) etc.
Second, methods for describing and solving sequential decision-making problems are often given different terms by different communities to describe the same thing.
Sequential decision problems are common to behavioral ecology [@Mangel], conservation [@Marescot2013] natural resource economics [@Clark1990], and some engineering fields [] where they are known as "optimal control" problems, while computer science, neuroscience, and robotics typically refer to them as 'reinfocement learning'.    


# RL essentials

**Model-based vs Model-free RL**

**Current algorithms**

- terminology, problem definition
- discussion / table of current benchmark algorithms (DQN, PPO, DDPG, A2C, TD3, SAC)
- training, hyperparameter tuning

# Examples

## Sustainable harvest

We provide three examples that illustrate the application and potential of deep RL to ecological and conservation problems, highlighting both the potential and challenges there-in.  The first example focuses on the well-studied problem of setting harvest quotas in fisheries management [@Schaefer1954; @Clark1974; @Reed1979; @Worm2006; @Costello2018].  Given an appropriately estimated model, the optimal harvest policy can be determined precisely (e.g. by stochastic dynamic programming, SDP, see @Marescot2013), and under most circumstances takes the form of a "constant-escapement" policy [see mathematical proof in @Reed1979]. This provides a natural first benchmark for deep RL approaches, since we can compare the RL solution to the mathematical optimum directly.  The problem is also of interest because the RL approach can be extended to consider ecological dynamics that either surpass the complexity possible with SDP methods, or violate assumptions of such classical approaches). Moreover, fisheries management is both an important global challenge in it's own right [@Worm2006; @Costello2018] as well as a common test case to understand ecological dynamics and conservation management more broadly [@].

For illustrative purposes, we consider the simplest version of the dynamic optimal harvest problem as outlined by @Clark1974 or @Reed1979 (covering the stochastic case).  In short, the manager seeks to optimize the net present value (discounted cumulative catch) of a fishery, observing the stock size each year and setting an appropriate harvest quota in response.  In the classical approach, the best model of the fish population dynamics must first be estimated from data, potentially with posterior distributions over parameter estimates reflecting any uncertainty. From this model, the optimal harvest policy -- that is, the a function which returns the optimal quota for each possible observed stock size -- can be determined either by analytic [@Reed1979] or numerical [@Marescot2013] methods, depending on the complexity of the model.  In a RL approach, the model is instead replaced by a computer simulation, usually referred to as an "environment."  The simplest simulation is merely an implementation of the estimated model itself.  However, unlike an analytic or SDP solution that is derived directly from the estimated model, the RL approach is in principle agnostic to the details of the simulation.


```{r knitr, include = FALSE}
library(tidyverse)
library(patchwork)
knitr::opts_chunk$set(echo=FALSE, message = FALSE, warning = FALSE,
                      fig.width = 7, fig.height = 4)
ggplot2::theme_set(ggplot2::theme_bw())

scale_colour_discrete <- function(...) ggthemes::scale_colour_solarized()
scale_fill_discrete <- function(...) ggthemes::scale_fill_solarized()
pal <- ggthemes::solarized_pal()(6)
txtcolor <- "#586e75"

```





```{r}
sims_df <- read_csv( "figs/sims_df.csv")
policy_df <- read_csv("figs/policy_df.csv")
reward_df <- read_csv("figs/reward_df.csv")

ymin <- function(x) last(x[(ntile(x, 20)==1)])
ymax <- function(x) last(x[(ntile(x, 20)==19)])

fig_sims <- 
sims_df %>% 
  group_by(time, model) %>% 
  summarise(ymin = ymin(state),
            ymax = ymax(state),
            state = mean(state), .groups = "drop") %>%
  ggplot(aes(time, state, ymin = ymin, ymax = ymax, fill=model)) + 
  geom_ribbon(alpha= 0.3) + geom_line(aes(col = model))

fig_policy <- 
  policy_df %>% ggplot(aes(state, state - action, 
                           group=interaction(rep, model),
                           col = model)) + 
  geom_line(show.legend = FALSE) + 
  coord_cartesian(xlim = c(0, 1.3), ylim=c(0,0.9)) + 
  ylab("escapement")


fig_reward <- reward_df %>% 
  ggplot(aes(time, mean_reward)) + 
  geom_ribbon(aes(ymin = mean_reward - 2*sd, 
                  ymax = mean_reward + 2*sd, fill = model),
              alpha=0.25, show.legend = FALSE) + 
  geom_line(aes(col = model), show.legend = FALSE) + 
  ylab("reward")
```

```{r fig1, fig.cap=""}
fig_sims / ( fig_policy + fig_reward)
```


Note that the A2C policy learns to shut down fishing entirely below a certain stock size, without any variation.  However, it keeps the fishery closed even once stock sizes are slightly higher than $B_{MSY}$.  Once it begins fishing, it slightly under-harvests for stock sizes a little over $B_{MSY}$, but tends to over-harvest for very large stock sizes (rarely ever realized, as fishing pressure is enough to keep it out of this range). Note also that the harvest varies across replicates, creating a fuzzy region in the policy function. Under such a policy, A2C maintains a stock size on average significantly above $B_{MSY}$, though also sees higher volatility particularly on the low side.  Note that due to the stochastic nature of the environment, any given replicate A2C may out-perform the optimal policy, though on average the optimal policy provides consistently better yields.  


## Beyond classic methods: Non-stationary models







```{r}
sims_df <-read_csv("figs/tipping_sims_df.csv")
policy_df <- read_csv("figs/tipping_policy_df.csv")
reward_df <- read_csv("figs/tipping_reward_df.csv")
```


```{r}
fig_sims <- 
  sims_df %>%
   group_by(time, model) %>% 
   summarise(ymin = ymin(state),
            ymax = ymax(state),
            state = mean(state), .groups = "drop") %>%
  ggplot(aes(time, state, ymin = ymin, ymax = ymax, fill=model)) + 
   geom_ribbon(alpha= 0.3) + geom_line(aes(col = model))

fig_policy <- 
  policy_df %>% ggplot(aes(state, action, 
                           group=interaction(rep, model),
                           col = model)) + 
  geom_line(show.legend = FALSE) 

fig_reward <- reward_df %>% 
  ggplot(aes(time, mean_reward)) + 
  geom_ribbon(aes(ymin = mean_reward - 2*sd, 
                  ymax = mean_reward + 2*sd, fill = model),
              alpha=0.25, show.legend = FALSE) + 
  geom_line(aes(col = model), show.legend = FALSE) + 
  ylab("reward")
```

```{r fig2, fig.cap=""}
fig_sims / ( fig_policy + fig_reward)
```



## Example of solving a problem that is difficult/impossible for conventional methods



two use cases to consider: 
  - model is known but too complex to solve by (approximate) dynamic programming etc, or non-markovian
  - model is unknown
  
model uncertainty/transfer learning? nonstationary dynamics? 

## Example of an open problem without an RL solution

fire gym?


# Open issues

- wild west of AI?
- reproducibility
- AI ethics, transparency

# a vision going forward

- our public/open source environments and agents, leaderboard
- the need for collaborative/competitive development both to make better benchmark environments and better agents


