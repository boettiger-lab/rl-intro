---
title: Deep Reinforcement Learning for Conservation Decisions
authors:
  - name: Marcus Lapeyrolerie
    department: Department of Environmental Science, Policy, and Management
    affiliation: University of California, Berkeley
    location: Berkeley, California
  - name: Melissa Chapman
    department: Department of Environmental Science, Policy, and Management
    affiliation: University of California, Berkeley
    location: Berkeley, California
  - name: Kari Norman
    department: Department of Environmental Science, Policy, and Management
    affiliation: University of California, Berkeley
    location: Berkeley, California
  - name: Carl Boettiger
    department: Department of Environmental Science, Policy, and Management
    affiliation: University of California, Berkeley
    location: Berkeley, California
    email: cboettig@berkeley.edu
abstract: |
  Enter the text of your abstract here.
keywords:
  - blah
  - blee
  - bloo
  - these are optional and can be removed
bibliography: references.bib
header-includes:
  - \usepackage{amsmath}
  - \usepackage{amssymb}
output: 
  rticles::arxiv_article:
    keep_tex: true
editor_options: 
  markdown: 
    wrap: sentence
---


# Introduction

<!-- 
- Global change problems: Important but tough decisions
- ML: exciting stuff, but only fitting data is driving through the rearview mirror
- RL is the ML for decisions (in dynamic/uncertain environments) 
- RL can use existing knowledge and models to improve decisions in complex/uncertain scenarios (read: global change biology)
- Here be dragons / map paragraph

-->


<!-- Environmental problems, ML -->
Advances in both available data and computing power have begun to open the door to a greater role for machine learning (ML) in addressing some of our planetâ€™s most pressing environmental problems, such as the growing frequency and intensity of wildfire [@wildfire], over-exploited fisheries [@Worm2006], declining biodiversity [@biodiversity], and zoonotic pandemics [@covid].
But will ML approaches really help us tackle a changing planet?
Applications of ML in ecology have begun to illustrate the promise of two methods of ML: *supervised learning* [@joseph2020] and *unsupervised learning* [@unsupervised], but have so far largely overlooked the third and possibly most promising approach in the ML triad: *reinforcement learning* (RL).
Three features distinguish RL from other ML methods in ways that are particularly well suited to addressing global ecological change issues:

1) RL is explicitly focused on the task of selecting actions in an uncertain and changing environment to maximize some objective,
2) RL does not require massive amounts of representative sampled historical data,
3) RL approaches easily integrate with existing ecological models and simulations, which may be our best guide to understanding and predicting future possibilities.

Despite relevance to uncertainty and decision making that could make RL uniquely well suited for ecological and conservation problems, it has so far seen little application in this area.
To date, the problems considered by RL research have largely been drawn from examples in robotic movement and games like Go and Starcraft [@lillicrap; @alphazero; @alphastar]
Complex environmental problems share many similarities to these tasks and games: the need to plan many moves ahead given a large number of possible outcomes, to account for uncertainty and to respond with contingency to the unexpected.
RL agents typically develop their strategies by interacting with simulators, a practice that should not be unsettling to ecologists since learning from simulators is common across ecology.
Rich, processes-based simulations such as the SORTIE model in forest management [@sortie], Ecopath with Ecosim in fisheries management [@ecopath], or climate change policy models [@dice] are already used to explore scenarios and inform ecosystem management.
Decision-theoretic approaches based on optimal control techniques can only find the best strategy in the simplest of ecological models; the so called "curse of dimensionality" makes problems with a large number of states or actions intractable by conventional methods [@Possingham2006; @Marescot2013].
Neural-network-based RL techniques, referred to as *deep RL*, have proven particularly effective in problems involving complex, high-dimensional spaces that have previously proven intractable to classical methods.
We believe that the application of RL methods to simulations of ecological processes will open the door to a new paradigm for tackling tough environmental change problems.
We also believe that such a transformation could bring risks and pitfalls comparable and varied as its solutions.

In this paper, we draw on examples from fisheries management and ecological tipping points to illustrate how deep RL techniques can successfully discover optimal solutions to previously solved management scenarios and discover highly effective solutions to unsolved problems.
Our simplest example approaches the known optimal solution for a classic fisheries management problem with no prior knowledge of the underlying model.
Our second example considers ecosystem management under slow environmental degradation: a scenario which is not amenable to classic optimization methods such as dynamic programming.
In this case there is no existing optimal solution to compare against, but an appropriately tuned RL agent can out-perform a sensible rule-of-thumb response.
These examples demonstrate that RL-based approaches are capable but by no means a magic bullet: reasonable solutions require careful design of training environments, choice of RL algorithms, tuning and evaluation, as well as substantial computational power.
Our examples are intentionally simple, aiming to provide a clear template for understanding that could be easily extended to cover more realistic conditions. 
We include an extensive appendix with carefully annotated code which should allow readers to both reproduce and extend this analysis.
We further include implementations of three fully featured simulation modules following current standards for RL benchmark problems as Python modules distributed on PyPi and GitHub.
Each module includes several more challenging variations to the examples described here.
We intend to continue to expand this library of ecological management benchmarks with the hope both of challenging researchers already working in RL to tackle some of these pressing issues and for ecologists and environmental scientists to continue to refine these benchmarks into more realistic ecological simulations.


# RL overview

<!-- Conceptual picture: (A) an "environment", and (B) an "agent"
  Components of an agent: 
    - *neural network*, whose *parameters* are estimated by *training* +
    - an RL algorithm, whose *hyperparameters* (including NN structure) are estimated by *tuning*
    - High-level overview of an RL algorithm components (model/model-free, on/off policy, stochastic/deterministic, etc).
  conceptual explanation of 'model-free', what it means to have a "policy" without modeling/predicting transitions

-->


Any application of RL can be divided into two components: an *environment* and an *agent*.
An *environment* is typically a computer simulation, though it is possible to use the real world as the RL environment [@robotics].
The *agent*, which is often a computer program, continuously interacts with the environment. 
At each time step, the agent observes the current *state* of the environment then performs an *action*; as a result of this action, the environment will transition to a new state and will transmit a numerical *reward* signal to the agent.
The goal of the agent is to learn how to maximize its expected cumulative reward.
The agent learns how to achieve this objective during a period called *training*.
In training, the agent *explores* the available states and actions. 
Once the agent comes across a highly rewarding sequence of states and actions, the agent will reinforce this behavior so that it is more likely for the agent to *exploit* the same high reward trajectory in the future.
Throughout this process, the agent's behavior is codified into what is called a *policy*, which describes what action an agent should take for a given state.


## Designing and building an environment

An environment is a mathematical function, computer program, or real world experience that takes an agent's proposed *action* as input and returns an observation of the environment's current *state* and an associated *reward* as output.
In contrast to classical approaches [@Marescot2013], there are few restrictions on what comprises a state or action. 
States and actions may be continuous or discrete, completely or partially observed, single or multidimensional.
The main focus of building an RL environment, however, is in the environment's transition dynamics and reward function.
The designer of the environment can make the environment follow any transition and reward function provided that both are functions of the current state and action.
This ability allows RL environments to model a broad range of decision making problems.
For example, we can set the transitions to be deterministic or stochastic.
We can also specify the reward function to be *sparse*, whereby a positive reward can only be received after a long sequence of actions, e.g. the end point in a maze.
In other environments, an agent may have to learn to forgo immediate rewards (or even accept an initial negative reward) in order to maximize the net discounted reward as we illustrate in examples here.


\begin{figure}
\includegraphics[width=\linewidth]{fig1.pdf}
\caption{Deep Reinforcement Learning:  A deep RL \emph{agent} uses a \emph{neural network} to select an \emph{action} in response to an \emph{observation} of the \emph{environment}, and receives a \emph{reward} from the environment as a result. During \emph{training}, the agent tries to maximize its cumulative reward by interacting with the environment and learning from experience. In the RL loop, the agent performs an action, and the environment returns a reward as well as an observation of the environment's state. The agent-environment loop continues until the environment reaches a terminal state, after which the environment will reset, causing a new \emph{episode} to begin. Across training episodes, the agent will continually update the \emph{parameters} in its neural network, so that the agent will select better actions. Before training starts, the researcher must input a set of \emph{hyperparameters} to the agent; hyperparameters direct the learning process and thus affect the outcome of training. A researcher finds the best set of hyperparameters during \emph{tuning}. Hyperparameter tuning consists of iterative \emph{trials}, in which the agent is trained with different sets of hyperparameters. At the end of a trial, the agent is evaluated to see which set of hyperparameters results in the highest cumulative reward. An agent is \emph{evaluated} by recording the cumulative reward over one episode, or the mean reward over multiple episodes. Within evaluation, the agent does not update its neural network; instead, the agent only uses the neural network to select actions.}
\end{figure}

@brockman2016 identified (1) the lack of standardization of environments used in publication and (2) the need for better benchmark environments as two core problems limiting more rapid advancement of RL research, and proposed the creation of the OpenAI `gym` framework to address both problems.
The `gym` framework defines a standard interface and methods by which a developer can describe an arbitrary environment in a computer program.
This allows the design and application of software agents which can interact and learn in that environment without knowing anything about the environment's internal details.
The `gym` Python module created by OpenAI also provides over 100 pre-built environments that serve as benchmark problems against which new RL algorithms can be tested.
The `gym` framework is now recognized by all major software frameworks for writing RL algorithms (see Appendix, 'Deep RL Frameworks').
While this library of environments spans a range of difficulties from classic control problems that can be solved in closed form to complex three-dimensional robotics simulators, they include no examples of ecological or biological problems.
We use the `gym` framework to turn several existing ecological models into valid environmental simulators that can similarly be used in any RL framework.
In Appendix C, we give detailed instruction on how an OpenAI `gym` is constructed.

## Training and tuning an agent

Training a deep RL agent on an environment involves allowing the agent to interact with the environment for thousands or even millions of time steps.
The training period involves a trial and error process in which the agent inputs actions to the environment and uses the reward signal to learn an optimal policy. 
According to the algorithm that the agent follows, the agent will explore the state-action space and use its experience to improve its decisions.
The amount of time needed for an agent to learn high reward yielding behavior cannot be predetermined and depends on a host of factors including the amount of dedicated compute, complexity of the environment, complexity of the agent and more.
Yet, overall, it has been well established that deep, model-free RL agents tend to be very sample inefficient [@gu_q-prop_2017], so it is recommended to provide a generous training budget for these agents.
An advantage of using deep, model-free algorithms is that since they function autonomously and do not require any knowledge about the environment, we can readily test how these algorithms perform without knowing anything about the environment's transition dynamics or the inner workings of the algorithm.

Hyperparameters refer to the parameters in a deep, RL algorithm that control the learning process, e.g. the step size to use for gradient descent steps.
Since the optimal hyperparameters vary across environments and can not be predetermined [@drlthatmatters], it is necessary to find a good-performing set of hyperparameters in a process called hyperparameter tuning.
Hyperparameter tuning consists of the following general steps in repetition: proposing a new set of hyperparameters, training an agent with these hyperparameters, then examining the resulting performance.
This procedure can be automated through standard multi-dimensional optimization approaches. 
Since one round of training can be time intensive, hyperparameter tuning, which consists of usually numerous rounds of training, can be particularly time intensive.
Hyperparameter tuning, however, is a necessary step in evaluating the performance of an algorithm and should be a standard component of any deep RL workflow.



# Formal RL Concepts
<!-- ML: As I read this section, I have some concerns on this coming off as overly repetitive with the parts above. -->

In this brief section, we will introduce the concept of the RL environment as a partially observable Markov decision process (POMDP); we will then use POMDP definitions to define the RL objective.

The reinforcement learning environment is typically formalized as a discrete-time POMDP.
A POMDP is a 7-tuple that consists of the following: 

- $\mathcal{S}$: a set of states called the state space

- $\mathcal{A}$: a set of actions called the action space

- $\Omega$ : a set of observations called the observation space

- $E(o_{t}| s_{t})$: an emission distribution, which accounts for an agent's observation being different from environment's state

- $T(s_{t+1}|s_t, a_t)$: a state transition operator which describes the dynamics of the system

- $r(s_t, a_t)$: a reward function

- $d_0(s_0)$: an initial state distribution

- $\gamma \in (0,1]$: a discount factor

The agent interacts with the environment in an iterative loop, whereby the agent only has access to the observation space, action space and the discounted reward signal, $\gamma^t \, r(s_t, a_t)$.
At each time step, the agent makes an observation of the environment at time t, $o_t$, and selects an action, $a_t$, according to its policy, $\pi$, which maps observations to a probability distribution over actions, $\pi: \Omega \rightarrow p(\mathcal{A} \mid \Omega)$.
The action will cause the environment to transition to a new state according to the state transition operator. 
The agent will then receive a new observation and a discounted reward signal, $\gamma^t \, r(s_t, a_t)$, as feedback. 
This process will repeat until a terminal state is reached, after which the environment can reset to an initial state.
As the agent interacts with the environment, the agent will create a trajectory in state-action space given as $\tau = (s_0, a_0, \dots, s_H, a_H)$ where H denotes the length of the state-action sequence.
From these definitions, we can provide an agent's trajectory distribution for a given policy as,

$$
  p_\pi(\tau) = d_0(s_0) \prod_{t=0}^H \pi(a_t | o_t) \, E(o_{t}| s_{t}) \, T(s_{t+1} | s_t, a_t)
$$

The goal of reinforcement learning is for the agent to find an optimal policy distribution, $\pi^*(a_t|o_t)$, that maximizes the expected return, $J(\pi)$:

$$
  \pi^* = \underset{\pi}{\text{argmax}} \,\, \mathbb{E}_{\tau \sim p_\pi(\tau)}\Big[\sum_{t=0}^{H} \gamma^t r(s_t, a_t) \Big] =  \underset{\pi}{\text{argmax}} \,\, J (\pi)
$$

Note that this formalism extends to an infinite horizon case when $H= \infty$.
While there are RL methods to deal with infinite horizon cases, we will only present methods for finite horizon or episodic POMDPs.
In the appendix, we will go into further detail on the general approaches that model-free deep RL algorithms take towards optimizing the RL objective. 

# Deep RL Algorithms

<!-- 

Conceptual figure:

Algorithm 
  hyper-parameters specifying neural network, learning rate, etc.  
  We refer to a particular selection of algorithm and hyperparameters as an "agent", which is then "trained" to solve one or more environments
  internal 'parameters', estimated by "training" the agent. 
  

-->

There is an ever increasing multitude of reinforcement learning algorithms, and while there are far too many algorithms to discuss in detail in this paper, we will herein classify and touch on some current methods.

To solve the RL problem, algorithms either take a *model-free* or *model-based* approach. 
The distinction is that *model-free* algorithms do not attempt to learn or use a model of the environment; yet, *model-based* algorithms employ a model of the environment to achieve the RL objective.
A trade-off between these approaches is that when it is possible to quickly learn a model of the environment or the model is already known, model-based algorithms tend to require much less interaction with the environment to learn good-performing policies [@mbpo].
Yet, frequently, learning a model of the environment is very difficult, and in these cases, model-free algorithms tend to outperform [@mbpo].
Since our examples use model-free agents, we will focus on model-free algorithms in this section.

Among model-free methods, there is divergence around what functions the agent is attempting to learn in order to achieve the RL objective.
Generally, model-free agents are either learning a policy function, a value function or both.
For context, *value functions* are proxies for how high of a cumulative reward an agent can expect to receive from a given state or state-action pair.
If an agent knows the value function, then the agent can find the optimal action at any state by selecting the action that will maximize the value function in expectation.
The class of algorithms that exclusively try to learn a value function are called *value-based* methods.
In contrast, *policy gradient* algorithms exclusively learn the policy.
And, lastly, *actor-critic* algorithms attempt to learn both a policy and value function, whereby the value function is used to inform the agent on the goodness of a selected action.
We go into greater depth on these different classes of RL algorithms in Appendix A.

Neural networks become useful in RL when the environment has a large state-action space, which happens frequently with realistic decision-making problems. 
Classic RL algorithms, like policy iteration, value iteration and TD-learning, fail when the state-action space is large, e.g. when the possible states and actions can not be represented in a tractable table [@suttonbarto].
Yet, over the last 10 years, researchers have shown that neural network-based RL algorithms can perform well on previously unsolvable environments [@DQN, @alphaGo2016].
Neural networks have been widely useful in reinforcement learning because neural nets have the property of being general function approximators [@universalapproximators].
Whenever there is a need for an agent to approximate some function, say a policy function or value function, neural networks can be used in this capacity.
While there are other function approximators that can be used in RL, e.g. Gaussian processes [@grande14], neural networks have excelled in this role because of their ability to learn complex, non-linear, high dimensional functions and their ability to adapt given new information [@briefsurveydrl].
We will not discuss in detail how neural networks work -- see [@lecun_deep_2015] for further background --, but the general process for how neural networks are used in RL is that at each learning step, the agent, say a policy gradient-based agent, will adjust the parameters in its policy-approximating neural network so that according to past experiences, the agent will be more likely to engage in highly rewarding behavior in the future. 
This same process occurs in value learning and actor-critic-based deep RL algorithms, except these agents are adjusting either a value network or both a value and policy network respectively

Over the course of training, RL agents will engage in trial and error learning, whereby it will be necessary for the agent to explore new sequences of actions as well as exploit past high rewarding sequences.
Occasionally, the agent will need to propose exploratory actions that may result in higher rewards than previously seen. 
Yet, the agent may also need to focus on past sequences of actions which may require marginal improvement to reach optimality.
The fundamental issue is that it is very difficult for the agent to know when it should engage in explorative or exploitative behavior. 
This *exploration-exploitation dilemma* is an open research problem [@berger-tal_exploration-exploitation_2014], and current deep RL algorithms take a range of approaches towards this dilemma -- in Appendix A, we discuss some of the more specific exploration algorithms.
Yet, overall, the manner in which RL algorithms explore depend on whether the algorithms are *on-policy* or *off-policy*.
The distinction between on and off-policy evaluation comes from what policy the agent is using to select actions during training.
For off-policy algorithms, the agent uses a policy during training that can be different from the policy that the agent will use during evaluation; while in on-policy algorithms, the agent updates the same policy in training as the agent will use during evaluation.
There are trade-offs to consider in using an off-policy or on-policy learner, which we will further discuss in the Appendix A.

On top of the aforementioned distinctions for model-free, deep RL algorithms, there are more algorithm design choices that can be made. 
One important point of divergence is whether the agent learns a deterministic or stochastic policy.
Another is whether the algorithm is formulated to work on a discrete, continuous or both a discrete and continuous state and action space.
In Appendices A and B, we will introduce the considerations one should make in deciding whether an algorithm is well suited for a specific environment. 




# Examples

We provide three examples that illustrate the application and potential of deep RL to ecological and conservation problems, highlighting both the potential and the inherent challenges.

```{r knitr, include = FALSE}
library(tidyverse)
library(patchwork)
knitr::opts_chunk$set(echo=FALSE, message = FALSE, warning = FALSE,
                      fig.width = 7, fig.height = 4)
ggplot2::theme_set(ggplot2::theme_bw())

scale_colour_discrete <- function(...) ggthemes::scale_colour_solarized()
scale_fill_discrete <- function(...) ggthemes::scale_fill_solarized()
pal <- ggthemes::solarized_pal()(6)
txtcolor <- "#586e75"
```

## Sustainable harvest

The first example focuses on the important but well-studied problem of setting harvest quotas in fisheries management.
Determining fishing quotas is both a critical ecological issue [@Worm2006; @Worm2009; @Costello2016], and also a textbook example that has long informed the management of renewable resources within fisheries and beyond [@Clark1990].
Given a population growth model that predicts the total biomass of a fish stock in the following year as a function of the current biomass, it is straight forward to determine what biomass corresponds to the maximum growth rate of the stock, or $B_{\textrm{MSY}}$, the biomass at Maximum Sustainable Yield (MSY) [@Schaefer1954].
When the population growth rate is stochastic, the problem is slightly harder to solve, as the harvest quota must constantly adjust to the ups and downs of stochastic growth, but it is still possible to show the optimal strategy merely seeks to maintain the stock at $B_{\textrm{MSY}}$, adjusted for any discounting of future yields [@Reed1979]. <!-- ML: Will probably want to de-emphasize this given the momentum to sigma of 0 -->

This provides a natural first benchmark for deep RL approaches, since we can compare the RL solution to the mathematical optimum directly.
The problem is also of interest because the RL approach can be extended to consider ecological dynamics that either surpass the complexity possible with SDP methods, or violate assumptions of such classical approaches).
Moreover, fisheries management is both an important global challenge in it's own right [@Worm2006; @Costello2016] as well as a common test case to understand ecological dynamics and conservation management more broadly [@something].

For illustrative purposes, we consider the simplest version of the dynamic optimal harvest problem as outlined by [@Clark1973] (for the deterministic case) and [@Reed1979] (under stochastic recruitment).
In short, the manager seeks to optimize the net present value (discounted cumulative catch) of a fishery, observing the stock size each year and setting an appropriate harvest quota in response.
In the classical approach, the best model of the fish population dynamics must first be estimated from data, potentially with posterior distributions over parameter estimates reflecting any uncertainty.
From this model, the optimal harvest policy -- that is, the function which returns the optimal quota for each possible observed stock size -- can be determined either by analytic [@Reed1979] or numerical [@Marescot2013] methods, depending on the complexity of the model.
In a RL approach, the model is instead replaced by a computer simulation, usually referred to as an "environment." The simplest simulation is merely an implementation of the estimated model itself.
However, unlike an analytic or SDP solution that is derived directly from the estimated model, the RL approach makes no assumption about the precise functional form or parameter values underlying the dynamics -- it is in principle agnostic to the details of the simulation. <!-- ML: I think this sentence should be tightened up re RL, like one could use a model-based algorithm which uses an inputted model under the RL umbrella-->

We will illustrate the potential differences between two RL algorithms used to train the agents, one based on the "Advantage Actor Critic" algorithm, usually referred to as A2C [@A2C], and one based on Twin Delayed Deep Deterministic Policy Gradient, commonly known as TD3 [@TD3] (a generalization of Deep Deterministic Policy Gradient, or DDPG, which extends the DQN algorithm @DQN to a continuous action space using the deterministic policy gradient).

A step-by-step walk-through for training both agents on this environment is provided in the Appendix.
We compare the resulting management, policy, and reward under either RL agent to that achieved by the optimal management solution [Fig 2].
Despite having no knowledge of the underlying model and clear differences in the resulting strategies, both RL agents learn enough to achieve nearly optimal performance.


```{r message=FALSE, include=FALSE}
sims_df <- read_csv( "figs/sims_df.csv")
policy_df <- read_csv("figs/policy_df.csv")
reward_df <- read_csv("figs/reward_df.csv")

ymin <- function(x) last(x[(ntile(x, 20)==1)])
ymax <- function(x) last(x[(ntile(x, 20)==19)])

fig_sims <- 
sims_df %>% 
  group_by(time, model) %>% 
  summarise(ymin = ymin(state),
            ymax = ymax(state),
            state = mean(state), .groups = "drop") %>%
  ggplot(aes(time, state, ymin = ymin, ymax = ymax, fill=model)) + 
  geom_ribbon(alpha= 0.3) + geom_line(aes(col = model))

fig_policy <- 
  policy_df %>% ggplot(aes(state, state - action, 
                           group=interaction(rep, model),
                           col = model)) + 
  geom_line(show.legend = FALSE) + 
  coord_cartesian(xlim = c(0, 1.3), ylim=c(0,0.9)) + 
  ylab("escapement")


fig_reward <- reward_df %>% 
  ggplot(aes(time, mean_reward)) + 
  geom_ribbon(aes(ymin = mean_reward - 2*sd, 
                  ymax = mean_reward + 2*sd, fill = model),
              alpha=0.25, show.legend = FALSE) + 
  geom_line(aes(col = model), show.legend = FALSE) + 
  ylab("reward")
```

```{r fig_fish, fig.cap="Fisheries management using neural network agents trained with RL algorithms A2C and TD3 compared to optimal management.  Top panel: mean fish population size over time across 100 replicates. Shaded region shows the 95% confidence interval over simulations.  Lower left: The optimal solution is policy of constant escapement. Below the target escapement of 0.5, no harvest occurs, while any stock above that level is immediately harvested back down (red line).  RL agents both adopt a policies that cease any harvest below this level, while allowing a somewhat higher escapement than optimal just above the optimal escapement level.  A2C policy is non-deterministic, and overharvest from very high stock sizes.  Lower Right: Mean cumulative reward over time across simulations. TD3 achieves nearly-optimal mean utility. Mean reward under A2C is lower, though the confidence interval still overlaps the mean reward of optimal management."}
fig_sims / ( fig_policy + fig_reward)
```

Note that the A2C policy learns to shut down fishing entirely below a certain stock size, without fail.
A2C also keeps the fishery closed even once stock sizes are slightly higher than $B_{MSY}$.
Once it begins fishing, it slightly under-harvests for stock sizes a little over $B_{MSY}$, but tends to over-harvest for very large stock sizes (rarely ever realized, as fishing pressure is enough to keep it out of this range).
Note also that the harvest varies across replicates, creating a fuzzy region in the policy function.
Under such a policy, A2C maintains a stock size on average significantly above $B_{MSY}$, though also sees higher volatility particularly on the low side.
Note that due to the stochastic nature of the environment, any given replicate A2C may out-perform the optimal policy, though on average the optimal policy provides a higher reward.

These features highlight a common challenge in the design and training of RL algorithms.
RL cares only about improving the realized cumulative reward, and may sometimes achieve this in unexpected ways.
Because these simulations rarely take stock sizes close to a state of 1.0, the heavy overfishing behavior A2C has in this range has little consequence.
Likewise it is more difficult for TD3 to learn the appropriate harvest level at these high states that are rarely sampled in the course of training.
Just as intuition would suggest, it is possible to devise alternative training schemes which may do a better job sampling the simulation dynamics at high stock levels.
Training these agents in a variety of alternative contexts can improve there ability to generalize to other scenarios.

## Ecological Tipping Points

Our second example focuses on a case for which we do not already have an existing, provably optimal policy to compare against.
We consider the generic problem of an ecosystem facing slowly deteriorating environmental conditions which move the dynamics closer towards a tipping point [Fig 2].
This model of a critical transition has been posited widely in ecological systems, from the simple consumer-resource model of [@May1977] on which our dynamics are based, to microbial dynamics [@Dai2012] lake ecosystem communities [@Carpenter2011] to planetary ecosystems [@Barnosky2012].
On top of these ecological dynamics we introduce an explicit ecosystem service model quantifying the value of more desirable 'high' state relative to the 'low' state.
For simplicity, we assume a proportional benefit $b$ associated with the ecosystem state $X(t)$.
Thus when the ecosystem is near the 'high' equilibrium $\hat X_H$, the corresponding ecosystem benefit $b \hat X_H$ is higher than at the low equilibrium, $b x_L$, consistent with the intuitive description of ecosystem tipping points [@Barnosky2012].
We also enumerate the possible actions which a manager may take in response to environmental degradation.
In the absence of any management response, we assume the environment deteriorates at a fixed rate $\alpha$, which can be thought of as the incremental increase in global mean temperature or similar anthropogenic forcing term.
Management can respond to slow or even reverse this trend by choosing an opposing action $A_t$.
We assume that large actions are proportionally more costly than small actions, consistent with the expectation of diminishing returns: taking the cost associated with an action $A_t$ as equal to $c A_t^2$.
Many alterations of these basic assumptions are also possible: our `gym_conservation` implements a range of different scenarios with user-configurable settings.
In each case, the manager observes the current state of the system each year and must then select the policy response that year.

```{r message=FALSE, fig.cap="Bifurcation diagram for tipping point scenario. The ecosystem begins in the desirable 'high' state under an evironmental parameter (e.g. global mean temperature, arbitrary units) of 0.19.  In the absence of conservation action, the environment worsens (e.g. rising mean temperature) as the parameter increases.  This results in only a slow degredation of the stable state, until the parameter crosses the tipping point threshold at about 0.215, where the upper stable branch is anihilated in a fold bifurcation and the system rapidly transitions to lower stable branch, around state of 0.1.   Recovery to the upper branch requires a much greater conservation investment, reducing the parameter all the way to 0.165 where the reverse bifurcation will carry it back to the upper stable branch."}
bifur_df <- read_csv("figs/bifur.csv", col_types = "dccd")
bifur_df %>%
  ggplot(aes(parameter, state, lty=equilibrium, group = group)) + 
  geom_line()

```

Because this problem involves a parameter whose value changes over time (the slowly deteriorating environment), the resulting ecosystem dynamics are not autonomous.
This precludes our ability to solve for the optimal management policy using classical theory such as for Markov Decision Processes (MDP, [@Marescot2013]), typically used to solve sequential decision-making problems.
However, it is often argued that simple rules can achieve nearly optimal management of ecological conservation objectives in many cases [@Possingham; @Possingham; @morePossingham].
A common conservation strategy employs a fixed response level rather than a dynamic policy which is toggled up or down each year: for example, declaring certain regions as protected areas in perpetuity.
An intuitive strategy faced with an ecosystem tipping point would be 'perfect conservation', in which the management response is perfectly calibrated to counter-balance any further decline.
While the precise rate of such decline may not be known in practice (and will not be known to RL algorithms before-hand either), it is easy to implement such a policy in simulation for comparative purposes.
We compare this rule-of-thumb to the optimal policy found by training an agent using the TD3 algorithm.

```{r}
sims_df <-read_csv("figs/tipping_sims_df.csv")
policy_df <- read_csv("figs/tipping_policy_df.csv")
reward_df <- read_csv("figs/tipping_reward_df.csv")
```

```{r fig.cap="Ecosystem dynamics under management using the steady-state rule-of-thumb strategy compared to management using a neural network trained using the TD3 RL algorithm.  Top panel: mean and 95% confidence interval of ecosystem state over 100 replicate simulations.  As more replicates cross the tipping point threshold under steady-state strategy, the mean slowly decreases, while the TD3 agent preserves most replicates safely above the tipping point.  Lower left: the policy function learned using TD3 relative to the policy function under the steady state.  Lower right: mean rewards under TD3 management evenutally exceed those expected under the steady state strategy as a large initial investment in conservation eventually pays off."}
fig_sims <- 
  sims_df %>%
   group_by(time, model) %>% 
   summarise(ymin = ymin(state),
            ymax = ymax(state),
            state = mean(state), .groups = "drop") %>%
  ggplot(aes(time, state, ymin = ymin, ymax = ymax, fill=model)) + 
   geom_ribbon(alpha= 0.3) + geom_line(aes(col = model))

fig_policy <- 
  policy_df %>% ggplot(aes(state, action, 
                           group=interaction(rep, model),
                           col = model)) + 
  geom_line(show.legend = FALSE) 

fig_reward <- reward_df %>% 
  ggplot(aes(time, mean_reward)) + 
  geom_ribbon(aes(ymin = mean_reward - 2*sd, 
                  ymax = mean_reward + 2*sd, fill = model),
              alpha=0.25, show.legend = FALSE) + 
  geom_line(aes(col = model), show.legend = FALSE) + 
  ylab("reward")
```

```{r fig_conservation, fig.cap=""}
fig_sims / ( fig_policy + fig_reward)
```

The TD3-trained agent proves far more successful in preventing chance transitions across the tipping point, consistently achieving a higher cumulative ecosystem service value across replicates than the steady-state strategy.

Examining the replicate management trajectories and corresponding rewards [Fig 3] reveal that the RL agent incurs significantly higher costs in the initial phases of the simulation, dipping the cumulative reward into negative territory on average and well below the reward realized by the simple steady-state strategy.
This initial investment then begins to pay off -- by about the 200th time step the RL agent has surpassed the performance of the steady state strategy.
The policy plot provides more intuition for the RL agent's strategy: at very high state values, the RL agent opts for no conservation action -- so far from the tipping point, no response is required.
Near the tipping point, the RL agent steeply ramps up the conservation effort, and retains this effort even as the system falls below the critical threshold, where a sufficiently aggressive response can tip the system back into recovery.
For a system at or very close to the zero-state, the RL agent gives up, opting for no action.
Recall that the quadratic scaling of cost makes the rapid response of the TD3 agent much more costly to achieve the same net environmental improvement divided into smaller increments over a longer timeline.
However, our RL agent has discovered that the extra investment for a rapid response is well justified as the risk of crossing a tipping point increases.

A close examination of the trajectories of individual simulations which cross the tipping point under either management strategy (see appendix B, fig S1, S2) further highlights the difference between these two approaches.
Under the steady-state strategy, the system remains poised too close to the tipping point: stochastic noise eventually drives most replicates across the threshold, where the steady-state strategy is too weak to bring them back once they collapse.
As replicate after replicate stochastically crashes, the mean state and mean reward bend increasingly downwards.
In contrast, the RL agent edges the system slightly farther away from the tipping point, decreasing but not eliminating the chance of a chance transition.
In the few replicates that experience a critical transition anyway, the RL agent usually responds with sufficient commitment to ensure their recovery (Fig S2).
Only 3 out of 100 replicates degrade far enough for the RL agent to give up the high cost of trying to rescue them.

Such examination makes it clear how the RL agent's use of a more dynamic strategy out-preforms the steady-state strategy.
The various kinks in the RL policy function also suggest that this solution is nevertheless surely not the optimal solution -- such features are typical of RL-based policies, where such spandral-like features from early experimentation may become baked into the long-term strategy if adjusting them has little improvement on the final reward. <!-- ML: the parenthetical here I understand but I feel a little ehh on the word choice. I also don't know how to rephrase it -->
Further tuning of hyper-parameters, increased training, alterations or alternatives to the training algorithm would likely be able to further improve upon this performance.

## Unsolved environments: Wildfires and other examples

<!-- ML: Need to rework this to talk about other environments that we are working on basically point to conservation gym website -->
One feature that most distinguishes RL from other approaches to decision theory (such as optimal control) is it's flexibility or ambition.
While optimal control approaches typically require extensive simplifying assumptions such as discrete states or actions, deterministic dynamics or short time horizons, RL is scoped more ambitiously.
Large, complex problems are still immensely challenging to RL, and existing RL approaches will frequently struggle to solve problems that can already be solved much more efficiently by other means.
Nevertheless, deep RL approaches using neural network architectures have proven successful on problems that no amount of computational power would put into reach of purely conventional methods.
Our previous examples have focused on relatively simple examples that can also be addressed at least approximately by conventional methods, and also permit RL-based solutions to be found relatively efficiently on commodity hardware.
To better illustrate the generality of RL approach and more typical complexity of conservation problems, we now introduce a third example for which we have not yet found an effective policy using RL or classical methods.

<!-- Marcus add ~ paragraph discussion of the fire gym, citing motivating examples, basic overview of the state space, action space and reward and a screenshot of the simulation as a figure? -->

# Discussion

<!-- Ecology has complex decisions and we lack necessary tools; RL could be it -->
Ecological challenges facing the planet today are complex and outcomes are both uncertain and consequential.
Even our best models and best research will never provide a crystal ball to the future, only better elucidate possible scenarios.
Consequently, that research must also confront the challenge of making robust, resilient decisions in a changing world.
The science of ecological management and quantitative decision-making has a long history [e.g. @Schaefer1954; @Walters1978] and remains an active area of research [@Possingham2006; @Fischer2009; @Polasky2011].
However, the limitations of classical methods such as optimal control frequently constrain applications to relatively simplified models [@Possingham2006], ignoring elements such as spatial or temporal heterogeneity and autocorrelation, stochasticity, imperfect observations, age or state structure or other sources of complexity that are both pervasive and influential on ecological dynamics [@Hastings2012].
Complexity comes not only from the ecological processes but also the available actions.
Conservation efforts can be implemented in a wide variety of ways, and the most effective policies may vary in both space and time.
While such complex dynamics and large state and action spaces are intractable to classical methods, they are straight forward to implement in simulation environments such as those used to train an RL algorithm.
Neural networks used in deep RL have proven remarkably effective in handling such complexity, particularly when leveraging immense computing resources increasingly available through advances in hardware and software [@gpu_computing]. 

<!-- Model free is great bc we don't know the true models -->
The rapidly expanding class of Model-free RL algorithms is particularly appealing given the ubiquitous presence of model uncertainty in ecological dynamics.
Rarely do we know the underlying functional forms for ecological processes -- is population recruitment more Beverton-Holt or Ricker?
Methods which must first assume something about the structure or functional form of a process before estimating the corresponding parameter can only ever be as good as those structural assumptions.
Frequently available ecological data is insufficient to distinguish between possible alternative models [@devalpine], or the correct model may be non-identifiable with any amount of data.
Model-free RL approaches offer a powerful way to factor this thorny issue.
The model-free RL agent needs to make no such a priori hypothesis about a particular functional form.
Model-free approaches have proven successful at learning effective policies even when the underlying model may be difficult or impossible to learn [@Pong2020], as long as simulations of possible mechanisms are available.

<!-- Real problems are harder than these examples -->
<!-- ML: I feel like this paragraph could be condensed with the one above -->
<!-- CB: I think "model free is great" and "real problems are harder than these examples" are two distinct messages though and at least one would get lost in mashing them together.  Perhaps the message doesn't come across in the current version.  Does look like this paragraph is similar in overall message to the next one though, maybe could be combined with that?  -->
The examples presented here only scrape the surface of possible RL applications to conservation problems.
The examples we have focused on are intentionally quite simple, though it is worth remembering that these very same simple models have a long history of relevance and application in both research and policy contexts.
Despite their simplicity, the optimal strategy is not always obvious before hand, however intuitive it may appear in retrospect.
In the case of the ecosystem tipping point scenario, the optimal strategy is unknown, and the approximate solution found by our RL implementation could almost certainly be improved upon.
Even at it's best, an RL-trained agent can only be as good as the scenarios on which it has been trained. <!-- ML: A counterpoint here is that a model-based agent may learn a correct model that generalizes outside of the examples it has seen-->
In these simple examples in which the simulation implements a single model, training is analogous to classical methods which take the model as given [@Marescot2013].
But classical approaches can be difficult to generalize when the underlying model is unknown: alternative models may require a completely different approach.
In contrast, the process of training an RL algorithm on a more complex problem is no different than the simple one: we only need access to a simulation which can generate plausible future states in response to possible actions.
Many of our most realistic ecological models exist only as numerical simulations of lower-level dynamics, such as the individual-based models used in the management of forests and wildfire [@sortie; @wildfire], disease [@covid], marine ecosystems [@ecopath], or global climate change [@dice].


<!-- Solving 'real' problems will be hard -->
Successfully applying RL to complex ecological problems is no easy task.
The flexibility of the approach makes it relatively straightforward to include realistic biology and environmental science in framing an RL problem.
Yet, even on relatively uncomplicated environments, training an RL agent can be more challenging than expected due to an entanglement of reasons like hyperparameter instability and poor exploration that can be very difficult to resolve [@drlthatmatters].
As the examples we have presented illustrate, it is important to begin with simple problems, including those for which an optimal strategy is already known.
Such examples provide important benchmarks against which to calibrate the performance, tuning and training requirements of RL,
and can easily be extended into more complex problems once RL agents can master the basics.
In the case that an agent has been trained to perform well on a realistic environment, there will still be a range of open questions in using this agent to inform decision-making: 

- Since deep neural networks lack transparency [@castelvecchi_can_2016], can we be confident that the deep RL agent will generalize its past experience to new situations?

- Given that there have been many examples of reward misspecification leading to undesirable behavior [@hadfield-menell_inverse_2020], what if we have selected an objective that unexpectedly causes damaging behavior?


<!-- But never fear, hard problems have been solved before!-->
Deep RL is still a very young field, where despite several landmark successes, potential far outstrips practice.
Recent advances in the field have proven the potential of the approach to solve complex problems [@alphaGo2016; @alphaGoZero; @alphazero; @DQN], but typically leveraging large teams with decades of experience in ML and millions of dollars worth of computing power [@alphaGoZero].
Successes have so far been concentrated in applications to games and robotics, not scientific and policy domains, though this is beginning to change [@protein_folding].
Leading developers of new deep RL algorithms benchmark the performance of their algorithms against such problems in part because simulations of games and robotic movement are readily available through open source frameworks designed to interface with RL algorithms [brockman2016], and performance can easily be compared against alternatives on public leaderboards [brockman2016].
Iterative improvements to well-posed public challenges have proven immensely effective in the computer science community in tackling difficult problems, which allow many teams with diverse expertise not only to compete but to learn from each other [@netflix_prize; @imagenet].
Advances in both computational power and algorithmic efficiency continue to reduce costs at a dramatic pace, significantly faster than Moore's law [@openai_costs].
By working to develop similarly well-posed challenges as clear benchmarks, ecology and environmental science researchers may both be better able to replicate that collaborative, iterative success in cracking hard problems.
Such benchmarks may further improve collaboration across disciplines as well, attracting the attention of AI expertise to these key issues facing our planet.
If RL opens the door to greater role for artificial intelligence and engagement of technology firms in the machinery that determines environmental policy, this raises new concerns as well as new opportunities.  


<!-- transition from big promise to perils -- Millie's section on risks here -->

<!-- from end of Carl's discussion edits: Such benchmarks may further improve collaboration across disciplines as well, attracting the attention of AI expertise to these key issues facing our planet. -->
<!-- If RL opens the door to greater role for artificial intelligence and engagement of technology firms in the machinery that determines environmental policy, this raises new concerns as well as new opportunities.   -->

The increasingly influential role of AI in sectors as diverse as policing, finance, and employment has raised concern about the ethics, biases, and power entrenched in both decision-making algorithms and their outputs [@kalluri2020]. 
As algorithms play a greater role in environmental and conservation policy, will similar issues arise?
While this question has garnered  recent interest in the academic literature [@wearn2019; @adams2019; @scoville2021], the use of decision making algorithms in environmental contexts is still relatively isolated and in it's early stages.
Examples of AI applications, and corresponding concerns, have largely been confined to the context of enforcing environmental laws (i.e. algorithms which predict wildlife poaching events [@adams2019] or identify illegal fishing behavior [@mcdonald2021; @swartz2021]). 
However, more diverse applications of AI algorithms - such as applications which contribute to meeting sustainable development goals [@vinuesa2020] or designing  protected area networks - raise additional concerns of entrenching power asymmetries between those reliant on natural resources and those designing algorithms and their objectives.
These issues of power become particularly acute when private industry holds the keys to propriety algorithms or environmental data, and generally operate without the same community engagement obligations of public agencies. 
In these cases, good intentions might not be sufficient to ensure equitable outcomes. 
Interdisciplinary collaborations with experts in algorithmic governance and ethics will be important ensure that applications of AI to ecological challenges promotes equity in both the procedural process of developing objective functions and the distribution of benefits that result from decisions.



<!-- Something about the way forward, point to appendix resources etc -->


<!-- Discussion really doesn't discuss/draw conclusions from the fisheries and tipping points examples! Do we need to or maybe not since it's not about the science-->


<!-- ML: I think talking about the examples in a very passing manner is fine as the point is primarily to show that RL can work. To conclude, I feel like we could portray the vision of open ai gym environments building a CS-ecology community.-->



<!-- 
The pitfalls of RL-based solutions to global environmental change problems are not only technical, but include issues of ethics and power, especially when the algorithms or data are proprietary.
We discuss not only technical issues that can cause RL solutions to fail in surprising ways, but also potential issues raised by the adoption of more algorithmic approaches to conservation generally.
We emphasize how an open, transparent and reproducible approach can help mitigate some (but not all) of these concerns, and facilitate a more effective interface between teams of researchers from both ecological and computer sciences.
If algorithms are to play a meaningful role in the allocation and exploitation of natural resources from fish to forests, good intentions [@Joppa] may not be sufficient to ensure equitable outcomes.
Ecologists and computer scientists will also need input from experts in governance and ethics to ensure that applications of AI to ecological challenges are fair and just.

-->
