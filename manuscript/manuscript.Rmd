---
title: Deep Reinforcement Learning for Conservation Decisions
authors:
    
  - name: Marcus Lapeyrolerie
    department: Department of Environmental Science, Policy, and Management
    affiliation: University of California, Berkeley
    location: Berkeley, California
  - name: Melissa Chapman
    department: Department of Environmental Science, Policy, and Management
    affiliation: University of California, Berkeley
    location: Berkeley, California
  - name: Kari Norman
    department: Department of Environmental Science, Policy, and Management
    affiliation: University of California, Berkeley
    location: Berkeley, California
  - name: Carl Boettiger
    department: Department of Environmental Science, Policy, and Management
    affiliation: University of California, Berkeley
    location: Berkeley, California
    email: cboettig@berkeley.edu
abstract: |
  Enter the text of your abstract here.
keywords:
  - blah
  - blee
  - bloo
  - these are optional and can be removed
bibliography: references.bib
header-includes:
  - \usepackage{amsmath}
  - \usepackage{amssymb}
output: 
  rticles::arxiv_article:
    keep_tex: true
editor_options: 
  markdown: 
    wrap: sentence
---


# Introduction

<!-- 
- Global change problems: Important but tough decisions
- ML: exciting stuff, but only fitting data is driving through the rearview mirror
- RL is the ML for decisions (in dynamic/uncertain environments) 
- RL can use existing knowledge and models to improve decisions in complex/uncertain scenarios (read: global change biology)
- Here be dragons / map paragraph

-->


<!-- Environmental problems, ML -->
Advances in both available data and computing power have begun to open the door to a greater role for machine learning (ML) in addressing some of our planet’s most pressing environmental problems, such as the growing frequency and intensity of wildfire [@wildfire], over-exploited fisheries [@worm2006], declining biodiversity [@], and zoonotic pandemics [@covid].
But will ML approaches really help us tackle a changing planet?
Applications of ML in ecology have begun to illustrate the promise of two methods of ML: *supervised learning* (e.g. image recognition) and *unsupervised learning* (including clustering and prediction), but have so far largely overlooked the third and possibly most promising approach in the ML triad: Reinforcement Learning (RL).
Three features distinguish RL from other ML methods in ways that are particularly well suited to addressing global ecological change issues:

1) RL is explicitly focused on the task of selecting actions in an uncertain and changing environment to maximize some objective,
2) RL does not require massive amounts of representative sampled historical data,
3) RL approaches easily integrate with existing ecological models and simulations, which may be our best guide to understanding and predicting future possibilities.

Despite relevance to uncertainty and decision making that could make RL uniquely well suited for ecological and conservation problems, it has so far seen little application in this area.
To date, the uncertain and changing environments considered by RL research have largely been drawn from examples in robotic movement and video games.
RL agents have recently achieved superhuman performance on classic Atari games [@DQN], while Deepmind’s RL-trained AlphaZero has defeated the world's grandmasters in Go, Chess, and Shogi [@alphazero].
Complex environmental problems share many similarities to these games: the need to plan many moves ahead, accept short-term costs to achieve long-term benefit, to account for uncertainty and respond with contingency to the unexpected.
Of course real ecosystems are also far more complex than a game of chess or MoonLander -- but can the same be said of our efforts at finding the best strategy?
Rich, processes-based simulations such as the SORTIE model in forest management [@sortie], Ecopath with Ecosim in fisheries management [@ecopath], or global circulation models (GCMs) in climate [@gcm] are already used to explore scenarios and inform ecosystem management. 
Even simpler models are frequently employed throughout ecology to guide understanding and response.
<!-- Kari: ick, jargon heavy.  Need to say: some existing methods, but limited to simple problems or simple solutions -->
Decision-theoretic approaches based on optimal control techniques can only find the best strategy in the simplest of models, the so called "curse of dimensionality" makes problems with too many possible states or actions intractable by conventional methods such as stochastic dynamic programming (SDP) and approximate dynamic programming (ADP) [@Polasky2011; @Marescot2013].
Neural-network-based ML techniques have proven particularly effective in problems involving complex, high-dimensional spaces that have previously proven intractable to classical methods.
We believe that the combination of such approaches from RL trained using sufficient simulations of suitable ecological processes will open the door to a new paradigm for tackling tough environmental change problems.
We also believe that such a transformation could bring risks and pitfalls comparable and varied as its solutions.

In this paper, we draw on examples from fisheries management, ecological tipping points, and wildfire spread to illustrate how deep RL techniques can successfully discover optimal solutions to previously solved management scenarios and discover highly effective solutions to unsolved problems.
Our simplest example approaches the known optimal solution for a classic fisheries management problem with no prior knowledge of the underlying model.
Our second example considers ecosystem management under slow environmental degradation: a scenario which is not amenable to classic optimization methods such as dynamic programming.
In this case there is no existing optimal solution to compare against, but an appropriately tuned RL agent can out-perform a sensible rule-of-thumb response.
In our third example simulating wildfire spread highlights an open problem which we expect will be considerably harder to find effective solutions.
These examples demonstrate that RL-based approaches are capable but by no means a magic bullet: reasonable solutions require careful design of training environments, choice of RL algorithms, tuning and evaluation, as well as substantial computational power.
Our examples are intentionally simple, aiming to provide a clear template for understanding that could be easily extended to cover more realistic conditions. 
We include an extensive appendix with carefully annotated code which should allow readers to both reproduce and extend this analysis.
We further include implementations of three fully featured simulation modules following current standards for RL benchmark problems as Python modules distributed on PyPi and GitHub.
Each module includes several more challenging variations to the examples described here.
We intend to continue to expand this library of ecological management benchmarks with the hope both of challenging researchers already working in RL to tackle some of these pressing issues and for ecologists and environmental scientists to continue to refine these benchmarks into more realistic ecological simulations.

The pitfalls of RL-based solutions to global environmental change problems are not only technical, but include issues of ethics and power, especially when the algorithms or data are proprietary.
We discuss not only technical issues that can cause RL solutions to fail in surprising ways, but also potential issues raised by the adoption of more algorithmic approaches to conservation generally.
We emphasize how an open, transparent and reproducible approach can help mitigate some (but not all) of these concerns, and facilitate a more effective interface between teams of researchers from both ecological and computer sciences.
If algorithms are to play a meaningful role in the allocation and exploitation of natural resources from fish to forests, good intentions [@Joppa] may not be sufficient to ensure equitable outcomes.
Ecologists and computer scientists will also need input from experts in governance and ethics to ensure that applications of AI to ecological challenges are fair and just.

# RL overview

<!-- Conceptual picture: (A) an "environment", and (B) an "agent"

What do we mean by an "environment"?  
  - Transition function
  - "reward"
  - examples
  
  
What do we mean by an "agent"?

  Model-based vs Model free.  We focus on Model free agents which use Neural networks, a subset of deep RL

  Components of an agent: 
    - *neural network*, whose *parameters* are estimated by *training* +
    - an RL algorithm, whose *hyperparameters* (including NN structure) are estimated by *tuning*
    - High-level overview of an RL algorithm components (model/model-free, on/off policy, stochastic/deterministic, etc).

  conceptual explanation of 'model-free', what it means to have a "policy" without modeling/predicting transitions

-->

Any application of RL can be divided into two components: an *environment* and an *agent*.
An *environment* is typically a computer simulation, though it is certainly possible to learn from real world environments, such as teaching physical robots to walk and navigate obstacles.
An *agent* is typically a computer algorithm which takes sequential observations from the environment and returns proposed actions. RL agents typically use one or more neural networks to represent the relationship between observed states and suitable actions, and 'train' these networks through repeated interactions with the environment to maximize the culumulative reward.
<!-- Blah blah skip this?-->
It is important to recognize that these definitions are significantly more general than requirements typically imposed by classical methods.  
For instance, there is no a priori assumption that the environment is *fully observed*, or that the environment is *Markovian* (future states depend only on the current state), commonly assumed by classical methods [e.g. @Marescot2013]. 
Just as important is the high degree of interoperability provided by the RL framework.
As we shall see, an RL algorithm developed to play Atari games can instead be trained on an environment that simulates fish population dynamics without modification.
(Note that this does not refer to the *trained* agent).
This not guarantee that such an algorithm will be successful (even after extensive tuning and training in the fish population dynamics environment), but does open the door to more rapid exchange between fields.
In particular, the adoption of standard interfaces for RL environments [@brockman2016] as well as the implementation of cutting edge algorithms in flexible open source frameworks [@sb3; @tf-agents] means that a researcher can test a range of RL algorithms against a single environment, or a given algorithm across a range of environments with equal ease.



## Designing and building an environment

An environment is a mathematical function, computer program, or real world experience that takes an agent's proposed *action* as input and returns an observation of it's current *state* and associated *reward* as output.
In contrast to classical approaches, there are few restrictions on what comprises a state or action, at least in principle.
The state may be continuous or discrete, completely or only partially observed, one or many dimensions.
For instance, we might represent a ecological community of 10 interacting species using an environment with 10 continuously valued states representing the possible population densities of each species.
The action space may similarly complex, and the response of the environment to any possible action may be deterministic or stochastic in nature.
The reward will in general depend on both the current state and choice of actions.
An environment must also specify a stopping condition, such as when a particular state is reached or a pre-defined interval of time has elapsed.
In some environments, a reward is only received after the agent successfully completes a potentially long sequence of complex actions, such as solving a maze.
In other environments, an agent may have to learn to forgo immediate reward (or even accept an initial negative reward or cost) in order to maximize the net discounted reward over the simulation period, as we illustrate in examples here.



@brockman2016 identified (1) the lack of standardization of environments used in publication and (2) the need for better benchmark environments as two core problems limiting more rapid advancement of RL research, and proposed the creation of the OpenAI `gym` framework to address both problems.
The `gym` framework defines a standard interface and methods by which a developer can describe an arbitrary environment in a computer program.
This allows the design of software agents which can interact and learn in that environment without knowing anything about its internal details.
The `gym` python module created by OpenAI also provides over 100 pre-built environments that serve as benchmark problems against which new RL algorithms can be tested.
The `gym` framework is now recognized by all major software frameworks for writing RL algorithms (see Appendix, 'Deep RL Frameworks').
While this library of environments span a range of difficulties from classic control problems that can be solved in closed form to complex three-dimensional robotics simulators, they include no examples of ecological or biological problems.
We use the `gym` framework to turn several existing ecological models into valid environmental simulators that can similarly be used in any RL framework.


## Designing and training an agent

The training an agent on an environment involves allowing the agent to experience and respond to repeated simulations from the environment, thousands or even millions of times. 
Over the course of training, weights on the neural network are updated in response to the reward given by the environment, much like reinforcement learning is believed to work in living creatures [@cite].
A neural network can be thought of as a generic function approximator, and these weights as parameters of that function that are estimated from 'data' generated by training.
A typical neural network in RL may have thousands or hundreds of thousands of parameters, giving it the flexibility to learn very complex behaviors.
The overall architecture of the neural network, as well as other parameters such as those governing how an agent learns, how an agent proposes actions, and how the agent discounts future rewards, and so forth are controlled by *hyperparameters*.
To initialize a new, untrained agent, we must specify a handful or so of these hyperparameters, which are held fixed over the course of training.
The particular selection of hyperparameters required depends on which *RL algorithm* our agent will be based.
The RL algorithm specifies precisely how the agent will learn from its environment:
Is the algorithm model-based or model-free? 
Stochastic or deterministic? 
"Off-policy" or "on-policy"? 
Does it use a random noise model to propose actions?
Does it permit a continuous action space or require discrete actions?
The design of RL algorithms has been and remains an area of intense research and rapid development [Table 2].
Open source implementations by professional software development teams often follow swiftly after new algorithms are published [@Ssb3; @tf-agents].
In the following section, we summarize some of the key concepts that make up an RL algorithm, and highlight relevant distinctions between important algorithms today.


<!-- Something about given RL algorithm being able to solve lots of different environments with no pre-specified knowledge. cite examples @AlphaZero and @DQN; but still depends on hyperparameters -->
The process of selecting hyperparameters for a given RL algorithm to maximize it's performance on a given environment is referred to as hyperparameter *tuning*.
The process of tuning can be automated through standard multi-dimensional optimization approaches: proposing new combinations of hyperparameters, training an agent, and examining the resulting performance.
Because training even on a fixed set of hyperparameters can be computationally intensive, tuning must be targeted appropriately.
Considerations in how the environment simulation is implemented, such as the normalization of state and/or action spaces, can significantly improve performance of many RL algorithms on a diverse range of problems without requiring extensive tuning.


<!--
A rapidly growing set of material for learning about deep reinforcement learning is now freely available.... (maybe just describe this in the appendix)
-->


<!-- 
Placeholder table as figure, rather hacky (pandoc doesn't like `longtable`) 
Make a nice table box in latex instead if we're keeping this
-->
\begin{figure}
\centering
\includegraphics[width=0.9\linewidth]{table1.pdf}
\end{figure}


# RL Algorithms

<!-- 

I think we can lean more heavily both in references to and paralleling the presentation of some of the existing background material. 



-->

The basic problem set-up of RL is as follows: at a time step, $t$, the agent observes a representation of the environment's state, $s_t$, and must accordingly select an action, $a_t$; at the next time step, the agent will observe a new state, $s_{t+1}$, and will receive a numerical reward, $r_{t+1}$.
This new state, $s_{t+1}$, and reward, $r_{t+1}$, are determined by the environment.
The task for the agent is to learn a *policy*, $\pi$, that prescribes what action the agent should perform after observing a certain state.
The agent interacts with the environment via trial and error in what is called the *training* period, during which the agent continually updates its policy in order to maximize its expected cumulative sum of rewards.


<!-- Still can't decide if this statement is too strong; i.e. that at least theoretically / LSTM, RL permits learning processes which are not Markovian in the observed variables -->
When the environment is fully observable, i.e. when the state that the agent observes fully characterizes the system, the reinforcement learning problem can be formalized as a Markov Decision Process (MDP).
An MDP consists of the following: a set of states, $\mathcal{S}$, called the state space; a set of actions, $\mathcal{A}$, called the action space; a state transition operator, $T(s_{t+1}|s_t, a_t)$, which describes the dynamics of the system; a reward function, $r(s_t, a_t)$; an initial state distribution, $d_0(s_0)$; and a discount factor, $\gamma \in (0,1]$. As the agent interacts with the environment, the agent will create a trajectory in state-action space given as $\tau = (s_0, a_0, \dots, s_H, a_H)$ where H denotes the horizon or length of the state-action sequence.
From this definition, we can provide an agent's trajectory distribution for a given policy as,

$$
  p_\pi(\tau) = d_0(s_0) \prod_{t=0}^H \pi(a_t \mid s_t) T(s_{t+1} \mid s_t, a_t)
$$

When the agent cannot fully observe the state of the system, reinforcement learning can then be described by a Partially Observable Markov Decision Process (POMDP), wherein there is an additional distribution, $E(o_{t}| s_{t})$, which accounts for the observation being different from system's actual state.
An important property of POMDPs and MDPs is that they are memoryless; the future, $s_{t+1}$, does not depend on the past but only on the state and action at the present time.
This Markov property, which is often an unrealistic assumption, is held in most of the RL algorithms that we will discuss.

In formal terms, the goal of reinforcement learning is to find an optimal policy distribution, $\pi^*(a_t | s_t)$ or $\pi^*(a_t|o_t)$ for MDP and POMDP cases respectively, that maximizes the expected cumulative, discounted reward:

$$
  \pi^* = \underset{\pi}{\text{argmax}} \,\, \mathbb{E}_{\tau \sim p_\pi(\tau)}\Big[\sum_{t=0}^{H} \gamma^t r(s_t, a_t) \Big]
$$

Note that this formalism extends to an infinite horizon case when $T= \infty$.
While there are RL methods to deal with infinite horizon cases, we will only consider episodic MDP and POMDPs.

Many reinforcement learning algorithms center around estimating value functions, which attempt to quantify the goodness of being in a state or taking an action from a state.
Given that the objective of RL is to maximize the cumulative expected reward, "goodness" refers to the likelihood that a state or state-action pair will result in a high cumulative reward trajectory.
There are 3 value functions that are commonly used across RL algorithms: the state-value function, $V^\pi(s_t)$, the state-action value function, $Q^\pi(s_t, a_t)$, and the advantage function, $A^\pi(s_t, a_t)$.
The state-value function is defined as the expected return from a given state,

$$
  V^\pi(s_t) = \mathbb{E}_{\tau \sim p_\pi(\tau \mid s_t)}\Bigg[\sum_{t' = t}^H \gamma^{t' - t} r(s_t, a_t)\Bigg]
$$

Meanwhile, the state-action-value or quality function, finds the expected return given a state and action, $$
  Q^\pi(s_t, a_t) = \mathbb{E}_{\tau \sim p_\pi(\tau \mid s_t, a_t)}\Bigg[\sum_{t' = t}^H \gamma^{t' - t} r(s_t, a_t)\Bigg]
$$

And lastly, the advantage function characterizes a relative state-action value in that it quantifies how valuable a state-action pair is over a baseline.
The advantage function can then be succinctly defined as 

$$
  A^\pi(s_t, a_t) = Q^\pi(s_t, a_t) - V^\pi(s_t)
$$ 
Learning $Q^\pi$, commonly called the Q-value function, is more powerful than learning the state-value function as Q-values easily prescribe what action to take when in a certain state, $a_t^* = \underset{a_t}{\text{argmax}} \, Q^\pi (s_t, a_t)$.
Conversely, if we know the state-value function, we would have to perform a maximization over an expected value to find the optimal action, $a_t^* = \underset{a_t}{\text{argmax}} \, \mathbb{E}_{s_{t+1} \sim \,T(s_{t+1} \mid s_t, a_t)}[V^\pi(s_{t+1})]$, which is less tractable.
Yet, learning the advantage function is often preferred in practice as advantage estimators favorably have lower variance than state-value and Q-value estimators [cite schulman gae].

**Survey of RL Algorithms**

<!-- 

Conceptual figure:

Algorithm 
  hyper-parameters specifying neural network, learning rate, etc.  
  We refer to a particular selection of algorithm and hyperparameters as an "agent", which is then "trained" to solve one or more environments
  internal 'parameters', estimated by "training" the agent. 
  

-->


There is an ever increasing multitude of reinforcement learning algorithms, and while there are far too many algorithms to discuss in detail in this paper, we will herein classify and touch on some current methods.
RL algorithms can generally be split into 2 groups, model-based and model-free RL.
In model-based RL, the agent is attempting to learn the state transition operator, $T$. <!-- Sometimes the agent is just assuming this is given, right? -->
Once the transition model is known, the agent can then use the transition model to plan out what is the optimal action, often with Model Predictive Control [cite tassa 2012 MPC], or the agent can use the transition model to generate more samples to aid learning [cite Sutton dyna, alphago?]. <!-- For most recent methods focus on an approximate solution, since optimal is computationally impossible.  In contrast most readers will assume this step is trivial, if the transition matrix is known there's nothing more to do but "look" at the predicted transitions and make a choice. -->
In model-free RL, the agent does not attempt to learn a dynamics model and instead tries to either learn a parameterized policy, learn a value function, or try to learn both a parameterized policy and a value function.
The general trade-off between model-free and model-based approaches is that in practice, model-based algorithms are often more sample efficient than model-free algorithms but often fail to surpass model-free algorithms in overall agent performance [cite?]


A further distinction to be made amongst RL algorithms is in what policy the agent uses to select actions.
If an agent uses the same policy to take actions as the policy that the agent attempts to evaluate or improve, then this algorithm is on-policy.  <!-- Unclear what "to evaluate or improve" means.  Perhaps an example would help? -->
If an agent uses a different policy to select actions than the policy that the agent evaluates or improves, then this algorithm is off-policy.
The benefit of off-policy learning is that the agent does not need to collect a sample every time the agent updates its behavior; instead, the agent can learn from past experience, which increases sample efficiency [cite SAC paper]. <!-- Not clear what this means -->
Yet, on-policy algorithms tend to be more stable in practice [cite Q-prop]. <!-- not clear how "stable" is defined/measured -->

The field of reinforcement learning has proliferated over the past 10 years on account of contributions from deep learning.
In every reinforcement learning algorithm, the agent is attempting to learn some function, whether it is a policy function and/or a value function.
Neural networks, which have the property of being universal function approximators [cite], are thus a powerful tool in reinforcement learning since neural networks can be used to approximate policy and/or value functions whose form is unknown.
Additionally, the ability of neural networks to autonomously devise low-dimensional representations of data is very beneficial when state observations are high dimension, for instance in the case of image-based observations. <!-- Yes! should stress why that matters -->
The typical process for using neural networks in RL is that the agent is attempting to optimize a policy and/or value network using the backpropagation algorithm -- an exception is made here for evolutionary algorithms which did not employ gradient-based methods. <!-- Fixme: needs to be clear that NN parameters are being fit by the "training" step. No point mentioning back-propagation, evolutionary algorithms, or gradient methdos if we are not going to define them.  -->

Deep RL has excelled in solving sequential decision-making problems that have non-tabular state-action spaces, problems that are typically intractable with classical methods. <!-- non-tabular undefined -->

In practice, the general process for attaining a good performing agent is to tune model hyperparameters then train an agent using the best hyperparameters. 
Hyperparameters refer to the parameters in a RL algorithm that are not directly learned during the training process. <!-- better to define before using in opening sentence.  -->
For example, a weight in the neural network is a model parameter, but the number of hidden layers in the neural network is a hyperparameter.
Since the optimal hyperparameters cannot be found analytically, tuning should be done to find a good-performing set of hyperparameters.
The hyperparameter tuning process consists of sampling then evaluating a set of hyperparameters, where tuning trials are often ranked by mean cumulative reward.
Since one round of training can be time intensive, hyperparameter tuning, which consists of usually numerous rounds of training, can be particularly time intensive.

<!-- Kari: I think we need more background on what a neural network is, how it fits in, and what other options are - maybe use examples of hyperparameters that would be in all RL approaches not just those that use neural nets -->

<!-- Can we add citations to each please -->
<!-- 

Should we mention:

- Temporal Differencing, RIENFORCE
- SARSA, 
- Double DQN?  
Can we add HER (since delayed reward case may be particularly interesting)
Should we do anything to better reflect the 'family tree' here? (e.g. DQN -> DDPG -> TD3)

-->
```{=latex}
\begin{center}
\begin{tabular}{|c|c|c|c|c|c|}
\hline
Algorithm & Description & Model & Policy & Action Space & State Space \\
\hline
MBPO & Model-based Policy Optimization & Model-based & On-policy& &\\
\hline
MCTS & Monte Carlo Tree Search & Model-based & Either & & \\
\hline
A2C  & Advantage Actor Critic & Model-free & On-policy& &\\
\hline
A3C  & Asynchronous A2C & Model-free & On-policy & &\\
\hline
TRPO  & Trust Region Policy Optimization & Model-free & On-policy & &\\
\hline
PPO  & Proximal Policy Optimization & Model-free & On-policy & &\\
\hline
DQN & Deep Q Networks & Model-free & Off-policy& &\\
\hline
DDPG & Deep Deterministic Policy Gradient & Model-free & Off-policy & &\\
\hline
TD3 & Twin Delayed DDPG & Model-free & Off-policy & &\\
\hline
SAC & Soft Actor Critic & Model-free & Off-policy & &\\
\hline
IMPALA & Importance Weighted Actor Learner & Model-free & Off-policy & &\\
\hline
APE-X & Distributed Prioritized Experience Replay & Model-free & Off-policy & & \\
\hline
\end{tabular}
\end{center}
```
# Examples

We provide three examples that illustrate the application and potential of deep RL to ecological and conservation problems, highlighting both the potential and challenges there-in.

```{r knitr, include = FALSE}
library(tidyverse)
library(patchwork)
knitr::opts_chunk$set(echo=FALSE, message = FALSE, warning = FALSE,
                      fig.width = 7, fig.height = 4)
ggplot2::theme_set(ggplot2::theme_bw())

scale_colour_discrete <- function(...) ggthemes::scale_colour_solarized()
scale_fill_discrete <- function(...) ggthemes::scale_fill_solarized()
pal <- ggthemes::solarized_pal()(6)
txtcolor <- "#586e75"
```

## Sustainable harvest

The first example focuses on the important but well-studied problem of setting harvest quotas in fisheries management.
Determining fishing quotas is both a critical ecological issue [@Worm2006; @Worm2009; @Costello2018], and also a textbook example that has long informed the management of renewable resources within fisheries and beyond [@Clark1990].
Given a population growth model that predicts the total biomass of a fish stock in the following year as a function of the current biomass, it straight forward to determine what biomass corresponds to the maximum growth rate of the stock, or $B_{\textrm{MSY}}$, the biomass at Maximum Sustainable Yield (MSY) [@Schaefer1954].
When the population growth rate is stochastic, the problem is slightly harder to solve, as the harvest quota must constantly adjust to the ups and downs of stochastic growth, but it is still possible to show the optimal strategy merely seeks to maintain the stock at $B_{\textrm{MSY}}$, adjusted for any discounting of future yields [@Reed1979].

This provides a natural first benchmark for deep RL approaches, since we can compare the RL solution to the mathematical optimum directly.
The problem is also of interest because the RL approach can be extended to consider ecological dynamics that either surpass the complexity possible with SDP methods, or violate assumptions of such classical approaches).
Moreover, fisheries management is both an important global challenge in it's own right [@Worm2006; @Costello2018] as well as a common test case to understand ecological dynamics and conservation management more broadly [@something].

For illustrative purposes, we consider the simplest version of the dynamic optimal harvest problem as outlined by [@Clark1974] (for the deterministic case) and [@Reed1979] (under stochastic recruitment).
In short, the manager seeks to optimize the net present value (discounted cumulative catch) of a fishery, observing the stock size each year and setting an appropriate harvest quota in response.
In the classical approach, the best model of the fish population dynamics must first be estimated from data, potentially with posterior distributions over parameter estimates reflecting any uncertainty.
From this model, the optimal harvest policy -- that is, the a function which returns the optimal quota for each possible observed stock size -- can be determined either by analytic [@Reed1979] or numerical [@Marescot2013] methods, depending on the complexity of the model.
In a RL approach, the model is instead replaced by a computer simulation, usually referred to as an "environment." The simplest simulation is merely an implementation of the estimated model itself.
However, unlike an analytic or SDP solution that is derived directly from the estimated model, the RL approach makes no assumption about the precise functional form or parameter values underlying the dynamics -- it is in principle agnostic to the details of the simulation.

We will illustrate the potential differences between two RL algorithms used to train the agents, one based on the "Advantage Actor Critic" algorithm, usually referred to as A2C [@A2C], and one based on Twin Delayed Deep Deterministic Policy Gradient, commonly known as TD3 [@TD3] (a generalization of Deep Deterministic Policy Gradient, or DDPG, which extends the DQN algorithm @DQN to a continuous action space using the deterministic policy gradient).

A step-by-step walk-through for training both agents on this environment is provided in the Appendix.
We compare the resulting management, policy, and reward under either RL agent to that achieved by the optimal management solution [Fig 1].
Despite having no knowledge of the underlying model and clear differences in the resulting strategies, both RL agents learn enough to achieve nearly optimal performance.

```{r message=FALSE, include=FALSE}
sims_df <- read_csv( "figs/sims_df.csv")
policy_df <- read_csv("figs/policy_df.csv")
reward_df <- read_csv("figs/reward_df.csv")

ymin <- function(x) last(x[(ntile(x, 20)==1)])
ymax <- function(x) last(x[(ntile(x, 20)==19)])

fig_sims <- 
sims_df %>% 
  group_by(time, model) %>% 
  summarise(ymin = ymin(state),
            ymax = ymax(state),
            state = mean(state), .groups = "drop") %>%
  ggplot(aes(time, state, ymin = ymin, ymax = ymax, fill=model)) + 
  geom_ribbon(alpha= 0.3) + geom_line(aes(col = model))

fig_policy <- 
  policy_df %>% ggplot(aes(state, state - action, 
                           group=interaction(rep, model),
                           col = model)) + 
  geom_line(show.legend = FALSE) + 
  coord_cartesian(xlim = c(0, 1.3), ylim=c(0,0.9)) + 
  ylab("escapement")


fig_reward <- reward_df %>% 
  ggplot(aes(time, mean_reward)) + 
  geom_ribbon(aes(ymin = mean_reward - 2*sd, 
                  ymax = mean_reward + 2*sd, fill = model),
              alpha=0.25, show.legend = FALSE) + 
  geom_line(aes(col = model), show.legend = FALSE) + 
  ylab("reward")
```

```{r fig1, fig.cap="Fisheries management using neural network agents trained with RL algorithms A2C and TD3 compared to optimal management.  Top panel: mean fish population size over time across 100 replicates. Shaded region shows the 95% confidence interval over simulations.  Lower left: The optimal solution is policy of constant escapement. Below the target escapement of 0.5, no harvest occurs, while any stock above that level is immediately harvested back down (red line).  RL agents both adopt a policies that cease any harvest below this level, while allowing a somewhat higher escapement than optimal just above the optimal escapement level.  A2C policy is non-deterministic, and overharvest from very high stock sizes.  Lower Right: Mean cumulative reward over time across simulations. TD3 achieves nearly-optimal mean utility. Mean reward under A2C is lower, though the confidence interval still overlaps the mean reward of optimal management."}
fig_sims / ( fig_policy + fig_reward)
```

Note that the A2C policy learns to shut down fishing entirely below a certain stock size, without fail.
A2C also keeps the fishery closed even once stock sizes are slightly higher than $B_{MSY}$.
Once it begins fishing, it slightly under-harvests for stock sizes a little over $B_{MSY}$, but tends to over-harvest for very large stock sizes (rarely ever realized, as fishing pressure is enough to keep it out of this range).
Note also that the harvest varies across replicates, creating a fuzzy region in the policy function.
Under such a policy, A2C maintains a stock size on average significantly above $B_{MSY}$, though also sees higher volatility particularly on the low side.
Note that due to the stochastic nature of the environment, any given replicate A2C may out-perform the optimal policy, though on average the optimal policy provides a higher reward.

These features highlight a common challenge in the design and training of RL algorithms.
RL cares only about improving the realized cumulative reward, and may sometimes achieve this in unexpected ways.
Because these simulations rarely take stock sizes close to a state of 1.0, the heavy overfishing behavior A2C has in this range has little consequence.
Likewise it is more difficult for TD3 to learn the appropriate harvest level at these high states that are rarely sampled in the course of training.
Just as intuition would suggest, it is possible to devise alternative training schemes which may do a better job sampling the simulation dynamics at high stock levels.
Training these agents in a variety of alternative contexts can improve there ability to generalize to other scenarios.

## Ecological Tipping Points

Our second example focuses on a case for which we do not already have an existing, provably optimal policy to compare against.
We consider the generic problem of an ecosystem facing slowly deteriorating environmental conditions which move the dynamics closer towards a tipping point [Fig 2].
This model of a critical transition has been posited widely in ecological systems, from the simple consumer-resource model of [@May1977] on which our dynamics are based, to microbial dynamics [@Dai2013] lake ecosystem communities [@Carpenter2008] to planetary ecosystems [@Barnkosky2014].
On top of these ecological dynamics we introduce an explicit ecosystem service model quantifying the value of more desirable 'high' state relative to the 'low' state.
For simplicity, we assume a proportional benefit $b$ associated with the ecosystem state $X(t)$.
Thus when the ecosystem is near the 'high' equilibrium $\hat X_H$, the corresponding ecosystem benefit $b \hat X_H$ is higher than at the low equilibrium, $b x_L$, consistent with the intuitive description of ecosystem tipping points [@Barnosky2014].
We also enumerate the possible actions which a manager may take in response to environmental degradation.
In the absence of any management response, we assume the environment deteriorates at a fixed rate $\alpha$, which can be thought of as the incremental increase in global mean temperature or similar anthropogenic forcing term.
Management can respond to slow or even reverse this trend by choosing an opposing action $A_t$.
We assume that large actions are proportionally more costly than small actions, consistent with the expectation of diminishing returns: taking the cost associated with an action $A_t$ as equal to $c A_t^2$.
Many alterations of these basic assumptions are also possible: our `gym_conservation` implements a range of different scenarios with user-configurable settings.
In each case, the manager observes the current state of the system each year and must then select the policy response that year.

```{r message=FALSE, fig.cap="Bifurcation diagram for tipping point scenario. The ecosystem begins in the desirable 'high' state under an evironmental parameter (e.g. global mean temperature, arbitrary units) of 0.19.  In the absence of conservation action, the environment worsens (e.g. rising mean temperature) as the parameter increases.  This results in only a slow degredation of the stable state, until the parameter crosses the tipping point threshold at about 0.215, where the upper stable branch is anihilated in a fold bifurcation and the system rapidly transitions to lower stable branch, around state of 0.1.   Recovery to the upper branch requires a much greater conservation investment, reducing the parameter all the way to 0.165 where the reverse bifurcation will carry it back to the upper stable branch."}
bifur_df <- read_csv("figs/bifur.csv", col_types = "dccd")
bifur_df %>%
  ggplot(aes(parameter, state, lty=equilibrium, group = group)) + 
  geom_line()

```

Because this problem involves a parameter whose value changes over time (the slowly deteriorating environment), the resulting ecosystem dynamics are not autonomous.
This precludes our ability to solve for the optimal management policy using classical theory such as for Markov Decision Processes (MDP, [@Marescot2013]), typically used to solve sequential decision-making problems.
However, it is often argued that simple rules can achieve nearly optimal management of ecological conservation objectives in many cases [@Possingham; @Possingham; @morePossingham].
A common conservation strategy employs a fixed response level rather than a dynamic policy which is toggled up or down each year: for example, declaring certain regions as protected areas in perpetuity.
An intuitive strategy faced with an ecosystem tipping point would be 'perfect conservation', in which the management response is perfectly calibrated to counter-balance any further decline.
While the precise rate of such decline may not be known in practice (and will not be known to RL algorithms before-hand either), it is easy to implement such a policy in simulation for comparative purposes.
We compare this rule-of-thumb to the optimal policy found by training an agent using the TD3 algorithm.

```{r}
sims_df <-read_csv("figs/tipping_sims_df.csv")
policy_df <- read_csv("figs/tipping_policy_df.csv")
reward_df <- read_csv("figs/tipping_reward_df.csv")
```

```{r fig.cap="Ecosystem dynamics under management using the steady-state rule-of-thumb strategy compared to management using a neural network trained using the TD3 RL algorithm.  Top panel: mean and 95% confidence interval of ecosystem state over 100 replicate simulations.  As more replicates cross the tipping point threshold under steady-state strategy, the mean slowly decreases, while the TD3 agent preserves most replicates safely above the tipping point.  Lower left: the policy function learned using TD3 relative to the policy function under the steady state.  Lower right: mean rewards under TD3 management evenutally exceed those expected under the steady state strategy as a large initial investment in conservation eventually pays off."}
fig_sims <- 
  sims_df %>%
   group_by(time, model) %>% 
   summarise(ymin = ymin(state),
            ymax = ymax(state),
            state = mean(state), .groups = "drop") %>%
  ggplot(aes(time, state, ymin = ymin, ymax = ymax, fill=model)) + 
   geom_ribbon(alpha= 0.3) + geom_line(aes(col = model))

fig_policy <- 
  policy_df %>% ggplot(aes(state, action, 
                           group=interaction(rep, model),
                           col = model)) + 
  geom_line(show.legend = FALSE) 

fig_reward <- reward_df %>% 
  ggplot(aes(time, mean_reward)) + 
  geom_ribbon(aes(ymin = mean_reward - 2*sd, 
                  ymax = mean_reward + 2*sd, fill = model),
              alpha=0.25, show.legend = FALSE) + 
  geom_line(aes(col = model), show.legend = FALSE) + 
  ylab("reward")
```

```{r fig2, fig.cap=""}
fig_sims / ( fig_policy + fig_reward)
```

The TD3-trained neural network agent proves far more successful in preventing chance transitions across the tipping point, consistently achieving a higher cumulative ecosystem service value across replicates than the steady-state strategy.

Examining the replicate management trajectories and corresponding rewards [Fig 3] reveal that the RL agent incurs significantly higher costs in the initial phases of the simulation, dipping the cumulative reward into negative territory on average and well below the reward realized by the simple steady-state strategy.
This initial investment then begins to pay off -- by about 200th time step the RL agent has surpassed the performance of the steady state strategy.\
The policy plot provides more intuition for the RL agent's strategy: at very high state values, the RL agent opts for no conservation action -- so far from the tipping point, no response is required.
Near the tipping point, the RL agent steeply ramps up the conservation effort, and retains this effort even as the system falls below the critical threshold, where a sufficiently aggressive response can tip the system back into recovery.
For a system at or very close to the zero-state, the RL agent gives up, opting for no action.
Recall that the quadratic scaling of cost makes the rapid response of the TD3 algorithm much more costly to achieve the same net environmental improvement divided into smaller increments over a longer timeline.
However, our RL agent has discovered that the extra investment for a rapid response is well justified as the risk of crossing a tipping point increases.

A close examination of the trajectories of individual simulations which cross the tipping point under either management strategy (see appendix, fig S1, S2) further highlights the difference between these two approaches.
Under the steady-state strategy, the system remains poised too close to the tipping point: stochastic noise eventually drives most replicates across the threshold, where the steady-state strategy is too weak to bring them back once they collapse.
As replicate after replicate stochastically crashes, the mean state and mean reward bend increasingly downwards.
In contrast, the RL agent edges the system slightly farther away from the tipping point, decreasing but not eliminating the chance of a chance transition.
In the few replicates that experience a critical transition anyway, the RL agent usually responds with sufficient commitment to ensure their recovery (Fig S2).
Only 3 out of 100 replicates degrade far enough for the RL agent to give up the high cost of trying to rescue them.

Such examination makes it clear how the RL agent's use of a more dynamic strategy out-preforms the steady-state strategy.
The various kinks in the RL policy function also suggest that this solution is nevertheless surely not the optimal solution -- such features are typical of RL-based policies, where such spandral features from early experimentation may become baked into the long-term strategy if adjusting them has little improvement on the final reward.
Further tuning of hyper-parameters, increased training, alterations to the training algorithm or an entirely different training algorithm would likely be able to further improve upon this performance.
The use of a recurrent neural network architecture instead of the multi-layer perceptron used here would open the door for the policy to reflect the non-autonomous dynamics more directly.
A recurrent neural network, such as long-short-term-memory (LSTM), could learn a strategy which reflects not only the current system state, but also latent information such as the time elapsed.
<!-- Kari: lots of jargon in this paragraph that needs to be defined-->

# Discussion

```{=html}
<!-- 

1. Return to theme of RL vs supervised/unsupervised learning.


-->
```
<!-- Something about relevance to both the _theory_ and practice of ecological conservation. (e.g. "strategic" 1-D models have probably had a greater influence, (for better or worse!) on real world policies than complex, detail-specific, precise "tactical" models) -->

## Open Problems

The examples presented here only scrape the surface of possible RL applications to conservation problems.
The examples we have focused on are intentionally quite simple, though it is worth remembering that these very same simple models have a long history of relevance and application in both research and policy contexts.
Despite their simplicity, the optimal strategy is not always obvious before hand, however intuitive it may appear in retrospect.
In the case of the ecosystem tipping point scenario, the optimal strategy is unknown, and the approximate solution found by our RL implementation could almost certainly be improved upon.

Even at it's best, an RL-trained agent can only be as good as the scenarios on which it has been trained.
In these simple examples in which the simulation implements a single model, training is analgous to classical methods which take the model as given [@Marescot2013].
But classical approaches can be difficult to generalize when the underlying model is unknown: alternative models may require a completely different approach.
In contrast, the process of training an RL algorithm on a more complex problem is no different than the simple one: we only need access to a simulation which can generate plausible future states in response to possible actions.
Many of our most realistic ecological models exist only as numerical simulations of lower-level dynamics, such as the individual-based models used in the management of forests and wildfire [@sortie; @firemodel], disease [@disease], marine ecosystems [@ecopath], or global climate change [@gcm].

# Discussion

# Open issues

Deep reinforcement learning has only become an established subfield of AI over the past decade; and, as with most young fields, there are a range of unsolved problems.
In practice, the foremost problem with DRL is instability.
For a range of reasons, including high variance and the exploration vs. exploitation trade-off, model-free and model-based algorithms tend to have unstable performance on environments with non-tabular state-action spaces [cite RL that matters].
For instance, it is not uncommon to observe vastly different behavior when two agents are initialized with different random seeds [cite ibid].
Since model-based and model-free algorithms typically have a large number of hyperparameters, each of which can contribute to training instability, it is practically impossible to identify the source of instability.
The hope of the RL practitioner is that after doing an exhaustive hyperparameter search, the practitioner will find a set of hyperparameters that results in a robust, well-performing agent, but there are no guarantees that hyperparameter tuning will lead to this outcome.
<!-- Kari: what does it mean for two algorithms with similar hyperparameters to perform differently? (As happens in our examples) -->

In the context of conservation, we anticipate that there will be a host of specific challenges to using DRL methods.
Interpretability is likely to be the biggest barrier to adoption.
Although there have been some attempts to increase the transparency of RL agents that use neural network as function approximators [cite], it is still very difficult to explain the behavior of RL-trained agents.
Another challenge is that DRL algorithms are very sample inefficient, and to overcome sample inefficiency for conservation decision-making problems that are often data poor, relying on simulations to train agents will be necessary.
While there are positives to using simulated data, any assumptions or biases that are included in the model will determine agent behavior.

An issue that is central to any applied optimization problem is using the right objective.
There have been many examples in which explicit reward design goes horribly wrong [cite].
And for ecological systems, which are often very complex and impossible to fully observe, it seems that bad reward design is quite likely.
Moreover, specifying a reward function for conservation brings up various ethical concerns -- e.g. who gets to specify the reward function and will the values of the greater community be represented in the reward function.
One recently proposed framework to overcome challenges with explicit reward design is called AI alignment, wherein the agent observes humans, infers the preferences of humans and then acts as to satisfy human preferences.
Yet, in the context of training an agent to say conserve a species, there are questions that could easily arise, foremost will the agent infer that humans don't care about a certain species so acts in a way that leads to that species' extinction?
There is a definite need for exploration of the ethics of using DRL for conservation.

Ultimately, we anticipate that getting DRL to work well on fully detailed conservation decision-making problems will be challenging.
Beyond typically having large state-action spaces for agents to explore, conservation problems tend to have sparse reward structures.
E.g. rarely does an action in conservation lead to immediate rewards; instead, benefits are typically observed after actions have been performed.
Sparse reward structures are challenging for DRL agents, because of the credit assignment problem [cite].
To solve sparse reward environments, many researchers have employed customized algorithms to achieve better performance than standard DRL methods [cite Montezuma].
For conservation, we anticipate that there will likely be a similar pattern, wherein bespoke RL methods will attain better performance than general methods.

-   wild west of AI?
    -can separate RL from having gyms implemented -\> how do you compare different models, -mention that customization is probably the way to go, baselines only go so far

-   AI ethics, transparency

# a vision going forward

-   our public/open source environments and agents, leaderboard
-   the need for collaborative/competitive development both to make better benchmark environments and better agents
