---
title: Deep Reinforcement Learning for Conservation Decisions
authors:
  - name: Marcus Lapeyrolerie
    department: Department of Environmental Science, Policy, and Management
    affiliation: University of California, Berkeley
    location: Berkeley, California
  - name: Melissa Chapman
    department: Department of Environmental Science, Policy, and Management
    affiliation: University of California, Berkeley
    location: Berkeley, California
  - name: Kari Norman
    department: Department of Environmental Science, Policy, and Management
    affiliation: University of California, Berkeley
    location: Berkeley, California
  - name: Carl Boettiger
    department: Department of Environmental Science, Policy, and Management
    affiliation: University of California, Berkeley
    location: Berkeley, California
    email: cboettig@berkeley.edu
    
abstract: |
  Enter the text of your abstract here.
keywords:
  - blah
  - blee
  - bloo
  - these are optional and can be removed
bibliography: references.bib
output: 
  rticles::arxiv_article:
    keep_tex: true
---

# Introduction

Despite rapidly expanding interest in machine learning (ML) and artificial intelligence (AI) applications to ecology, the use of reinforcement learning (RL) has been largely neglected, which have focused primarily on applications of either supervised learning [@examples] or unsupervised learning [@examples].  In third learning paradigm in AI, reinforcement learning, a _software agent_ learns by repeated experience to select _actions_ to maximize some cumulative reward.  RL is an incredibly general approach, and the one that in principle looks most like artificial general intelligence.  RL has been notable for successes such as AlphaZero, a single agent that can beat the best human players in go, chess, and shogi [@alphazero], as well as applications in robotics [@robotics], but also faces major challenges [@henderson2017].

We believe RL approaches hold particular interest for conservation decision-making problems, for two reasons.  First, ecological systems are complex and difficult to represent accurately using models that remain tractable with classical approaches.  For example, convenient mathematical assumptions such as stationarity, often required by classical techniques [@Marescot2013], stand in stark odds with the picture of an environment which is constantly changing.  Second, RL approaches do not require massive amounts of data, in stark contrast with other more widely used applications of ML.  Despite the expansion in data collection from remote sensors, citizen science, and other efforts, ecological data often remains too sparse in the details that matter.  And in a changing environment, no amount of historical data may be able to train supervised or unsupervised ML approaches to anticipate a future that has never been observed.  Stationarity is an even stronger assumption there than in classical approaches. However, RL are not trained by historical data alone, but rather, they rely on computer simulations.  Simulations are our most powerful approach to exploring future scenario, and simulated 'data' represents the largest contribution to the exponential growth of data in climate science [@].  As we illustrate here, a software agent can be trained to successfully navigate hypothetical scenarios through it's experience in simulation that would be difficult or impossible to solve with classical methods.  In this manner, RL can compliment, rather than replace, existing empirical and theoretical research which would continue to form the basis for those simulations.

If RL has great promise, the approach also imposes great risk.  Lacking the tractability of classical methods, an RL agent can appear to solve a wide class of scenarios with ease, but fail unexpectedly under seemingly similar conditions.  Despite recent progress, Deep RL is a young field and stability and robustness of RL agents remain largely open problems. Giving algorithms direct influence over conservation policy may also raise ethical and political issues, particularly if those algorithms (or components thereof) are treated as propietary intellectual property. 


# RL essentials

- terminology, problem definition
- discussion / table of current benchmark algorithms (DQN, PPO, DDPG, A2C, TD3, SAC)
- training, hyperparameter tuning

# Examples


```{r setup, cache = FALSE, include = FALSE}
library(tidyverse)
library(patchwork)
library(reticulate)

np <- import("numpy")
np.random.seed(24L)

knitr::opts_chunk$set(echo=FALSE)
```

## Finding a known optimal solution using RL

fishing gym?

```{r cache=TRUE}
## Python dependencies
gym         <- import ("gym")
gym_fishing <- import("gym_fishing")
sb3         <- import ("stable_baselines3")

## initialize the environment
env <- gym$make("fishing-v1")
```

```{r eval=!file.exists("cache/a2c.zip")}
# train an agent (model) on one of the environments:
a2c <- sb3$A2C('MlpPolicy', env, verbose=0L, seed = 42L) # Must use L for integers!
a2c$learn(total_timesteps=300000L)
a2c$save("cache/a2c")
```

```{r}
# Simulate management under the trained agent
a2c <- sb3$A2C$load("cache/a2c")
sims <- env$simulate(a2c, reps = 500L)

policy <- env$policyfn(a2c, reps = 50L)
```





```{r}
# Simulate under the optimal solution (given the model)
opt <- gym_fishing$models$escapement(env)
opt_sims <- env$simulate(opt, reps = 500L)
opt_policy <- env$policyfn(opt) %>% rename(optimal = action)

sims_df <- bind_rows(sims, opt_sims, .id = "model")
policy <- left_join(policy, opt_policy)
```


```{r}
ymin <- function(x) last(x[(ntile(x, 20)==1)])
ymax <- function(x) last(x[(ntile(x, 20)==19)])

fig_sims <- 
sims_df %>% 
  group_by(time, model) %>% 
  summarise(ymin = ymin(state),
            ymax = ymax(state),
            state = mean(state), .groups = "drop") %>%
  ggplot(aes(time, state, ymin = ymin, ymax = ymax, fill=model)) + 
  geom_ribbon(alpha= 0.3) + geom_line(aes(col = model))
```


```{r}
fig_policy <- 
  policy %>% ggplot(aes(state, state - action, group=rep)) + 
  geom_point(alpha=.05) + geom_line(aes(y = state - optimal), col = "blue") + 
  coord_cartesian(xlim = c(0, 1.5), ylim=c(0,0.9))
```

```{r fig1, fig.cap=""}
fig_sims + fig_policy
```



## Example of solving a problem that is difficult/impossible for conventional methods

two use cases to consider: 
  - model is known but too complex to solve by (approximate) dynamic programming etc, or non-markovian
  - model is unknown
  
model uncertainty/transfer learning? nonstationary dynamics? 

## Example of an open problem without an RL solution

fire gym?


# Open issues

- wild west of AI?
- reproducibility
- AI ethics, transparency

# a vision going forward

- our public/open source environments and agents, leaderboard
- the need for collaborative/competitive development both to make better benchmark environments and better agents


