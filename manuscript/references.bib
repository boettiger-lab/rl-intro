
@article{brockman2016,
	title = {{OpenAI} {Gym}},
	url = {http://arxiv.org/abs/1606.01540},
	abstract = {OpenAI Gym is a toolkit for reinforcement learning research. It includes a growing collection of benchmark problems that expose a common interface, and a website where people can share their results and compare the performance of algorithms. This whitepaper discusses the components of OpenAI Gym and the design decisions that went into the software.},
	urldate = {2021-04-29},
	journal = {arXiv:1606.01540 [cs]},
	author = {Brockman, Greg and Cheung, Vicki and Pettersson, Ludwig and Schneider, Jonas and Schulman, John and Tang, Jie and Zaremba, Wojciech},
	month = jun,
	year = {2016},
	note = {arXiv: 1606.01540},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/home/cboettig/Zotero/storage/AIS2GFG4/Brockman et al. - 2016 - OpenAI Gym.pdf:application/pdf;arXiv.org Snapshot:/home/cboettig/Zotero/storage/62NTAUKT/1606.html:text/html}
}



@article{sac,
	title = {Soft {Actor}-{Critic}: {Off}-{Policy} {Maximum} {Entropy} {Deep} {Reinforcement} {Learning} with a {Stochastic} {Actor}},
	shorttitle = {Soft {Actor}-{Critic}},
	url = {http://arxiv.org/abs/1801.01290},
	abstract = {Model-free deep reinforcement learning (RL) algorithms have been demonstrated on a range of challenging decision making and control tasks. However, these methods typically suffer from two major challenges: very high sample complexity and brittle convergence properties, which necessitate meticulous hyperparameter tuning. Both of these challenges severely limit the applicability of such methods to complex, real-world domains. In this paper, we propose soft actor-critic, an off-policy actor-critic deep RL algorithm based on the maximum entropy reinforcement learning framework. In this framework, the actor aims to maximize expected reward while also maximizing entropy. That is, to succeed at the task while acting as randomly as possible. Prior deep RL methods based on this framework have been formulated as Q-learning methods. By combining off-policy updates with a stable stochastic actor-critic formulation, our method achieves state-of-the-art performance on a range of continuous control benchmark tasks, outperforming prior on-policy and off-policy methods. Furthermore, we demonstrate that, in contrast to other off-policy algorithms, our approach is very stable, achieving very similar performance across different random seeds.},
	urldate = {2021-05-05},
	journal = {arXiv:1801.01290 [cs, stat]},
	author = {Haarnoja, Tuomas and Zhou, Aurick and Abbeel, Pieter and Levine, Sergey},
	month = aug,
	year = {2018},
	note = {arXiv: 1801.01290},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: ICML 2018 Videos: sites.google.com/view/soft-actor-critic Code: github.com/haarnoja/sac},
	file = {arXiv Fulltext PDF:/home/cboettig/Zotero/storage/EMT3HQTG/Haarnoja et al. - 2018 - Soft Actor-Critic Off-Policy Maximum Entropy Deep.pdf:application/pdf;arXiv.org Snapshot:/home/cboettig/Zotero/storage/TRBDQEN5/1801.html:text/html}
}

@article{td3,
	title = {Addressing {Function} {Approximation} {Error} in {Actor}-{Critic} {Methods}},
	url = {http://arxiv.org/abs/1802.09477},
	abstract = {In value-based reinforcement learning methods such as deep Q-learning, function approximation errors are known to lead to overestimated value estimates and suboptimal policies. We show that this problem persists in an actor-critic setting and propose novel mechanisms to minimize its effects on both the actor and the critic. Our algorithm builds on Double Q-learning, by taking the minimum value between a pair of critics to limit overestimation. We draw the connection between target networks and overestimation bias, and suggest delaying policy updates to reduce per-update error and further improve performance. We evaluate our method on the suite of OpenAI gym tasks, outperforming the state of the art in every environment tested.},
	urldate = {2021-05-05},
	journal = {arXiv:1802.09477 [cs, stat]},
	author = {Fujimoto, Scott and van Hoof, Herke and Meger, David},
	month = oct,
	year = {2018},
	note = {arXiv: 1802.09477},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: Accepted at ICML 2018},
	file = {arXiv Fulltext PDF:/home/cboettig/Zotero/storage/7QWN2IXY/Fujimoto et al. - 2018 - Addressing Function Approximation Error in Actor-C.pdf:application/pdf;arXiv.org Snapshot:/home/cboettig/Zotero/storage/8QGUZGAP/1802.html:text/html}
}

@article{her,
	title = {Hindsight {Experience} {Replay}},
	url = {http://arxiv.org/abs/1707.01495},
	abstract = {Dealing with sparse rewards is one of the biggest challenges in Reinforcement Learning (RL). We present a novel technique called Hindsight Experience Replay which allows sample-efficient learning from rewards which are sparse and binary and therefore avoid the need for complicated reward engineering. It can be combined with an arbitrary off-policy RL algorithm and may be seen as a form of implicit curriculum. We demonstrate our approach on the task of manipulating objects with a robotic arm. In particular, we run experiments on three different tasks: pushing, sliding, and pick-and-place, in each case using only binary rewards indicating whether or not the task is completed. Our ablation studies show that Hindsight Experience Replay is a crucial ingredient which makes training possible in these challenging environments. We show that our policies trained on a physics simulation can be deployed on a physical robot and successfully complete the task.},
	urldate = {2021-05-05},
	journal = {arXiv:1707.01495 [cs]},
	author = {Andrychowicz, Marcin and Wolski, Filip and Ray, Alex and Schneider, Jonas and Fong, Rachel and Welinder, Peter and McGrew, Bob and Tobin, Josh and Abbeel, Pieter and Zaremba, Wojciech},
	month = feb,
	year = {2018},
	note = {arXiv: 1707.01495},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Computer Science - Robotics},
	file = {arXiv Fulltext PDF:/home/cboettig/Zotero/storage/FH3QSE54/Andrychowicz et al. - 2018 - Hindsight Experience Replay.pdf:application/pdf;arXiv.org Snapshot:/home/cboettig/Zotero/storage/DH3PBZBY/1707.html:text/html}
}

@article{dqn,
	title = {Human-level control through deep reinforcement learning},
	volume = {518},
	issn = {0028-0836, 1476-4687},
	url = {http://www.nature.com/articles/nature14236},
	doi = {10.1038/nature14236},
	language = {en},
	number = {7540},
	urldate = {2021-05-05},
	journal = {Nature},
	author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A. and Veness, Joel and Bellemare, Marc G. and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K. and Ostrovski, Georg and Petersen, Stig and Beattie, Charles and Sadik, Amir and Antonoglou, Ioannis and King, Helen and Kumaran, Dharshan and Wierstra, Daan and Legg, Shane and Hassabis, Demis},
	month = feb,
	year = {2015},
	pages = {529--533}
}

@article{dqn_arxiv,
	title = {Playing {Atari} with {Deep} {Reinforcement} {Learning}},
	url = {http://arxiv.org/abs/1312.5602},
	abstract = {We present the first deep learning model to successfully learn control policies directly from high-dimensional sensory input using reinforcement learning. The model is a convolutional neural network, trained with a variant of Q-learning, whose input is raw pixels and whose output is a value function estimating future rewards. We apply our method to seven Atari 2600 games from the Arcade Learning Environment, with no adjustment of the architecture or learning algorithm. We find that it outperforms all previous approaches on six of the games and surpasses a human expert on three of them.},
	urldate = {2021-05-05},
	journal = {arXiv:1312.5602 [cs]},
	author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Graves, Alex and Antonoglou, Ioannis and Wierstra, Daan and Riedmiller, Martin},
	month = dec,
	year = {2013},
	note = {arXiv: 1312.5602},
	keywords = {Computer Science - Machine Learning},
	annote = {Comment: NIPS Deep Learning Workshop 2013},
	file = {arXiv Fulltext PDF:/home/cboettig/Zotero/storage/9759TK3T/Mnih et al. - 2013 - Playing Atari with Deep Reinforcement Learning.pdf:application/pdf;arXiv.org Snapshot:/home/cboettig/Zotero/storage/927GQNHK/1312.html:text/html}
}

@article{ddpg,
	title = {Continuous control with deep reinforcement learning},
	url = {http://arxiv.org/abs/1509.02971},
	abstract = {We adapt the ideas underlying the success of Deep Q-Learning to the continuous action domain. We present an actor-critic, model-free algorithm based on the deterministic policy gradient that can operate over continuous action spaces. Using the same learning algorithm, network architecture and hyper-parameters, our algorithm robustly solves more than 20 simulated physics tasks, including classic problems such as cartpole swing-up, dexterous manipulation, legged locomotion and car driving. Our algorithm is able to find policies whose performance is competitive with those found by a planning algorithm with full access to the dynamics of the domain and its derivatives. We further demonstrate that for many of the tasks the algorithm can learn policies end-to-end: directly from raw pixel inputs.},
	urldate = {2021-05-05},
	journal = {arXiv:1509.02971 [cs, stat]},
	author = {Lillicrap, Timothy P. and Hunt, Jonathan J. and Pritzel, Alexander and Heess, Nicolas and Erez, Tom and Tassa, Yuval and Silver, David and Wierstra, Daan},
	month = jul,
	year = {2019},
	note = {arXiv: 1509.02971},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: 10 pages + supplementary},
	file = {arXiv Fulltext PDF:/home/cboettig/Zotero/storage/AJNAETMB/Lillicrap et al. - 2019 - Continuous control with deep reinforcement learnin.pdf:application/pdf;arXiv.org Snapshot:/home/cboettig/Zotero/storage/SL6FXX4V/1509.html:text/html}
}

@article{a2c,
	title = {Asynchronous {Methods} for {Deep} {Reinforcement} {Learning}},
	url = {http://arxiv.org/abs/1602.01783},
	abstract = {We propose a conceptually simple and lightweight framework for deep reinforcement learning that uses asynchronous gradient descent for optimization of deep neural network controllers. We present asynchronous variants of four standard reinforcement learning algorithms and show that parallel actor-learners have a stabilizing effect on training allowing all four methods to successfully train neural network controllers. The best performing method, an asynchronous variant of actor-critic, surpasses the current state-of-the-art on the Atari domain while training for half the time on a single multi-core CPU instead of a GPU. Furthermore, we show that asynchronous actor-critic succeeds on a wide variety of continuous motor control problems as well as on a new task of navigating random 3D mazes using a visual input.},
	urldate = {2021-05-05},
	journal = {arXiv:1602.01783 [cs]},
	author = {Mnih, Volodymyr and Badia, Adrià Puigdomènech and Mirza, Mehdi and Graves, Alex and Lillicrap, Timothy P. and Harley, Tim and Silver, David and Kavukcuoglu, Koray},
	month = jun,
	year = {2016},
	note = {arXiv: 1602.01783},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/home/cboettig/Zotero/storage/PHJJPHS2/Mnih et al. - 2016 - Asynchronous Methods for Deep Reinforcement Learni.pdf:application/pdf;arXiv.org Snapshot:/home/cboettig/Zotero/storage/YDTHBLKH/1602.html:text/html}
}


@misc{sb3,
  author = {Raffin, Antonin and Hill, Ashley and Ernestus, Maximilian and Gleave, Adam and Kanervisto, Anssi and Dormann, Noah},
  title = {Stable Baselines3},
  year = {2019},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/DLR-RM/stable-baselines3}},
}

@misc{tf-agents,
  title = {{TF-Agents}: A library for Reinforcement Learning in TensorFlow},
  author = {Sergio Guadarrama and Anoop Korattikara and Oscar Ramirez and
     Pablo Castro and Ethan Holly and Sam Fishman and Ke Wang and
     Ekaterina Gonina and Neal Wu and Efi Kokiopoulou and Luciano Sbaiz and
     Jamie Smith and Gábor Bartók and Jesse Berent and Chris Harris and
     Vincent Vanhoucke and Eugene Brevdo},
  howpublished = {\url{https://github.com/tensorflow/agents}},
  url = "https://github.com/tensorflow/agents",
  year = 2018,
  note = "[Online; accessed May 2021]"
}

@inproceedings{kour2014real,
  title={Real-time segmentation of on-line handwritten arabic script},
  author={Kour, George and Saabne, Raid},
  booktitle={Frontiers in Handwriting Recognition (ICFHR), 2014 14th International Conference on},
  pages={417--422},
  year={2014},
  organization={IEEE}
}

@inproceedings{kour2014fast,
  title={Fast classification of handwritten on-line Arabic characters},
  author={Kour, George and Saabne, Raid},
  booktitle={Soft Computing and Pattern Recognition (SoCPaR), 2014 6th International Conference of},
  pages={312--318},
  year={2014},
  organization={IEEE}
}

@article{hadash2018estimate,
  title={Estimate and Replace: A Novel Approach to Integrating Deep Neural Networks with Existing Applications},
  author={Hadash, Guy and Kermany, Einat and Carmeli, Boaz and Lavi, Ofer and Kour, George and Jacovi, Alon},
  journal={arXiv preprint arXiv:1804.09028},
  year={2018}
}



@article{covid,
	title = {Ecology and economics for pandemic prevention},
	volume = {369},
	issn = {0036-8075},
	url = {https://science.sciencemag.org/content/369/6502/379},
	doi = {10.1126/science.abc3189},
	number = {6502},
	journal = {Science},
	author = {Dobson, Andrew P. and Pimm, Stuart L. and Hannah, Lee and Kaufman, Les and Ahumada, Jorge A. and Ando, Amy W. and Bernstein, Aaron and Busch, Jonah and Daszak, Peter and Engelmann, Jens and Kinnaird, Margaret F. and Li, Binbin V. and Loch-Temzelides, Ted and Lovejoy, Thomas and Nowak, Katarzyna and Roehrdanz, Patrick R. and Vale, Mariana M.},
	year = {2020},
	pages = {379--381}
}

@article{fire_2,
	title = {Reform forest fire management},
	volume = {349},
	issn = {0036-8075, 1095-9203},
	url = {https://www.sciencemag.org/lookup/doi/10.1126/science.aab2356},
	doi = {10.1126/science.aab2356},
	language = {en},
	number = {6254},
	urldate = {2021-04-16},
	journal = {Science},
	author = {North, M. P. and Stephens, S. L. and Collins, B. M. and Agee, J. K. and Aplet, G. and Franklin, J. F. and Fule, P. Z.},
	month = sep,
	year = {2015},
	pages = {1280--1281}
}

@article{wildfire,
	title = {Learning to coexist with wildfire},
	volume = {515},
	issn = {0028-0836, 1476-4687},
	url = {http://www.nature.com/articles/nature13946},
	doi = {10.1038/nature13946},
	language = {en},
	number = {7525},
	urldate = {2021-04-16},
	journal = {Nature},
	author = {Moritz, Max A. and Batllori, Enric and Bradstock, Ross A. and Gill, A. Malcolm and Handmer, John and Hessburg, Paul F. and Leonard, Justin and McCaffrey, Sarah and Odion, Dennis C. and Schoennagel, Tania and Syphard, Alexandra D.},
	month = nov,
	year = {2014},
	pages = {58--66}
}


@article{biodiversity_oceans,
	title = {Protecting the global ocean for biodiversity, food and climate},
	volume = {592},
	issn = {0028-0836, 1476-4687},
	url = {http://www.nature.com/articles/s41586-021-03371-z},
	doi = {10.1038/s41586-021-03371-z},
	language = {en},
	number = {7854},
	urldate = {2021-04-16},
	journal = {Nature},
	author = {Sala, Enric and Mayorga, Juan and Bradley, Darcy and Cabral, Reniel B. and Atwood, Trisha B. and Auber, Arnaud and Cheung, William and Costello, Christopher and Ferretti, Francesco and Friedlander, Alan M. and Gaines, Steven D. and Garilao, Cristina and Goodell, Whitney and Halpern, Benjamin S. and Hinson, Audra and Kaschner, Kristin and Kesner-Reyes, Kathleen and Leprieur, Fabien and McGowan, Jennifer and Morgan, Lance E. and Mouillot, David and Palacios-Abrantes, Juliano and Possingham, Hugh P. and Rechberger, Kristin D. and Worm, Boris and Lubchenco, Jane},
	month = apr,
	year = {2021},
	pages = {397--402}
}


@article{fishwatch,
	title = {Ending hide and seek at sea},
	volume = {351},
	issn = {0036-8075, 1095-9203},
	url = {https://www.sciencemag.org/lookup/doi/10.1126/science.aad5686},
	doi = {10.1126/science.aad5686},
	language = {en},
	number = {6278},
	urldate = {2021-04-17},
	journal = {Science},
	author = {McCauley, Douglas J. and Woods, Paul and Sullivan, Brian and Bergman, Bjorn and Jablonicky, Caroline and Roan, Aaron and Hirshfield, Michael and Boerder, Kristina and Worm, Boris},
	month = mar,
	year = {2016},
	pages = {1148--1150}
}


@article{forestwatch,
	title = {High-{Resolution} {Global} {Maps} of 21st-{Century} {Forest} {Cover} {Change}},
	volume = {342},
	url = {http://science.sciencemag.org/content/342/6160/850.abstract},
	doi = {10.1126/science.1244693},
	abstract = {Forests worldwide are in a state of flux, with accelerating losses in some regions and gains in others. Hansen et al. (p. 850) examined global Landsat data at a 30-meter spatial resolution to characterize forest extent, loss, and gain from 2000 to 2012. Globally, 2.3 million square kilometers of forest were lost during the 12-year study period and 0.8 million square kilometers of new forest were gained. The tropics exhibited both the greatest losses and the greatest gains (through regrowth and plantation), with losses outstripping gains. Quantification of global forest change has been lacking despite the recognized importance of forest ecosystem services. In this study, Earth observation satellite data were used to map global forest loss (2.3 million square kilometers) and gain (0.8 million square kilometers) from 2000 to 2012 at a spatial resolution of 30 meters. The tropics were the only climate domain to exhibit a trend, with forest loss increasing by 2101 square kilometers per year. Brazil’s well-documented reduction in deforestation was offset by increasing forest loss in Indonesia, Malaysia, Paraguay, Bolivia, Zambia, Angola, and elsewhere. Intensive forestry practiced within subtropical forests resulted in the highest rates of forest change globally. Boreal forest loss due largely to fire and forestry was second to that in the tropics in absolute and proportional terms. These results depict a globally consistent and locally relevant record of forest change.},
	number = {6160},
	journal = {Science},
	author = {Hansen, M. C. and Potapov, P. V. and Moore, R. and Hancher, M. and Turubanova, S. A. and Tyukavina, A. and Thau, D. and Stehman, S. V. and Goetz, S. J. and Loveland, T. R. and Kommareddy, A. and Egorov, A. and Chini, L. and Justice, C. O. and Townshend, J. R. G.},
	month = nov,
	year = {2013},
	pages = {850}
}


@article{Joppa2017,
	title = {The case for technology investments in the environment},
	volume = {552},
	issn = {0028-0836, 1476-4687},
	url = {http://www.nature.com/articles/d41586-017-08675-7},
	doi = {10.1038/d41586-017-08675-7},
	language = {en},
	number = {7685},
	urldate = {2021-04-17},
	journal = {Nature},
	author = {Joppa, Lucas N.},
	month = dec,
	year = {2017},
	pages = {325--328},
	file = {Full Text:/home/cboettig/Zotero/storage/WYG4P7SM/Joppa - 2017 - The case for technology investments in the environ.pdf:application/pdf}
}



@article{noise-phenomena,
author = {Boettiger, Carl},
doi = {10.1111/ele.13085},
issn = {1461023X},
journal = {Ecology Letters},
title = {{From noise to knowledge: how randomness generates novel phenomena and reveals information}},
year = {2018}
}


@article{Nature2013,
author = {Boettiger, Carl and Hastings, Alan},
doi = {10.1038/493157a},
issn = {0028-0836},
journal = {Nature},
month = {jan},
number = {7431},
pages = {157--158},
title = {{Tipping points: From patterns to predictions}},
volume = {493},
year = {2013}
}


@article{EcoApps2016,
author = {Boettiger, Carl and Bode, Michael and Sanchirico, James N. and LaRiviere, Jacob and Hastings, Alan and Armsworth, Paul R.},
doi = {10.1890/15-0236},
issn = {10510761},
journal = {Ecological Applications},
month = {apr},
number = {3},
pages = {808--817},
title = {{Optimal management of a stochastically varying population when policy adjustment is costly}},
volume = {26},
year = {2016}
}

@article{nonparametric,
author = {Boettiger, Carl and Mangel, Marc and Munch, S.},
doi = {10.1098/rspb.2014.1631},
issn = {0962-8452},
journal = {Proceedings of the Royal Society B: Biological Sciences},
keywords = {ecology,theoretical biology},
month = {jan},
number = {1801},
pages = {20141631--20141631},
pmid = {25567644},
title = {{Avoiding tipping points in fisheries management through Gaussian process dynamic programming}},
volume = {282},
year = {2015}
}


@article{pomdp-ram,
	title = {Rebuilding global fisheries under uncertainty},
	issn = {0027-8424, 1091-6490},
	url = {http://www.pnas.org/lookup/doi/10.1073/pnas.1902657116},
	doi = {10.1073/pnas.1902657116},
	language = {en},
	urldate = {2019-08-01},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Memarzadeh, Milad and Britten, Gregory L. and Worm, Boris and Boettiger, Carl},
	month = jul,
	year = {2019},
	pages = {201902657}
}


@article{taxadb,
	title = {taxadb: {A} high‐performance local taxonomic database interface},
	volume = {11},
	copyright = {All rights reserved},
	issn = {2041-210X, 2041-210X},
	shorttitle = {taxadb},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/2041-210X.13440},
	doi = {10.1111/2041-210X.13440},
	language = {en},
	number = {9},
	urldate = {2021-04-17},
	journal = {Methods in Ecology and Evolution},
	author = {Norman, Kari E. A. and Chamberlain, Scott and Boettiger, Carl},
	editor = {Price, Samantha},
	month = sep,
	year = {2020},
	pages = {1153--1159},
	file = {Full Text:/home/cboettig/Zotero/storage/E9UEFQSS/Norman et al. - 2020 - taxadb A high‐performance local taxonomic databas.pdf:application/pdf}
}


@article{RJournal2018,
  author = {Carl Boettiger and Dirk Eddelbuettel},
  title = {{An Introduction to Rocker: Docker Containers for R}},
  year = {2017},
  journal = {{The R Journal}},
  url = {https://journal.r-project.org/archive/2017/RJ-2017-065/index.html},
  pages = {527--536},
  volume = {9},
  number = {2}
}

@article{Dietze2018,
author = {Dietze, Michael C. and Fox, Andrew and Beck-Johnson, Lindsay M. and Betancourt, Julio L. and Hooten, Mevin B. and Jarnevich, Catherine S. and Keitt, Timothy H. and Kenney, Melissa A. and Laney, Christine M. and Larsen, Laurel G. and Loescher, Henry W. and Lunch, Claire K. and Pijanowski, Bryan C. and Randerson, James T. and Read, Emily K. and Tredennick, Andrew T. and Vargas, Rodrigo and Weathers, Kathleen C. and White, Ethan P.},
doi = {10.1073/pnas.1710231115},
issn = {0027-8424},
journal = {Proceedings of the National Academy of Sciences},
pages = {201710231},
pmid = {29382745},
title = {{Iterative near-term ecological forecasting: Needs, opportunities, and challenges}},
year = {2018}
}


@article{Worm2006,
author = {Worm, Boris and Barbier, Edward B and Beaumont, Nicola and Duffy, J Emmett and Folke, Carl and Halpern, Benjamin S. and Jackson, Jeremy B C and Lotze, Heike K and Micheli, Fiorenza and Palumbi, Stephen R and Sala, Enric and Selkoe, Kimberley a and Stachowicz, John J and Watson, Reg},
doi = {10.1126/science.1132294},
issn = {1095-9203},
journal = {Science (New York, N.Y.)},
month = {nov},
number = {5800},
pages = {787--90},
pmid = {17082450},
title = {{Impacts of biodiversity loss on ocean ecosystem services.}},
volume = {314},
year = {2006}
}


@article{pomdp-intro,
	title = {Resolving the {Measurement} {Uncertainty} {Paradox} in {Ecological} {Management}},
	copyright = {All rights reserved},
	issn = {0003-0147, 1537-5323},
	url = {https://www.journals.uchicago.edu/doi/10.1086/702704},
	doi = {10.1086/702704},
	language = {en},
	urldate = {2019-04-07},
	journal = {The American Naturalist},
	author = {Memarzadeh, Milad and Boettiger, Carl},
	month = apr,
	year = {2019},
	pages = {000--000}
}


@article{ppo,
	title = {Proximal {Policy} {Optimization} {Algorithms}},
	url = {http://arxiv.org/abs/1707.06347},
	urldate = {2020-04-25},
	journal = {arXiv:1707.06347 [cs]},
	author = {Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
	month = aug,
	year = {2017},
	note = {arXiv: 1707.06347},
	keywords = {Computer Science - Machine Learning},
}


@article{mbpo,
	title = {When to {Trust} {Your} {Model}: {Model}-{Based} {Policy} {Optimization}},
	shorttitle = {When to {Trust} {Your} {Model}},
	url = {http://arxiv.org/abs/1906.08253},
	urldate = {2021-05-04},
	journal = {arXiv:1906.08253 [cs, stat]},
	author = {Janner, Michael and Fu, Justin and Zhang, Marvin and Levine, Sergey},
	month = nov,
	year = {2019},
	note = {arXiv: 1906.08253},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: NeurIPS 2019. Code at https://github.com/JannerM/mbpo, project page at: https://people.eecs.berkeley.edu/{\textasciitilde}janner/mbpo/},
}

@article{a3c,
	title = {Reinforcement {Learning} through {Asynchronous} {Advantage} {Actor}-{Critic} on a {GPU}},
	url = {http://arxiv.org/abs/1611.06256},
	urldate = {2021-05-04},
	journal = {arXiv:1611.06256 [cs]},
	author = {Babaeizadeh, Mohammad and Frosio, Iuri and Tyree, Stephen and Clemons, Jason and Kautz, Jan},
	month = mar,
	year = {2017},
	note = {arXiv: 1611.06256},
	keywords = {Computer Science - Machine Learning},
}

@article{trpo,
	title = {Trust {Region} {Policy} {Optimization}},
	url = {http://arxiv.org/abs/1502.05477},
	urldate = {2021-05-04},
	journal = {arXiv:1502.05477 [cs]},
	author = {Schulman, John and Levine, Sergey and Moritz, Philipp and Jordan, Michael I. and Abbeel, Pieter},
	month = apr,
	year = {2017},
	note = {arXiv: 1502.05477},
	keywords = {Computer Science - Machine Learning},
	annote = {Comment: 16 pages, ICML 2015},
}

@article{apex,
	title = {Distributed {Prioritized} {Experience} {Replay}},
	url = {http://arxiv.org/abs/1803.00933},
	urldate = {2021-05-04},
	journal = {arXiv:1803.00933 [cs]},
	author = {Horgan, Dan and Quan, John and Budden, David and Barth-Maron, Gabriel and Hessel, Matteo and van Hasselt, Hado and Silver, David},
	month = mar,
	year = {2018},
	note = {arXiv: 1803.00933},
	keywords = {Computer Science - Machine Learning},
	annote = {Comment: Accepted to International Conference on Learning Representations 2018},
}


@article{impala,
	title = {{IMPALA}: {Scalable} {Distributed} {Deep}-{RL} with {Importance} {Weighted} {Actor}-{Learner} {Architectures}},
	shorttitle = {{IMPALA}},
	url = {http://arxiv.org/abs/1802.01561},
	urldate = {2021-05-04},
	journal = {arXiv:1802.01561 [cs]},
	author = {Espeholt, Lasse and Soyer, Hubert and Munos, Remi and Simonyan, Karen and Mnih, Volodymir and Ward, Tom and Doron, Yotam and Firoiu, Vlad and Harley, Tim and Dunning, Iain and Legg, Shane and Kavukcuoglu, Koray},
	month = jun,
	year = {2018},
	note = {arXiv: 1802.01561},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}

@article{drlthatmatters,
	title = {Deep {Reinforcement} {Learning} that {Matters}},
	url = {http://arxiv.org/abs/1709.06560},
	urldate = {2021-05-04},
	journal = {arXiv:1709.06560 [cs, stat]},
	author = {Henderson, Peter and Islam, Riashat and Bachman, Philip and Pineau, Joelle and Precup, Doina and Meger, David},
	month = jan,
	year = {2019},
	note = {arXiv: 1709.06560},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: Accepted to the Thirthy-Second AAAI Conference On Artificial Intelligence (AAAI), 2018},
}

@article{rl_brain,
	title = {Reinforcement learning in the brain},
	volume = {53},
	issn = {00222496},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0022249608001181},
	doi = {10.1016/j.jmp.2008.12.005},
	language = {en},
	number = {3},
	urldate = {2021-05-17},
	journal = {Journal of Mathematical Psychology},
	author = {Niv, Yael},
	month = jun,
	year = {2009},
	pages = {139--154},
}

@book{suttonbarto,
  title={Reinforcement learning: An introduction},
  author={Sutton, Richard S and Barto, Andrew G},
  year={2018},
  publisher={MIT press},
}


@article{qbias,
	title = {Maxmin {Q}-learning: {Controlling} the {Estimation} {Bias} of {Q}-learning},
	shorttitle = {Maxmin {Q}-learning},
	url = {http://arxiv.org/abs/2002.06487},
	urldate = {2021-05-17},
	journal = {arXiv:2002.06487 [cs]},
	author = {Lan, Qingfeng and Pan, Yangchen and Fyshe, Alona and White, Martha},
	month = feb,
	year = {2020},
	note = {arXiv: 2002.06487},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}


@article{universalapproximators,
	title = {Multilayer feedforward networks are universal approximators},
	volume = {2},
	issn = {0893-6080},
	url = {https://www.sciencedirect.com/science/article/pii/0893608089900208},
	doi = {10.1016/0893-6080(89)90020-8},
	language = {en},
	number = {5},
	urldate = {2021-05-17},
	journal = {Neural Networks},
	author = {Hornik, Kurt and Stinchcombe, Maxwell and White, Halbert},
	month = jan,
	year = {1989},
	keywords = {Back-propagation networks, Feedforward networks, Mapping networks, Network representation capability, Sigma-Pi networks, Squashing functions, Stone-Weierstrass Theorem, Universal approximation},
	pages = {359--366},
}

@InProceedings{grande14,
  title = 	 {Sample Efficient Reinforcement Learning with Gaussian Processes},
  author = 	 {Robert Grande and Thomas Walsh and Jonathan How},
  booktitle = 	 {Proceedings of the 31st International Conference on Machine Learning},
  pages = 	 {1332--1340},
  year = 	 {2014},
  editor = 	 {Eric P. Xing and Tony Jebara},
  volume = 	 {32},
  number =       {2},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Bejing, China},
  month = 	 {22--24 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v32/grande14.pdf},
  url = 	 {http://proceedings.mlr.press/v32/grande14.html},
}

@article{briefsurveydrl,
	title = {A {Brief} {Survey} of {Deep} {Reinforcement} {Learning}},
	volume = {34},
	issn = {1053-5888},
	url = {http://arxiv.org/abs/1708.05866},
	doi = {10.1109/MSP.2017.2743240},
	number = {6},
	urldate = {2021-05-17},
	journal = {IEEE Signal Processing Magazine},
	author = {Arulkumaran, Kai and Deisenroth, Marc Peter and Brundage, Miles and Bharath, Anil Anthony},
	month = nov,
	year = {2017},
	note = {arXiv: 1708.05866},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
	pages = {26--38},
}

@article{lecun_deep_2015,
	title = {Deep learning},
	volume = {521},
	issn = {0028-0836, 1476-4687},
	url = {http://www.nature.com/articles/nature14539},
	doi = {10.1038/nature14539},
	language = {en},
	number = {7553},
	urldate = {2021-05-17},
	journal = {Nature},
	author = {LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
	month = may,
	year = {2015},
	pages = {436--444},
}

@article{berger-tal_exploration-exploitation_2014,
	title = {The {Exploration}-{Exploitation} {Dilemma}: {A} {Multidisciplinary} {Framework}},
	volume = {9},
	issn = {1932-6203},
	shorttitle = {The {Exploration}-{Exploitation} {Dilemma}},
	url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0095693},
	doi = {10.1371/journal.pone.0095693},
	language = {en},
	number = {4},
	urldate = {2021-05-17},
	journal = {PLOS ONE},
	author = {Berger-Tal, Oded and Nathan, Jonathan and Meron, Ehud and Saltz, David},
	month = apr,
	year = {2014},
	note = {Publisher: Public Library of Science},
	keywords = {Animal behavior, Behavior, Decision making, Foraging, Human learning, Learning, Machine learning, Machine learning algorithms},
	pages = {e95693},
}

@article{gu_q-prop_2017,
	title = {Q-{Prop}: {Sample}-{Efficient} {Policy} {Gradient} with {An} {Off}-{Policy} {Critic}},
	shorttitle = {Q-{Prop}},
	url = {http://arxiv.org/abs/1611.02247},
	urldate = {2021-05-17},
	journal = {arXiv:1611.02247 [cs]},
	author = {Gu, Shixiang and Lillicrap, Timothy and Ghahramani, Zoubin and Turner, Richard E. and Levine, Sergey},
	month = feb,
	year = {2017},
	note = {arXiv: 1611.02247},
	keywords = {Computer Science - Machine Learning},
}

@article{van_hasselt_deep_2015,
	title = {Deep {Reinforcement} {Learning} with {Double} {Q}-learning},
	url = {http://arxiv.org/abs/1509.06461},
	urldate = {2021-05-18},
	journal = {arXiv:1509.06461 [cs]},
	author = {van Hasselt, Hado and Guez, Arthur and Silver, David},
	month = dec,
	year = {2015},
	note = {arXiv: 1509.06461},
	keywords = {Computer Science - Machine Learning},
}


@article{wang_sample_2017,
	title = {Sample {Efficient} {Actor}-{Critic} with {Experience} {Replay}},
	url = {http://arxiv.org/abs/1611.01224},
	urldate = {2021-05-18},
	journal = {arXiv:1611.01224 [cs]},
	author = {Wang, Ziyu and Bapst, Victor and Heess, Nicolas and Mnih, Volodymyr and Munos, Remi and Kavukcuoglu, Koray and de Freitas, Nando},
	month = jul,
	year = {2017},
	note = {arXiv: 1611.01224},
	keywords = {Computer Science - Machine Learning},
}