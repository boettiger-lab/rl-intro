
@article{brockman2016,
	title = {{OpenAI} {Gym}},
	url = {http://arxiv.org/abs/1606.01540},
	abstract = {OpenAI Gym is a toolkit for reinforcement learning research. It includes a growing collection of benchmark problems that expose a common interface, and a website where people can share their results and compare the performance of algorithms. This whitepaper discusses the components of OpenAI Gym and the design decisions that went into the software.},
	urldate = {2021-04-29},
	journal = {arXiv:1606.01540 [cs]},
	author = {Brockman, Greg and Cheung, Vicki and Pettersson, Ludwig and Schneider, Jonas and Schulman, John and Tang, Jie and Zaremba, Wojciech},
	month = jun,
	year = {2016},
	note = {arXiv: 1606.01540},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/home/cboettig/Zotero/storage/AIS2GFG4/Brockman et al. - 2016 - OpenAI Gym.pdf:application/pdf;arXiv.org Snapshot:/home/cboettig/Zotero/storage/62NTAUKT/1606.html:text/html}
}

@article{openai_costs,
	title = {Measuring the {Algorithmic} {Efficiency} of {Neural} {Networks}},
	url = {http://arxiv.org/abs/2005.04305},
	abstract = {Three factors drive the advance of AI: algorithmic innovation, data, and the amount of compute available for training. Algorithmic progress has traditionally been more difficult to quantify than compute and data. In this work, we argue that algorithmic progress has an aspect that is both straightforward to measure and interesting: reductions over time in the compute needed to reach past capabilities. We show that the number of floating-point operations required to train a classifier to AlexNet-level performance on ImageNet has decreased by a factor of 44x between 2012 and 2019. This corresponds to algorithmic efficiency doubling every 16 months over a period of 7 years. By contrast, Moore's Law would only have yielded an 11x cost improvement. We observe that hardware and algorithmic efficiency gains multiply and can be on a similar scale over meaningful horizons, which suggests that a good model of AI progress should integrate measures from both.},
	urldate = {2021-05-11},
	journal = {arXiv:2005.04305 [cs, stat]},
	author = {Hernandez, Danny and Brown, Tom B.},
	month = may,
	year = {2020},
	note = {arXiv: 2005.04305},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: 20 pages, 5 figures},
	file = {arXiv Fulltext PDF:/home/cboettig/Zotero/storage/UGAGPLRA/Hernandez and Brown - 2020 - Measuring the Algorithmic Efficiency of Neural Net.pdf:application/pdf;arXiv.org Snapshot:/home/cboettig/Zotero/storage/7DBDL3IU/2005.html:text/html}
}

@article{biodiversity,
 title={Defaunation in the Anthropocene},
  author={Dirzo, Rodolfo and Young, Hillary S and Galetti, Mauro and Ceballos, Gerardo and Isaac, Nick JB and Collen, Ben},
   journal={science},
   volume={345},
   number={6195},
   pages={401--406},
   year={2014},
   publisher={American Association for the Advancement of Science}
 }

@article{sac,
	title = {Soft {Actor}-{Critic}: {Off}-{Policy} {Maximum} {Entropy} {Deep} {Reinforcement} {Learning} with a {Stochastic} {Actor}},
	shorttitle = {Soft {Actor}-{Critic}},
	url = {http://arxiv.org/abs/1801.01290},
	abstract = {Model-free deep reinforcement learning (RL) algorithms have been demonstrated on a range of challenging decision making and control tasks. However, these methods typically suffer from two major challenges: very high sample complexity and brittle convergence properties, which necessitate meticulous hyperparameter tuning. Both of these challenges severely limit the applicability of such methods to complex, real-world domains. In this paper, we propose soft actor-critic, an off-policy actor-critic deep RL algorithm based on the maximum entropy reinforcement learning framework. In this framework, the actor aims to maximize expected reward while also maximizing entropy. That is, to succeed at the task while acting as randomly as possible. Prior deep RL methods based on this framework have been formulated as Q-learning methods. By combining off-policy updates with a stable stochastic actor-critic formulation, our method achieves state-of-the-art performance on a range of continuous control benchmark tasks, outperforming prior on-policy and off-policy methods. Furthermore, we demonstrate that, in contrast to other off-policy algorithms, our approach is very stable, achieving very similar performance across different random seeds.},
	urldate = {2021-05-05},
	journal = {arXiv:1801.01290 [cs, stat]},
	author = {Haarnoja, Tuomas and Zhou, Aurick and Abbeel, Pieter and Levine, Sergey},
	month = aug,
	year = {2018},
	note = {arXiv: 1801.01290},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: ICML 2018 Videos: sites.google.com/view/soft-actor-critic Code: github.com/haarnoja/sac},
	file = {arXiv Fulltext PDF:/home/cboettig/Zotero/storage/EMT3HQTG/Haarnoja et al. - 2018 - Soft Actor-Critic Off-Policy Maximum Entropy Deep.pdf:application/pdf;arXiv.org Snapshot:/home/cboettig/Zotero/storage/TRBDQEN5/1801.html:text/html}
}

@article{TD3,
	title = {Addressing {Function} {Approximation} {Error} in {Actor}-{Critic} {Methods}},
	url = {http://arxiv.org/abs/1802.09477},
	abstract = {In value-based reinforcement learning methods such as deep Q-learning, function approximation errors are known to lead to overestimated value estimates and suboptimal policies. We show that this problem persists in an actor-critic setting and propose novel mechanisms to minimize its effects on both the actor and the critic. Our algorithm builds on Double Q-learning, by taking the minimum value between a pair of critics to limit overestimation. We draw the connection between target networks and overestimation bias, and suggest delaying policy updates to reduce per-update error and further improve performance. We evaluate our method on the suite of OpenAI gym tasks, outperforming the state of the art in every environment tested.},
	urldate = {2021-05-05},
	journal = {arXiv:1802.09477 [cs, stat]},
	author = {Fujimoto, Scott and van Hoof, Herke and Meger, David},
	month = oct,
	year = {2018},
	note = {arXiv: 1802.09477},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: Accepted at ICML 2018},
	file = {arXiv Fulltext PDF:/home/cboettig/Zotero/storage/7QWN2IXY/Fujimoto et al. - 2018 - Addressing Function Approximation Error in Actor-C.pdf:application/pdf;arXiv.org Snapshot:/home/cboettig/Zotero/storage/8QGUZGAP/1802.html:text/html}
}

@article{her,
	title = {Hindsight {Experience} {Replay}},
	url = {http://arxiv.org/abs/1707.01495},
	abstract = {Dealing with sparse rewards is one of the biggest challenges in Reinforcement Learning (RL). We present a novel technique called Hindsight Experience Replay which allows sample-efficient learning from rewards which are sparse and binary and therefore avoid the need for complicated reward engineering. It can be combined with an arbitrary off-policy RL algorithm and may be seen as a form of implicit curriculum. We demonstrate our approach on the task of manipulating objects with a robotic arm. In particular, we run experiments on three different tasks: pushing, sliding, and pick-and-place, in each case using only binary rewards indicating whether or not the task is completed. Our ablation studies show that Hindsight Experience Replay is a crucial ingredient which makes training possible in these challenging environments. We show that our policies trained on a physics simulation can be deployed on a physical robot and successfully complete the task.},
	urldate = {2021-05-05},
	journal = {arXiv:1707.01495 [cs]},
	author = {Andrychowicz, Marcin and Wolski, Filip and Ray, Alex and Schneider, Jonas and Fong, Rachel and Welinder, Peter and McGrew, Bob and Tobin, Josh and Abbeel, Pieter and Zaremba, Wojciech},
	month = feb,
	year = {2018},
	note = {arXiv: 1707.01495},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Computer Science - Robotics},
	file = {arXiv Fulltext PDF:/home/cboettig/Zotero/storage/FH3QSE54/Andrychowicz et al. - 2018 - Hindsight Experience Replay.pdf:application/pdf;arXiv.org Snapshot:/home/cboettig/Zotero/storage/DH3PBZBY/1707.html:text/html}
}

@article{DQN,
	title = {Human-level control through deep reinforcement learning},
	volume = {518},
	issn = {0028-0836, 1476-4687},
	url = {http://www.nature.com/articles/nature14236},
	doi = {10.1038/nature14236},
	language = {en},
	number = {7540},
	urldate = {2021-05-05},
	journal = {Nature},
	author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A. and Veness, Joel and Bellemare, Marc G. and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K. and Ostrovski, Georg and Petersen, Stig and Beattie, Charles and Sadik, Amir and Antonoglou, Ioannis and King, Helen and Kumaran, Dharshan and Wierstra, Daan and Legg, Shane and Hassabis, Demis},
	month = feb,
	year = {2015},
	pages = {529--533}
}

@article{dqn_arxiv,
	title = {Playing {Atari} with {Deep} {Reinforcement} {Learning}},
	url = {http://arxiv.org/abs/1312.5602},
	abstract = {We present the first deep learning model to successfully learn control policies directly from high-dimensional sensory input using reinforcement learning. The model is a convolutional neural network, trained with a variant of Q-learning, whose input is raw pixels and whose output is a value function estimating future rewards. We apply our method to seven Atari 2600 games from the Arcade Learning Environment, with no adjustment of the architecture or learning algorithm. We find that it outperforms all previous approaches on six of the games and surpasses a human expert on three of them.},
	urldate = {2021-05-05},
	journal = {arXiv:1312.5602 [cs]},
	author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Graves, Alex and Antonoglou, Ioannis and Wierstra, Daan and Riedmiller, Martin},
	month = dec,
	year = {2013},
	note = {arXiv: 1312.5602},
	keywords = {Computer Science - Machine Learning},
	annote = {Comment: NIPS Deep Learning Workshop 2013},
	file = {arXiv Fulltext PDF:/home/cboettig/Zotero/storage/9759TK3T/Mnih et al. - 2013 - Playing Atari with Deep Reinforcement Learning.pdf:application/pdf;arXiv.org Snapshot:/home/cboettig/Zotero/storage/927GQNHK/1312.html:text/html}
}

@article{ddpg,
	title = {Continuous control with deep reinforcement learning},
	url = {http://arxiv.org/abs/1509.02971},
	abstract = {We adapt the ideas underlying the success of Deep Q-Learning to the continuous action domain. We present an actor-critic, model-free algorithm based on the deterministic policy gradient that can operate over continuous action spaces. Using the same learning algorithm, network architecture and hyper-parameters, our algorithm robustly solves more than 20 simulated physics tasks, including classic problems such as cartpole swing-up, dexterous manipulation, legged locomotion and car driving. Our algorithm is able to find policies whose performance is competitive with those found by a planning algorithm with full access to the dynamics of the domain and its derivatives. We further demonstrate that for many of the tasks the algorithm can learn policies end-to-end: directly from raw pixel inputs.},
	urldate = {2021-05-05},
	journal = {arXiv:1509.02971 [cs, stat]},
	author = {Lillicrap, Timothy P. and Hunt, Jonathan J. and Pritzel, Alexander and Heess, Nicolas and Erez, Tom and Tassa, Yuval and Silver, David and Wierstra, Daan},
	month = jul,
	year = {2019},
	note = {arXiv: 1509.02971},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: 10 pages + supplementary},
	file = {arXiv Fulltext PDF:/home/cboettig/Zotero/storage/AJNAETMB/Lillicrap et al. - 2019 - Continuous control with deep reinforcement learnin.pdf:application/pdf;arXiv.org Snapshot:/home/cboettig/Zotero/storage/SL6FXX4V/1509.html:text/html}
}

@article{A2C,
	title = {Asynchronous {Methods} for {Deep} {Reinforcement} {Learning}},
	url = {http://arxiv.org/abs/1602.01783},
	abstract = {We propose a conceptually simple and lightweight framework for deep reinforcement learning that uses asynchronous gradient descent for optimization of deep neural network controllers. We present asynchronous variants of four standard reinforcement learning algorithms and show that parallel actor-learners have a stabilizing effect on training allowing all four methods to successfully train neural network controllers. The best performing method, an asynchronous variant of actor-critic, surpasses the current state-of-the-art on the Atari domain while training for half the time on a single multi-core CPU instead of a GPU. Furthermore, we show that asynchronous actor-critic succeeds on a wide variety of continuous motor control problems as well as on a new task of navigating random 3D mazes using a visual input.},
	urldate = {2021-05-05},
	journal = {arXiv:1602.01783 [cs]},
	author = {Mnih, Volodymyr and Badia, Adrià Puigdomènech and Mirza, Mehdi and Graves, Alex and Lillicrap, Timothy P. and Harley, Tim and Silver, David and Kavukcuoglu, Koray},
	month = jun,
	year = {2016},
	note = {arXiv: 1602.01783},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/home/cboettig/Zotero/storage/PHJJPHS2/Mnih et al. - 2016 - Asynchronous Methods for Deep Reinforcement Learni.pdf:application/pdf;arXiv.org Snapshot:/home/cboettig/Zotero/storage/YDTHBLKH/1602.html:text/html}
}


@misc{sb3,
  author = {Raffin, Antonin and Hill, Ashley and Ernestus, Maximilian and Gleave, Adam and Kanervisto, Anssi and Dormann, Noah},
  title = {Stable Baselines3},
  year = {2019},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/DLR-RM/stable-baselines3}},
}

@misc{tf-agents,
  title = {{TF-Agents}: A library for Reinforcement Learning in TensorFlow},
  author = {Sergio Guadarrama and Anoop Korattikara and Oscar Ramirez and
     Pablo Castro and Ethan Holly and Sam Fishman and Ke Wang and
     Ekaterina Gonina and Neal Wu and Efi Kokiopoulou and Luciano Sbaiz and
     Jamie Smith and Gábor Bartók and Jesse Berent and Chris Harris and
     Vincent Vanhoucke and Eugene Brevdo},
  howpublished = {\url{https://github.com/tensorflow/agents}},
  url = "https://github.com/tensorflow/agents",
  year = 2018,
  note = "[Online; accessed May 2021]"
}

@inproceedings{kour2014real,
  title={Real-time segmentation of on-line handwritten arabic script},
  author={Kour, George and Saabne, Raid},
  booktitle={Frontiers in Handwriting Recognition (ICFHR), 2014 14th International Conference on},
  pages={417--422},
  year={2014},
  organization={IEEE}
}

@inproceedings{kour2014fast,
  title={Fast classification of handwritten on-line Arabic characters},
  author={Kour, George and Saabne, Raid},
  booktitle={Soft Computing and Pattern Recognition (SoCPaR), 2014 6th International Conference of},
  pages={312--318},
  year={2014},
  organization={IEEE}
}

@article{hadash2018estimate,
  title={Estimate and Replace: A Novel Approach to Integrating Deep Neural Networks with Existing Applications},
  author={Hadash, Guy and Kermany, Einat and Carmeli, Boaz and Lavi, Ofer and Kour, George and Jacovi, Alon},
  journal={arXiv preprint arXiv:1804.09028},
  year={2018}
}


@article{alphazero,
	title = {A general reinforcement learning algorithm that masters chess, shogi, and {Go} through self-play},
	volume = {362},
	issn = {0036-8075, 1095-9203},
	url = {https://www.sciencemag.org/lookup/doi/10.1126/science.aar6404},
	doi = {10.1126/science.aar6404},
	abstract = {The game of chess is the longest-studied domain in the history of artificial intelligence. The strongest programs are based on a combination of sophisticated search techniques, domain-specific adaptations, and handcrafted evaluation functions that have been refined by human experts over several decades. By contrast, the AlphaGo Zero program recently achieved superhuman performance in the game of Go by reinforcement learning from self-play. In this paper, we generalize this approach into a single AlphaZero algorithm that can achieve superhuman performance in many challenging games. Starting from random play and given no domain knowledge except the game rules, AlphaZero convincingly defeated a world champion program in the games of chess and shogi (Japanese chess), as well as Go.},
	language = {en},
	number = {6419},
	urldate = {2021-05-07},
	journal = {Science},
	author = {Silver, David and Hubert, Thomas and Schrittwieser, Julian and Antonoglou, Ioannis and Lai, Matthew and Guez, Arthur and Lanctot, Marc and Sifre, Laurent and Kumaran, Dharshan and Graepel, Thore and Lillicrap, Timothy and Simonyan, Karen and Hassabis, Demis},
	month = dec,
	year = {2018},
	pages = {1140--1144},
	file = {Full Text:/home/cboettig/Zotero/storage/CMYED7HD/Silver et al. - 2018 - A general reinforcement learning algorithm that ma.pdf:application/pdf}
}


@article{Pong2020,
	title = {Temporal {Difference} {Models}: {Model}-{Free} {Deep} {RL} for {Model}-{Based} {Control}},
	shorttitle = {Temporal {Difference} {Models}},
	url = {http://arxiv.org/abs/1802.09081},
	abstract = {Model-free reinforcement learning (RL) is a powerful, general tool for learning complex behaviors. However, its sample efficiency is often impractically large for solving challenging real-world problems, even with off-policy algorithms such as Q-learning. A limiting factor in classic model-free RL is that the learning signal consists only of scalar rewards, ignoring much of the rich information contained in state transition tuples. Model-based RL uses this information, by training a predictive model, but often does not achieve the same asymptotic performance as model-free RL due to model bias. We introduce temporal difference models (TDMs), a family of goal-conditioned value functions that can be trained with model-free learning and used for model-based control. TDMs combine the benefits of model-free and model-based RL: they leverage the rich information in state transitions to learn very efficiently, while still attaining asymptotic performance that exceeds that of direct model-based RL methods. Our experimental results show that, on a range of continuous control tasks, TDMs provide a substantial improvement in efficiency compared to state-of-the-art model-based and model-free methods.},
	urldate = {2021-05-25},
	journal = {arXiv:1802.09081 [cs]},
	author = {Pong, Vitchyr and Gu, Shixiang and Dalal, Murtaza and Levine, Sergey},
	month = feb,
	year = {2020},
	note = {arXiv: 1802.09081},
	keywords = {Computer Science - Machine Learning},
	annote = {Comment: Appeared in ICLR 2018; typos corrected},
	file = {arXiv Fulltext PDF:/home/cboettig/Zotero/storage/AU5CTSZT/Pong et al. - 2020 - Temporal Difference Models Model-Free Deep RL for.pdf:application/pdf;arXiv.org Snapshot:/home/cboettig/Zotero/storage/ZJD3I26K/1802.html:text/html}
}


@article{alphaGoZero,
	title = {Mastering the game of {Go} without human knowledge},
	volume = {550},
	issn = {0028-0836, 1476-4687},
	url = {http://www.nature.com/articles/nature24270},
	doi = {10.1038/nature24270},
	language = {en},
	number = {7676},
	urldate = {2021-05-07},
	journal = {Nature},
	author = {Silver, David and Schrittwieser, Julian and Simonyan, Karen and Antonoglou, Ioannis and Huang, Aja and Guez, Arthur and Hubert, Thomas and Baker, Lucas and Lai, Matthew and Bolton, Adrian and Chen, Yutian and Lillicrap, Timothy and Hui, Fan and Sifre, Laurent and van den Driessche, George and Graepel, Thore and Hassabis, Demis},
	month = oct,
	year = {2017},
	pages = {354--359},
	file = {Submitted Version:/home/cboettig/Zotero/storage/S7M2HI28/Silver et al. - 2017 - Mastering the game of Go without human knowledge.pdf:application/pdf}
}


@article{alphaGo2016,
	title = {Mastering the game of {Go} with deep neural networks and tree search},
	volume = {529},
	issn = {0028-0836, 1476-4687},
	url = {http://www.nature.com/articles/nature16961},
	doi = {10.1038/nature16961},
	language = {en},
	number = {7587},
	urldate = {2021-05-07},
	journal = {Nature},
	author = {Silver, David and Huang, Aja and Maddison, Chris J. and Guez, Arthur and Sifre, Laurent and van den Driessche, George and Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam, Veda and Lanctot, Marc and Dieleman, Sander and Grewe, Dominik and Nham, John and Kalchbrenner, Nal and Sutskever, Ilya and Lillicrap, Timothy and Leach, Madeleine and Kavukcuoglu, Koray and Graepel, Thore and Hassabis, Demis},
	month = jan,
	year = {2016},
	pages = {484--489}
}


@article{covid,
	title = {Ecology and economics for pandemic prevention},
	volume = {369},
	issn = {0036-8075},
	url = {https://science.sciencemag.org/content/369/6502/379},
	doi = {10.1126/science.abc3189},
	number = {6502},
	journal = {Science},
	author = {Dobson, Andrew P. and Pimm, Stuart L. and Hannah, Lee and Kaufman, Les and Ahumada, Jorge A. and Ando, Amy W. and Bernstein, Aaron and Busch, Jonah and Daszak, Peter and Engelmann, Jens and Kinnaird, Margaret F. and Li, Binbin V. and Loch-Temzelides, Ted and Lovejoy, Thomas and Nowak, Katarzyna and Roehrdanz, Patrick R. and Vale, Mariana M.},
	year = {2020},
	pages = {379--381}
}

@article{fire_2,
	title = {Reform forest fire management},
	volume = {349},
	issn = {0036-8075, 1095-9203},
	url = {https://www.sciencemag.org/lookup/doi/10.1126/science.aab2356},
	doi = {10.1126/science.aab2356},
	language = {en},
	number = {6254},
	urldate = {2021-04-16},
	journal = {Science},
	author = {North, M. P. and Stephens, S. L. and Collins, B. M. and Agee, J. K. and Aplet, G. and Franklin, J. F. and Fule, P. Z.},
	month = sep,
	year = {2015},
	pages = {1280--1281}
}

@article{wildfire,
	title = {Learning to coexist with wildfire},
	volume = {515},
	issn = {0028-0836, 1476-4687},
	url = {http://www.nature.com/articles/nature13946},
	doi = {10.1038/nature13946},
	language = {en},
	number = {7525},
	urldate = {2021-04-16},
	journal = {Nature},
	author = {Moritz, Max A. and Batllori, Enric and Bradstock, Ross A. and Gill, A. Malcolm and Handmer, John and Hessburg, Paul F. and Leonard, Justin and McCaffrey, Sarah and Odion, Dennis C. and Schoennagel, Tania and Syphard, Alexandra D.},
	month = nov,
	year = {2014},
	pages = {58--66}
}


@article{biodiversity_oceans,
	title = {Protecting the global ocean for biodiversity, food and climate},
	volume = {592},
	issn = {0028-0836, 1476-4687},
	url = {http://www.nature.com/articles/s41586-021-03371-z},
	doi = {10.1038/s41586-021-03371-z},
	language = {en},
	number = {7854},
	urldate = {2021-04-16},
	journal = {Nature},
	author = {Sala, Enric and Mayorga, Juan and Bradley, Darcy and Cabral, Reniel B. and Atwood, Trisha B. and Auber, Arnaud and Cheung, William and Costello, Christopher and Ferretti, Francesco and Friedlander, Alan M. and Gaines, Steven D. and Garilao, Cristina and Goodell, Whitney and Halpern, Benjamin S. and Hinson, Audra and Kaschner, Kristin and Kesner-Reyes, Kathleen and Leprieur, Fabien and McGowan, Jennifer and Morgan, Lance E. and Mouillot, David and Palacios-Abrantes, Juliano and Possingham, Hugh P. and Rechberger, Kristin D. and Worm, Boris and Lubchenco, Jane},
	month = apr,
	year = {2021},
	pages = {397--402}
}


@article{fishwatch,
	title = {Ending hide and seek at sea},
	volume = {351},
	issn = {0036-8075, 1095-9203},
	url = {https://www.sciencemag.org/lookup/doi/10.1126/science.aad5686},
	doi = {10.1126/science.aad5686},
	language = {en},
	number = {6278},
	urldate = {2021-04-17},
	journal = {Science},
	author = {McCauley, Douglas J. and Woods, Paul and Sullivan, Brian and Bergman, Bjorn and Jablonicky, Caroline and Roan, Aaron and Hirshfield, Michael and Boerder, Kristina and Worm, Boris},
	month = mar,
	year = {2016},
	pages = {1148--1150}
}


@article{forestwatch,
	title = {High-{Resolution} {Global} {Maps} of 21st-{Century} {Forest} {Cover} {Change}},
	volume = {342},
	url = {http://science.sciencemag.org/content/342/6160/850.abstract},
	doi = {10.1126/science.1244693},
	abstract = {Forests worldwide are in a state of flux, with accelerating losses in some regions and gains in others. Hansen et al. (p. 850) examined global Landsat data at a 30-meter spatial resolution to characterize forest extent, loss, and gain from 2000 to 2012. Globally, 2.3 million square kilometers of forest were lost during the 12-year study period and 0.8 million square kilometers of new forest were gained. The tropics exhibited both the greatest losses and the greatest gains (through regrowth and plantation), with losses outstripping gains. Quantification of global forest change has been lacking despite the recognized importance of forest ecosystem services. In this study, Earth observation satellite data were used to map global forest loss (2.3 million square kilometers) and gain (0.8 million square kilometers) from 2000 to 2012 at a spatial resolution of 30 meters. The tropics were the only climate domain to exhibit a trend, with forest loss increasing by 2101 square kilometers per year. Brazil’s well-documented reduction in deforestation was offset by increasing forest loss in Indonesia, Malaysia, Paraguay, Bolivia, Zambia, Angola, and elsewhere. Intensive forestry practiced within subtropical forests resulted in the highest rates of forest change globally. Boreal forest loss due largely to fire and forestry was second to that in the tropics in absolute and proportional terms. These results depict a globally consistent and locally relevant record of forest change.},
	number = {6160},
	journal = {Science},
	author = {Hansen, M. C. and Potapov, P. V. and Moore, R. and Hancher, M. and Turubanova, S. A. and Tyukavina, A. and Thau, D. and Stehman, S. V. and Goetz, S. J. and Loveland, T. R. and Kommareddy, A. and Egorov, A. and Chini, L. and Justice, C. O. and Townshend, J. R. G.},
	month = nov,
	year = {2013},
	pages = {850}
}


@article{Joppa2017,
	title = {The case for technology investments in the environment},
	volume = {552},
	issn = {0028-0836, 1476-4687},
	url = {http://www.nature.com/articles/d41586-017-08675-7},
	doi = {10.1038/d41586-017-08675-7},
	language = {en},
	number = {7685},
	urldate = {2021-04-17},
	journal = {Nature},
	author = {Joppa, Lucas N.},
	month = dec,
	year = {2017},
	pages = {325--328},
	file = {Full Text:/home/cboettig/Zotero/storage/WYG4P7SM/Joppa - 2017 - The case for technology investments in the environ.pdf:application/pdf}
}



@article{noise-phenomena,
author = {Boettiger, Carl},
doi = {10.1111/ele.13085},
issn = {1461023X},
journal = {Ecology Letters},
title = {{From noise to knowledge: how randomness generates novel phenomena and reveals information}},
year = {2018}
}


@article{Nature2013,
author = {Boettiger, Carl and Hastings, Alan},
doi = {10.1038/493157a},
issn = {0028-0836},
journal = {Nature},
month = {jan},
number = {7431},
pages = {157--158},
title = {{Tipping points: From patterns to predictions}},
volume = {493},
year = {2013}
}


@article{EcoApps2016,
author = {Boettiger, Carl and Bode, Michael and Sanchirico, James N. and LaRiviere, Jacob and Hastings, Alan and Armsworth, Paul R.},
doi = {10.1890/15-0236},
issn = {10510761},
journal = {Ecological Applications},
month = {apr},
number = {3},
pages = {808--817},
title = {{Optimal management of a stochastically varying population when policy adjustment is costly}},
volume = {26},
year = {2016}
}

@article{nonparametric,
author = {Boettiger, Carl and Mangel, Marc and Munch, S.},
doi = {10.1098/rspb.2014.1631},
issn = {0962-8452},
journal = {Proceedings of the Royal Society B: Biological Sciences},
keywords = {ecology,theoretical biology},
month = {jan},
number = {1801},
pages = {20141631--20141631},
pmid = {25567644},
title = {{Avoiding tipping points in fisheries management through Gaussian process dynamic programming}},
volume = {282},
year = {2015}
}


@article{pomdp-ram,
	title = {Rebuilding global fisheries under uncertainty},
	issn = {0027-8424, 1091-6490},
	url = {http://www.pnas.org/lookup/doi/10.1073/pnas.1902657116},
	doi = {10.1073/pnas.1902657116},
	language = {en},
	urldate = {2019-08-01},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Memarzadeh, Milad and Britten, Gregory L. and Worm, Boris and Boettiger, Carl},
	month = jul,
	year = {2019},
	pages = {201902657}
}


@article{taxadb,
	title = {taxadb: {A} high‐performance local taxonomic database interface},
	volume = {11},
	copyright = {All rights reserved},
	issn = {2041-210X, 2041-210X},
	shorttitle = {taxadb},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/2041-210X.13440},
	doi = {10.1111/2041-210X.13440},
	language = {en},
	number = {9},
	urldate = {2021-04-17},
	journal = {Methods in Ecology and Evolution},
	author = {Norman, Kari E. A. and Chamberlain, Scott and Boettiger, Carl},
	editor = {Price, Samantha},
	month = sep,
	year = {2020},
	pages = {1153--1159},
	file = {Full Text:/home/cboettig/Zotero/storage/E9UEFQSS/Norman et al. - 2020 - taxadb A high‐performance local taxonomic databas.pdf:application/pdf}
}


@article{RJournal2018,
  author = {Carl Boettiger and Dirk Eddelbuettel},
  title = {{An Introduction to Rocker: Docker Containers for R}},
  year = {2017},
  journal = {{The R Journal}},
  url = {https://journal.r-project.org/archive/2017/RJ-2017-065/index.html},
  pages = {527--536},
  volume = {9},
  number = {2}
}

@article{Dietze2018,
author = {Dietze, Michael C. and Fox, Andrew and Beck-Johnson, Lindsay M. and Betancourt, Julio L. and Hooten, Mevin B. and Jarnevich, Catherine S. and Keitt, Timothy H. and Kenney, Melissa A. and Laney, Christine M. and Larsen, Laurel G. and Loescher, Henry W. and Lunch, Claire K. and Pijanowski, Bryan C. and Randerson, James T. and Read, Emily K. and Tredennick, Andrew T. and Vargas, Rodrigo and Weathers, Kathleen C. and White, Ethan P.},
doi = {10.1073/pnas.1710231115},
issn = {0027-8424},
journal = {Proceedings of the National Academy of Sciences},
pages = {201710231},
pmid = {29382745},
title = {{Iterative near-term ecological forecasting: Needs, opportunities, and challenges}},
year = {2018}
}


@article{Worm2006,
author = {Worm, Boris and Barbier, Edward B and Beaumont, Nicola and Duffy, J Emmett and Folke, Carl and Halpern, Benjamin S. and Jackson, Jeremy B C and Lotze, Heike K and Micheli, Fiorenza and Palumbi, Stephen R and Sala, Enric and Selkoe, Kimberley a and Stachowicz, John J and Watson, Reg},
doi = {10.1126/science.1132294},
issn = {1095-9203},
journal = {Science (New York, N.Y.)},
month = {nov},
number = {5800},
pages = {787--90},
pmid = {17082450},
title = {{Impacts of biodiversity loss on ocean ecosystem services.}},
volume = {314},
year = {2006}
}


@article{pomdp-intro,
	title = {Resolving the {Measurement} {Uncertainty} {Paradox} in {Ecological} {Management}},
	copyright = {All rights reserved},
	issn = {0003-0147, 1537-5323},
	url = {https://www.journals.uchicago.edu/doi/10.1086/702704},
	doi = {10.1086/702704},
	language = {en},
	urldate = {2019-04-07},
	journal = {The American Naturalist},
	author = {Memarzadeh, Milad and Boettiger, Carl},
	month = apr,
	year = {2019},
	pages = {000--000}
}


@article{Possingham2013,
	title = {Predicting species distributions for conservation decisions},
	volume = {16},
	issn = {1461-023X, 1461-0248},
	url = {https://onlinelibrary.wiley.com/doi/10.1111/ele.12189},
	doi = {10.1111/ele.12189},
	language = {en},
	number = {12},
	urldate = {2021-05-07},
	journal = {Ecology Letters},
	author = {Guisan, Antoine and Tingley, Reid and Baumgartner, John B. and Naujokaitis‐Lewis, Ilona and Sutcliffe, Patricia R. and Tulloch, Ayesha I. T. and Regan, Tracey J. and Brotons, Lluis and McDonald‐Madden, Eve and Mantyka‐Pringle, Chrystal and Martin, Tara G. and Rhodes, Jonathan R. and Maggini, Ramona and Setterfield, Samantha A. and Elith, Jane and Schwartz, Mark W. and Wintle, Brendan A. and Broennimann, Olivier and Austin, Mike and Ferrier, Simon and Kearney, Michael R. and Possingham, Hugh P. and Buckley, Yvonne M.},
	editor = {Arita, Hector},
	month = dec,
	year = {2013},
	pages = {1424--1435},
	file = {Full Text:/home/cboettig/Zotero/storage/I4VERGLV/Guisan et al. - 2013 - Predicting species distributions for conservation .pdf:application/pdf}
}

@article{ppo,
	title = {Proximal {Policy} {Optimization} {Algorithms}},
	url = {http://arxiv.org/abs/1707.06347},
	urldate = {2020-04-25},
	journal = {arXiv:1707.06347 [cs]},
	author = {Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
	month = aug,
	year = {2017},
	note = {arXiv: 1707.06347},
	keywords = {Computer Science - Machine Learning},
}


@article{mbpo,
	title = {When to {Trust} {Your} {Model}: {Model}-{Based} {Policy} {Optimization}},
	shorttitle = {When to {Trust} {Your} {Model}},
	url = {http://arxiv.org/abs/1906.08253},
	urldate = {2021-05-04},
	journal = {arXiv:1906.08253 [cs, stat]},
	author = {Janner, Michael and Fu, Justin and Zhang, Marvin and Levine, Sergey},
	month = nov,
	year = {2019},
	note = {arXiv: 1906.08253},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: NeurIPS 2019. Code at https://github.com/JannerM/mbpo, project page at: https://people.eecs.berkeley.edu/{\textasciitilde}janner/mbpo/},
}

@article{a3c,
	title = {Reinforcement {Learning} through {Asynchronous} {Advantage} {Actor}-{Critic} on a {GPU}},
	url = {http://arxiv.org/abs/1611.06256},
	urldate = {2021-05-04},
	journal = {arXiv:1611.06256 [cs]},
	author = {Babaeizadeh, Mohammad and Frosio, Iuri and Tyree, Stephen and Clemons, Jason and Kautz, Jan},
	month = mar,
	year = {2017},
	note = {arXiv: 1611.06256},
	keywords = {Computer Science - Machine Learning},
}

@article{trpo,
	title = {Trust {Region} {Policy} {Optimization}},
	url = {http://arxiv.org/abs/1502.05477},
	urldate = {2021-05-04},
	journal = {arXiv:1502.05477 [cs]},
	author = {Schulman, John and Levine, Sergey and Moritz, Philipp and Jordan, Michael I. and Abbeel, Pieter},
	month = apr,
	year = {2017},
	note = {arXiv: 1502.05477},
	keywords = {Computer Science - Machine Learning},
	annote = {Comment: 16 pages, ICML 2015},
}

@article{apex,
	title = {Distributed {Prioritized} {Experience} {Replay}},
	url = {http://arxiv.org/abs/1803.00933},
	urldate = {2021-05-04},
	journal = {arXiv:1803.00933 [cs]},
	author = {Horgan, Dan and Quan, John and Budden, David and Barth-Maron, Gabriel and Hessel, Matteo and van Hasselt, Hado and Silver, David},
	month = mar,
	year = {2018},
	note = {arXiv: 1803.00933},
	keywords = {Computer Science - Machine Learning},
	annote = {Comment: Accepted to International Conference on Learning Representations 2018},
}


@article{impala,
	title = {{IMPALA}: {Scalable} {Distributed} {Deep}-{RL} with {Importance} {Weighted} {Actor}-{Learner} {Architectures}},
	shorttitle = {{IMPALA}},
	url = {http://arxiv.org/abs/1802.01561},
	urldate = {2021-05-04},
	journal = {arXiv:1802.01561 [cs]},
	author = {Espeholt, Lasse and Soyer, Hubert and Munos, Remi and Simonyan, Karen and Mnih, Volodymir and Ward, Tom and Doron, Yotam and Firoiu, Vlad and Harley, Tim and Dunning, Iain and Legg, Shane and Kavukcuoglu, Koray},
	month = jun,
	year = {2018},
	note = {arXiv: 1802.01561},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}

@article{drlthatmatters,
	title = {Deep {Reinforcement} {Learning} that {Matters}},
	url = {http://arxiv.org/abs/1709.06560},
	urldate = {2021-05-04},
	journal = {arXiv:1709.06560 [cs, stat]},
	author = {Henderson, Peter and Islam, Riashat and Bachman, Philip and Pineau, Joelle and Precup, Doina and Meger, David},
	month = jan,
	year = {2019},
	note = {arXiv: 1709.06560},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: Accepted to the Thirthy-Second AAAI Conference On Artificial Intelligence (AAAI), 2018},
}

@article{rl_brain,
	title = {Reinforcement learning in the brain},
	volume = {53},
	issn = {00222496},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0022249608001181},
	doi = {10.1016/j.jmp.2008.12.005},
	language = {en},
	number = {3},
	urldate = {2021-05-17},
	journal = {Journal of Mathematical Psychology},
	author = {Niv, Yael},
	month = jun,
	year = {2009},
	pages = {139--154},
}

@book{suttonbarto,
  title={Reinforcement learning: An introduction},
  author={Sutton, Richard S and Barto, Andrew G},
  year={2018},
  publisher={MIT press},
}

@article{Possingham2006,
	title = {Prioritizing global conservation efforts},
	volume = {440},
	issn = {0028-0836, 1476-4687},
	url = {http://www.nature.com/articles/nature04366},
	doi = {10.1038/nature04366},
	language = {en},
	number = {7082},
	urldate = {2021-05-07},
	journal = {Nature},
	author = {Wilson, Kerrie A. and McBride, Marissa F. and Bode, Michael and Possingham, Hugh P.},
	month = mar,
	year = {2006},
	pages = {337--340}
}

@article{Possingham2009,
	title = {Optimal {Allocation} of {Resources} among {Threatened} {Species}: a {Project} {Prioritization} {Protocol}},
	volume = {23},
	issn = {08888892, 15231739},
	shorttitle = {Optimal {Allocation} of {Resources} among {Threatened} {Species}},
	url = {http://doi.wiley.com/10.1111/j.1523-1739.2008.01124.x},
	doi = {10.1111/j.1523-1739.2008.01124.x},
	language = {en},
	number = {2},
	urldate = {2021-05-07},
	journal = {Conservation Biology},
	author = {Joseph, Liana N. and Maloney, Richard F. and Possingham, Hugh P.},
	month = apr,
	year = {2009},
	pages = {328--338}
}

@article{Possingham2004,
	title = {Does conservation planning matter in a dynamic and uncertain world?},
	volume = {7},
	issn = {1461-023X, 1461-0248},
	url = {http://doi.wiley.com/10.1111/j.1461-0248.2004.00624.x},
	doi = {10.1111/j.1461-0248.2004.00624.x},
	language = {en},
	number = {8},
	urldate = {2021-05-07},
	journal = {Ecology Letters},
	author = {Meir, Eli and Andelman, Sandy and Possingham, Hugh P.},
	month = aug,
	year = {2004},
	pages = {615--622}
}

@article{Possignham2014,
	title = {Targeting {Global} {Protected} {Area} {Expansion} for {Imperiled} {Biodiversity}},
	volume = {12},
	issn = {1545-7885},
	url = {https://dx.plos.org/10.1371/journal.pbio.1001891},
	doi = {10.1371/journal.pbio.1001891},
	language = {en},
	number = {6},
	urldate = {2021-05-07},
	journal = {PLoS Biology},
	author = {Venter, Oscar and Fuller, Richard A. and Segan, Daniel B. and Carwardine, Josie and Brooks, Thomas and Butchart, Stuart H. M. and Di Marco, Moreno and Iwamura, Takuya and Joseph, Liana and O'Grady, Damien and Possingham, Hugh P. and Rondinini, Carlo and Smith, Robert J. and Venter, Michelle and Watson, James E. M.},
	editor = {Moritz, Craig},
	month = jun,
	year = {2014},
	pages = {e1001891},
	file = {Full Text:/home/cboettig/Zotero/storage/RHQ7T8S8/Venter et al. - 2014 - Targeting Global Protected Area Expansion for Impe.pdf:application/pdf}
}



@article{netflix_prize,
	title = {Innovation and learning performance implications of free revealing and knowledge brokering in competing communities: insights from the {Netflix} {Prize} challenge},
	volume = {19},
	issn = {1381-298X, 1572-9346},
	shorttitle = {Innovation and learning performance implications of free revealing and knowledge brokering in competing communities},
	url = {http://link.springer.com/10.1007/s10588-012-9137-7},
	doi = {10.1007/s10588-012-9137-7},
	language = {en},
	number = {1},
	urldate = {2021-05-11},
	journal = {Computational and Mathematical Organization Theory},
	author = {Villarroel, J. Andrei and Taylor, John E. and Tucci, Christopher L.},
	month = mar,
	year = {2013},
	pages = {42--77}
}


@article{gpu_computing,
	title = {Supercharge your data wrangling with a graphics card},
	volume = {562},
	issn = {0028-0836, 1476-4687},
	url = {http://www.nature.com/articles/d41586-018-06870-8},
	doi = {10.1038/d41586-018-06870-8},
	language = {en},
	number = {7725},
	urldate = {2021-05-11},
	journal = {Nature},
	author = {Matthews, David},
	month = oct,
	year = {2018},
	pages = {151--152},
	file = {Full Text:/home/cboettig/Zotero/storage/JC4ZJSS2/Matthews - 2018 - Supercharge your data wrangling with a graphics ca.pdf:application/pdf}
}

@article{qbias,
	title = {Maxmin {Q}-learning: {Controlling} the {Estimation} {Bias} of {Q}-learning},
	shorttitle = {Maxmin {Q}-learning},
	url = {http://arxiv.org/abs/2002.06487},
	urldate = {2021-05-17},
	journal = {arXiv:2002.06487 [cs]},
	author = {Lan, Qingfeng and Pan, Yangchen and Fyshe, Alona and White, Martha},
	month = feb,
	year = {2020},
	note = {arXiv: 2002.06487},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}


@article{universalapproximators,
	title = {Multilayer feedforward networks are universal approximators},
	volume = {2},
	issn = {0893-6080},
	url = {https://www.sciencedirect.com/science/article/pii/0893608089900208},
	doi = {10.1016/0893-6080(89)90020-8},
	language = {en},
	number = {5},
	urldate = {2021-05-17},
	journal = {Neural Networks},
	author = {Hornik, Kurt and Stinchcombe, Maxwell and White, Halbert},
	month = jan,
	year = {1989},
	keywords = {Back-propagation networks, Feedforward networks, Mapping networks, Network representation capability, Sigma-Pi networks, Squashing functions, Stone-Weierstrass Theorem, Universal approximation},
	pages = {359--366},
}

@InProceedings{grande14,
  title = 	 {Sample Efficient Reinforcement Learning with Gaussian Processes},
  author = 	 {Robert Grande and Thomas Walsh and Jonathan How},
  booktitle = 	 {Proceedings of the 31st International Conference on Machine Learning},
  pages = 	 {1332--1340},
  year = 	 {2014},
  editor = 	 {Eric P. Xing and Tony Jebara},
  volume = 	 {32},
  number =       {2},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Bejing, China},
  month = 	 {22--24 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v32/grande14.pdf},
  url = 	 {http://proceedings.mlr.press/v32/grande14.html},
}

@article{briefsurveydrl,
	title = {A {Brief} {Survey} of {Deep} {Reinforcement} {Learning}},
	volume = {34},
	issn = {1053-5888},
	url = {http://arxiv.org/abs/1708.05866},
	doi = {10.1109/MSP.2017.2743240},
	number = {6},
	urldate = {2021-05-17},
	journal = {IEEE Signal Processing Magazine},
	author = {Arulkumaran, Kai and Deisenroth, Marc Peter and Brundage, Miles and Bharath, Anil Anthony},
	month = nov,
	year = {2017},
	note = {arXiv: 1708.05866},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
	pages = {26--38},
}

@article{lecun_deep_2015,
	title = {Deep learning},
	volume = {521},
	issn = {0028-0836, 1476-4687},
	url = {http://www.nature.com/articles/nature14539},
	doi = {10.1038/nature14539},
	language = {en},
	number = {7553},
	urldate = {2021-05-17},
	journal = {Nature},
	author = {LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
	month = may,
	year = {2015},
	pages = {436--444},
}

@article{berger-tal_exploration-exploitation_2014,
	title = {The {Exploration}-{Exploitation} {Dilemma}: {A} {Multidisciplinary} {Framework}},
	volume = {9},
	issn = {1932-6203},
	shorttitle = {The {Exploration}-{Exploitation} {Dilemma}},
	url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0095693},
	doi = {10.1371/journal.pone.0095693},
	language = {en},
	number = {4},
	urldate = {2021-05-17},
	journal = {PLOS ONE},
	author = {Berger-Tal, Oded and Nathan, Jonathan and Meron, Ehud and Saltz, David},
	month = apr,
	year = {2014},
	note = {Publisher: Public Library of Science},
	keywords = {Animal behavior, Behavior, Decision making, Foraging, Human learning, Learning, Machine learning, Machine learning algorithms},
	pages = {e95693},
}

@article{gu_q-prop_2017,
	title = {Q-{Prop}: {Sample}-{Efficient} {Policy} {Gradient} with {An} {Off}-{Policy} {Critic}},
	shorttitle = {Q-{Prop}},
	url = {http://arxiv.org/abs/1611.02247},
	urldate = {2021-05-17},
	journal = {arXiv:1611.02247 [cs]},
	author = {Gu, Shixiang and Lillicrap, Timothy and Ghahramani, Zoubin and Turner, Richard E. and Levine, Sergey},
	month = feb,
	year = {2017},
	note = {arXiv: 1611.02247},
	keywords = {Computer Science - Machine Learning},
}

@article{van_hasselt_deep_2015,
	title = {Deep {Reinforcement} {Learning} with {Double} {Q}-learning},
	url = {http://arxiv.org/abs/1509.06461},
	urldate = {2021-05-18},
	journal = {arXiv:1509.06461 [cs]},
	author = {van Hasselt, Hado and Guez, Arthur and Silver, David},
	month = dec,
	year = {2015},
	note = {arXiv: 1509.06461},
	keywords = {Computer Science - Machine Learning},
}


@article{wang_sample_2017,
	title = {Sample {Efficient} {Actor}-{Critic} with {Experience} {Replay}},
	url = {http://arxiv.org/abs/1611.01224},
	urldate = {2021-05-18},
	journal = {arXiv:1611.01224 [cs]},
	author = {Wang, Ziyu and Bapst, Victor and Heess, Nicolas and Mnih, Volodymyr and Munos, Remi and Kavukcuoglu, Koray and de Freitas, Nando},
	month = jul,
	year = {2017},
	note = {arXiv: 1611.01224},
	keywords = {Computer Science - Machine Learning},
}


@article{scoville2021,
  title={Algorithmic conservation in a changing climate},
  author={Scoville, Caleb and Chapman, Melissa and Amironesei, Razvan and Boettiger, Carl},
  journal={Current Opinion in Environmental Sustainability},
  volume={51},
  pages={30--35},
  year={2021},
  publisher={Elsevier}
}

@article{vinuesa2020,
  title={The role of artificial intelligence in achieving the Sustainable Development Goals},
  author={Vinuesa, Ricardo and Azizpour, Hossein and Leite, Iolanda and Balaam, Madeline and Dignum, Virginia and Domisch, Sami and Fell{\"a}nder, Anna and Langhans, Simone Daniela and Tegmark, Max and Nerini, Francesco Fuso},
  journal={Nature communications},
  volume={11},
  number={1},
  pages={1--10},
  year={2020},
  publisher={Nature Publishing Group}
}

@article{mcdonald2021,
  title={Satellites can reveal global extent of forced labor in the world’s fishing fleet},
  author={McDonald, Gavin G and Costello, Christopher and Bone, Jennifer and Cabral, Reniel B and Farabee, Valerie and Hochberg, Timothy and Kroodsma, David and Mangin, Tracey and Meng, Kyle C and Zahn, Oliver},
  journal={Proceedings of the National Academy of Sciences},
  volume={118},
  number={3},
  year={2021},
  publisher={National Acad Sciences}
}

@article{adams2019,
  title={Geographies of conservation II: Technology, surveillance and conservation by algorithm},
  author={Adams, William M},
  journal={Progress in Human Geography},
  volume={43},
  number={2},
  pages={337--350},
  year={2019},
  publisher={SAGE Publications Sage UK: London, England}
}

@article{wearn2019,
  title={Responsible AI for conservation},
  author={Wearn, Oliver R and Freeman, Robin and Jacoby, David MP},
  journal={Nature Machine Intelligence},
  volume={1},
  number={2},
  pages={72--73},
  year={2019},
  publisher={Nature Publishing Group}
}

@article{kalluri2020,
  title={Don't ask if artificial intelligence is good or fair, ask how it shifts power.},
  author={Kalluri, Pratyusha},
  journal={Nature},
  volume={583},
  number={7815},
  pages={169--169},
  year={2020}
}

@article{vinuesa2020,
  title={The role of artificial intelligence in achieving the Sustainable Development Goals},
  author={Vinuesa, Ricardo and Azizpour, Hossein and Leite, Iolanda and Balaam, Madeline and Dignum, Virginia and Domisch, Sami and Fell{\"a}nder, Anna and Langhans, Simone Daniela and Tegmark, Max and Nerini, Francesco Fuso},
  journal={Nature communications},
  volume={11},
  number={1},
  pages={1--10},
  year={2020},
  publisher={Nature Publishing Group}
}

@article{swartz2021,
  title={AIS-based profiling of fishing vessels falls short as a “proof of concept” for identifying forced labor at sea},
  author={Swartz, Wilf and Cisneros-Montemayor, Andr{\'e}s M and Singh, Gerald G and Boutet, Patrick and Ota, Yoshitaka},
  journal={Proceedings of the National Academy of Sciences},
  volume={118},
  number={19},
  year={2021},
  publisher={National Acad Sciences}
}


@article{dice,
	title = {An {Optimal} {Transition} {Path} for {Controlling} {Greenhouse} {Gases}},
	volume = {258},
	issn = {0036-8075, 1095-9203},
	url = {https://www.sciencemag.org/lookup/doi/10.1126/science.258.5086.1315},
	doi = {10.1126/science.258.5086.1315},
	language = {en},
	number = {5086},
	urldate = {2021-05-24},
	journal = {Science},
	author = {Nordhaus, W. D.},
	month = nov,
	year = {1992},
	pages = {1315--1319}
}



@article{Reed1979,
author = {Reed, William J},
doi = {10.1016/0095-0696(79)90014-7},
issn = {00950696},
journal = {Journal of Environmental Economics and Management},
month = {dec},
number = {4},
pages = {350--363},
publisher = {Elsevier},
title = {{Optimal escapement levels in stochastic and deterministic harvesting models}},
volume = {6},
year = {1979}
}


@article{Schaefer1954,
author = {Schaefer, Milner B.},
doi = {10.1007/BF02464432},
issn = {0092-8240},
journal = {Bulletin of the Inter-American Tropical Tuna Commission},
month = {mar},
number = {2},
pages = {27--56},
title = {{Some aspects of the dynamics of populations important to the management of the commercial marine fisheries}},
volume = {1},
year = {1954}
}


@article{Ludwig1982,
	title = {Optimal harvesting with imprecise parameter estimates},
	volume = {14},
	issn = {03043800},
	doi = {10.1016/0304-3800(82)90023-0},
	number = {3-4},
	journal = {Ecological Modelling},
	author = {Ludwig, Donald and Walters, Carl J},
	month = jan,
	year = {1982},
	keywords = {parameter uncertainty},
	pages = {273--292}
}

@article{May1977,
	title = {Thresholds and breakpoints in ecosystems with a multiplicity of stable states},
	volume = {269},
	issn = {0028-0836},
	url = {http://www.nature.com/doifinder/10.1038/269471a0},
	doi = {10.1038/269471a0},
	number = {5628},
	journal = {Nature},
	author = {May, Robert M},
	month = oct,
	year = {1977},
	pages = {471--477}
}

@article{Polasky2011,
author = {Polasky, Stephen and Carpenter, Stephen R. and Folke, Carl and Keeler, Bonnie},
doi = {10.1016/j.tree.2011.04.007},
issn = {01695347},
journal = {Trends in Ecology {\&} Evolution},
month = {aug},
number = {8},
pages = {398--404},
pmid = {21616553},
publisher = {Elsevier Ltd},
title = {{Decision-making under great uncertainty: environmental management in an era of global change}},
volume = {26},
year = {2011}
}
@article{Walters1981,
	title = {Optimum {Escapements} in the {Face} of {Alternative} {Recruitment} {Hypotheses}},
	volume = {38},
	issn = {0706-652X, 1205-7533},
	url = {http://www.nrcresearchpress.com/doi/10.1139/f81-091},
	doi = {10.1139/f81-091},
	language = {en},
	number = {6},
	urldate = {2021-02-04},
	journal = {Canadian Journal of Fisheries and Aquatic Sciences},
	author = {Walters, Carl J.},
	month = jun,
	year = {1981},
	pages = {678--689}
}


@book{Clark1990,
author = {Clark, Colin W},
isbn = {0471508837},
pages = {400},
publisher = {Wiley-Interscience},
title = {{Mathematical Bioeconomics: The Optimal Management of Renewable Resources, 2nd Edition}},
year = {1990}
}

@article{Clark1973,
author = { Colin W. Clark },
title = {Profit Maximization and the Extinction of Animal Species},
journal = {Journal of Political Economy},
volume = {81},
number = {4},
pages = {950-961},
year = {1973},
doi = {10.1086/260090},
}


@article{Marescot2013,
	title = {Complex decisions made simple: a primer on stochastic dynamic programming},
	volume = {4},
	url = {http://doi.wiley.com/10.1111/2041-210X.12082},
	doi = {10.1111/2041-210X.12082},
	number = {9},
	journal = {Methods in Ecology and Evolution},
	author = {Marescot, Lucile and Chapron, Guillaume and Chadès, Iadine and Fackler, Paul L. and Duchamp, Christophe and Marboutin, Eric and Gimenez, Olivier},
	year = {2013},
	pages = {872--884}
}


@article{Dai2012,
	title = {Generic {Indicators} for {Loss} of {Resilience} {Before} a {Tipping} {Point} {Leading} to {Population} {Collapse}},
	volume = {336},
	issn = {0036-8075},
	url = {http://www.sciencemag.org/cgi/doi/10.1126/science.1219805},
	doi = {10.1126/science.1219805},
	number = {6085},
	journal = {Science (New York, N.Y.)},
	author = {Dai, Lei and Vorselen, Daan and Korolev, Kirill S and Gore, J.},
	month = may,
	year = {2012},
	pages = {1175--1177}
}

@article{Carpenter2011,
	title = {Early {Warnings} of {Regime} {Shifts}: {A} {Whole}-{Ecosystem} {Experiment}},
	volume = {1079},
	issn = {0036-8075},
	url = {dx.doi.org/10.1126/science.1203672},
	doi = {10.1126/science.1203672},
	journal = {Science (New York, N.Y.)},
	author = {Carpenter, Stephen R and Cole, J. J. and Pace, Michael L and Batt, Ryan D. and Brock, William A and Cline, Timothy J. and Coloso, J. and Hodgson, J. R. and Kitchell, J F and Seekell, David A and Smith, L. and Weidel, B.},
	month = apr,
	year = {2011}
}

@article{Barnosky2012,
	title = {Approaching a state shift in {Earth}’s biosphere},
	volume = {486},
	issn = {0028-0836, 1476-4687},
	url = {http://www.nature.com/articles/nature11018},
	doi = {10.1038/nature11018},
	language = {en},
	number = {7401},
	urldate = {2021-05-24},
	journal = {Nature},
	author = {Barnosky, Anthony D. and Hadly, Elizabeth A. and Bascompte, Jordi and Berlow, Eric L. and Brown, James H. and Fortelius, Mikael and Getz, Wayne M. and Harte, John and Hastings, Alan and Marquet, Pablo A. and Martinez, Neo D. and Mooers, Arne and Roopnarine, Peter and Vermeij, Geerat and Williams, John W. and Gillespie, Rosemary and Kitzes, Justin and Marshall, Charles and Matzke, Nicholas and Mindell, David P. and Revilla, Eloy and Smith, Adam B.},
	month = jun,
	year = {2012},
	pages = {52--58}
}


@article{Worm2006,
author = {Worm, Boris and Barbier, Edward B and Beaumont, Nicola and Duffy, J Emmett and Folke, Carl and Halpern, Benjamin S and Jackson, Jeremy B C and Lotze, Heike K and Micheli, Fiorenza and Palumbi, Stephen R and Sala, Enric and Selkoe, Kimberley a and Stachowicz, John J and Watson, Reg},
doi = {10.1126/science.1132294},
journal = {Science (New York, N.Y.)},
month = {nov},
number = {5800},
pages = {787--90},
pmid = {17082450},
title = {{Impacts of biodiversity loss on ocean ecosystem services.}},
url = {doi.org/10.1126/science.1132294},
volume = {314},
year = {2006}
}
@article{Worm2009,
author = {Worm, Boris and Hilborn, Ray and Baum, Julia K and Branch, Trevor A and Collie, Jeremy S and Costello, Christopher and Fogarty, Michael J and Fulton, Elizabeth a and Hutchings, Jeffrey a and Jennings, Simon and Jensen, Olaf P and Lotze, Heike K and Mace, Pamela M and McClanahan, Tim R and Minto, C{\'{o}}il{\'{i}}n and Palumbi, Stephen R and Parma, Ana M and Ricard, Daniel and Rosenberg, Andrew a and Watson, Reg and Zeller, Dirk},
doi = {10.1126/science.1173146},
journal = {Science (New York, N.Y.)},
month = {jul},
number = {5940},
pages = {578--85},
pmid = {19644114},
title = {{Rebuilding global fisheries.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/19644114},
volume = {325},
year = {2009}
}

@article{pomdp-intro,
author = {Memarzadeh, Milad and Boettiger, Carl},
title = {Resolving the Measurement Uncertainty Paradox in Ecological Management},
journal = {The American Naturalist},
volume = {193},
number = {5},
pages = {645-660},
year = {2019},
doi = {10.1086/702704}
}

@article {Memarzadeh2019,
	author = {Memarzadeh, Milad and Britten, Gregory L. and Worm, Boris and Boettiger, Carl},
	title = {Rebuilding global fisheries under uncertainty},
	volume = {116},
	number = {32},
	pages = {15985--15990},
	year = {2019},
	doi = {10.1073/pnas.1902657116},
	publisher = {National Academy of Sciences},
	issn = {0027-8424},
	URL = {https://www.pnas.org/content/116/32/15985},
	eprint = {https://www.pnas.org/content/116/32/15985.full.pdf},
	journal = {Proceedings of the National Academy of Sciences}
}

@article{Costello2016,
author = {Costello, Christopher and Ovando, Daniel and Clavelle, Tyler and Strauss, C. Kent and Hilborn, Ray and Melnychuk, Michael C. and Branch, Trevor A and Gaines, Steven D and Szuwalski, Cody S. and Cabral, Reniel B. and Rader, Douglas N. and Leland, Amanda},
doi = {10.1073/pnas.1520420113},
issn = {0027-8424},
journal = {Proceedings of the National Academy of Sciences},
month = {may},
number = {18},
pages = {5125--5129},
title = {{Global fishery prospects under contrasting management regimes}},
volume = {113},
year = {2016}
}

@article{Walters1978,
	title = {Ecological {Optimization} and {Adaptive} {Management}},
	volume = {9},
	issn = {0066-4162},
	doi = {10.1146/annurev.es.09.110178.001105},
	number = {1},
	journal = {Annual Review of Ecology and Systematics},
	author = {Walters, Carl J and Hilborn, Ray},
	month = nov,
	year = {1978},
	keywords = {parameter uncertainty},
	pages = {157--188}
}

@article{Fischer2009,
	title = {Integrating resilience thinking and optimisation for conservation.},
	volume = {24},
	issn = {0169-5347},
	url = {http://www.ncbi.nlm.nih.gov/pubmed/19665820},
	doi = {10.1016/j.tree.2009.03.020},
	number = {10},
	journal = {Trends in ecology \& evolution},
	author = {Fischer, Joern and Peterson, Garry D and Gardner, Toby A. and Gordon, Line J and Fazey, Ioan and Elmqvist, Thomas and Felton, Adam and Folke, Carl and Dovers, Stephen},
	month = oct,
	year = {2009},
	pmid = {19665820},
	pages = {549--54}
}


@book{Hastings2012,
	address = {Oakland, CA},
	title = {Encyclopedia of {Theoretical} {Ecology}},
	isbn = {978-0-520-26965-1},
	publisher = {University of California Press},
	editor = {Hastings, Alan and Gross, Louis J.},
	year = {2012}
}



@article{sortie,
	title = {Forest {Models} {Defined} by {Field} {Measurements}: {Estimation}, {Error} {Analysis} and {Dynamics}},
	volume = {66},
	issn = {00129615},
	shorttitle = {Forest {Models} {Defined} by {Field} {Measurements}},
	url = {http://doi.wiley.com/10.2307/2963479},
	doi = {10.2307/2963479},
	language = {en},
	number = {1},
	urldate = {2021-05-24},
	journal = {Ecological Monographs},
	author = {Pacala, Stephen W. and Canham, Charles D. and Saponara, John and Silander, John A. and Kobe, Richard K. and Ribbens, Eric},
	month = feb,
	year = {1996},
	pages = {1--43}
}

@article{ecopath,
	title = {Ecopath with {Ecosim} as a model-building toolbox: {Source} code capabilities, extensions, and variations},
	volume = {319},
	issn = {03043800},
	shorttitle = {Ecopath with {Ecosim} as a model-building toolbox},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S030438001500280X},
	doi = {10.1016/j.ecolmodel.2015.06.031},
	language = {en},
	urldate = {2021-05-24},
	journal = {Ecological Modelling},
	author = {Steenbeek, Jeroen and Buszowski, Joe and Christensen, Villy and Akoglu, Ekin and Aydin, Kerim and Ellis, Nick and Felinto, Dalai and Guitton, Jerome and Lucey, Sean and Kearney, Kelly and Mackinson, Steven and Pan, Mike and Platts, Mark and Walters, Carl},
	month = jan,
	year = {2016},
	pages = {178--189}
}

@inproceedings{imagenet,
	address = {Miami, FL},
	title = {{ImageNet}: {A} large-scale hierarchical image database},
	isbn = {978-1-4244-3992-8},
	shorttitle = {{ImageNet}},
	url = {https://ieeexplore.ieee.org/document/5206848/},
	doi = {10.1109/CVPR.2009.5206848},
	urldate = {2021-05-24},
	booktitle = {2009 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {IEEE},
	author = {Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and {Kai Li} and {Li Fei-Fei}},
	month = jun,
	year = {2009},
	pages = {248--255}
}

@article{salimans_evolution_2017,
	title = {Evolution {Strategies} as a {Scalable} {Alternative} to {Reinforcement} {Learning}},
	url = {http://arxiv.org/abs/1703.03864},
	urldate = {2021-05-19},
	journal = {arXiv:1703.03864 [cs, stat]},
	author = {Salimans, Tim and Ho, Jonathan and Chen, Xi and Sidor, Szymon and Sutskever, Ilya},
	month = sep,
	year = {2017},
	note = {arXiv: 1703.03864},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
}

@article{plappert_parameter_2018,
	title = {Parameter {Space} {Noise} for {Exploration}},
	url = {http://arxiv.org/abs/1706.01905},
	urldate = {2021-06-01},
	journal = {arXiv:1706.01905 [cs, stat]},
	author = {Plappert, Matthias and Houthooft, Rein and Dhariwal, Prafulla and Sidor, Szymon and Chen, Richard Y. and Chen, Xi and Asfour, Tamim and Abbeel, Pieter and Andrychowicz, Marcin},
	month = jan,
	year = {2018},
	note = {arXiv: 1706.01905},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Computer Science - Robotics, Statistics - Machine Learning},
}

@article{go-explore,
	title = {Go-{Explore}: a {New} {Approach} for {Hard}-{Exploration} {Problems}},
	shorttitle = {Go-{Explore}},
	url = {http://arxiv.org/abs/1901.10995},
	urldate = {2021-06-01},
	journal = {arXiv:1901.10995 [cs, stat]},
	author = {Ecoffet, Adrien and Huizinga, Joost and Lehman, Joel and Stanley, Kenneth O. and Clune, Jeff},
	month = feb,
	year = {2021},
	note = {arXiv: 1901.10995},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{ruder_overview_2017,
	title = {An overview of gradient descent optimization algorithms},
	url = {http://arxiv.org/abs/1609.04747},
	urldate = {2021-05-19},
	journal = {arXiv:1609.04747 [cs]},
	author = {Ruder, Sebastian},
	month = jun,
	year = {2017},
	note = {arXiv: 1609.04747},
	keywords = {Computer Science - Machine Learning},
}

@article{Qproof,
	title = {Convergence of {Q}-learning: a simple proof},
	language = {en},
	author = {Melo, Francisco S},
	pages = {4},
}

@article{schulman_high-dimensional_2018,
	title = {High-{Dimensional} {Continuous} {Control} {Using} {Generalized} {Advantage} {Estimation}},
	url = {http://arxiv.org/abs/1506.02438},
	urldate = {2021-06-01},
	journal = {arXiv:1506.02438 [cs]},
	author = {Schulman, John and Moritz, Philipp and Levine, Sergey and Jordan, Michael and Abbeel, Pieter},
	month = oct,
	year = {2018},
	note = {arXiv: 1506.02438},
	keywords = {Computer Science - Machine Learning, Computer Science - Robotics, Electrical Engineering and Systems Science - Systems and Control},
}

@article{gsde,
	title = {Generalized {State}-{Dependent} {Exploration} for {Deep} {Reinforcement} {Learning} in {Robotics}},
	url = {http://arxiv.org/abs/2005.05719},
	urldate = {2021-06-01},
	journal = {arXiv:2005.05719 [cs, stat]},
	author = {Raffin, Antonin and Stulp, Freek},
	month = may,
	year = {2020},
	note = {arXiv: 2005.05719},
	keywords = {Computer Science - Machine Learning, Computer Science - Robotics, Statistics - Machine Learning},
}

@article{ahmed_understanding_2019,
	title = {Understanding the impact of entropy on policy optimization},
	url = {http://arxiv.org/abs/1811.11214},
	urldate = {2021-06-01},
	journal = {arXiv:1811.11214 [cs, stat]},
	author = {Ahmed, Zafarali and Roux, Nicolas Le and Norouzi, Mohammad and Schuurmans, Dale},
	month = jun,
	year = {2019},
	note = {arXiv: 1811.11214},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}


@article{schaul_prioritized_2016,
	title = {Prioritized {Experience} {Replay}},
	url = {http://arxiv.org/abs/1511.05952},
	urldate = {2021-06-01},
	journal = {arXiv:1511.05952 [cs]},
	author = {Schaul, Tom and Quan, John and Antonoglou, Ioannis and Silver, David},
	month = feb,
	year = {2016},
	note = {arXiv: 1511.05952},
	keywords = {Computer Science - Machine Learning},
}

@article{teh_distral_2017,
	title = {Distral: {Robust} {Multitask} {Reinforcement} {Learning}},
	shorttitle = {Distral},
	url = {http://arxiv.org/abs/1707.04175},
	urldate = {2021-05-27},
	journal = {arXiv:1707.04175 [cs, stat]},
	author = {Teh, Yee Whye and Bapst, Victor and Czarnecki, Wojciech Marian and Quan, John and Kirkpatrick, James and Hadsell, Raia and Heess, Nicolas and Pascanu, Razvan},
	month = jul,
	year = {2017},
	note = {arXiv: 1707.04175},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@InProceedings{pmlr-v97-cobbe19a,
  title = 	 {Quantifying Generalization in Reinforcement Learning},
  author =       {Cobbe, Karl and Klimov, Oleg and Hesse, Chris and Kim, Taehoon and Schulman, John},
  booktitle = 	 {Proceedings of the 36th International Conference on Machine Learning},
  pages = 	 {1282--1289},
  year = 	 {2019},
  editor = 	 {Kamalika Chaudhuri and Ruslan Salakhutdinov},
  volume = 	 {97},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {09--15 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v97/cobbe19a/cobbe19a.pdf},
  url = 	 {
http://proceedings.mlr.press/v97/cobbe19a.html
},
}

@article{cogswell_reducing_2016,
	title = {Reducing {Overfitting} in {Deep} {Networks} by {Decorrelating} {Representations}},
	url = {http://arxiv.org/abs/1511.06068},
	urldate = {2021-05-27},
	journal = {arXiv:1511.06068 [cs, stat]},
	author = {Cogswell, Michael and Ahmed, Faruk and Girshick, Ross and Zitnick, Larry and Batra, Dhruv},
	month = jun,
	year = {2016},
	note = {arXiv: 1511.06068},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{heuillet_explainability_2021,
	title = {Explainability in deep reinforcement learning},
	volume = {214},
	issn = {0950-7051},
	url = {https://www.sciencedirect.com/science/article/pii/S0950705120308145},
	doi = {10.1016/j.knosys.2020.106685},
	language = {en},
	urldate = {2021-05-27},
	journal = {Knowledge-Based Systems},
	author = {Heuillet, Alexandre and Couthouis, Fabien and Díaz-Rodríguez, Natalia},
	month = feb,
	year = {2021},
	keywords = {Deep Learning, Explainable artificial intelligence, Machine Learning, Reinforcement Learning, Representation learning, Responsible artificial intelligence},
	pages = {106685},
}


@article{castelvecchi_can_2016,
	title = {Can we open the black box of {AI}?},
	volume = {538},
	url = {http://www.nature.com/news/can-we-open-the-black-box-of-ai-1.20731},
	doi = {10.1038/538020a},
	abstract = {Artificial intelligence is everywhere. But before scientists trust it, they first need to understand how machines learn.},
	language = {en},
	number = {7623},
	urldate = {2021-05-27},
	journal = {Nature News},
	author = {Castelvecchi, Davide},
	month = oct,
	year = {2016},
	note = {Section: News Feature},
	pages = {20},
}


@article{hadfield-menell_inverse_2020,
	title = {Inverse {Reward} {Design}},
	url = {http://arxiv.org/abs/1711.02827},
	urldate = {2021-05-27},
	journal = {arXiv:1711.02827 [cs]},
	author = {Hadfield-Menell, Dylan and Milli, Smitha and Abbeel, Pieter and Russell, Stuart and Dragan, Anca},
	month = oct,
	year = {2020},
	note = {arXiv: 1711.02827},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}

@inproceedings{reward_shaping,
  author = {Grzeundefined, Marek},
  title = {Reward Shaping in Episodic Reinforcement Learning},
  year = {2017},
  publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
  address = {Richland, SC},
  pages = {565–573},
  numpages = {9},
  keywords = {reinforcement learning, reward structures for learning, reward shaping, multiagent learning, potential-based reward shaping},
  location = {S\~{a}o Paulo, Brazil},
  series = {AAMAS '17}
}

@article{bellemare2016,
	title = {Unifying {Count}-{Based} {Exploration} and {Intrinsic} {Motivation}},
	url = {http://arxiv.org/abs/1606.01868},
	urldate = {2021-06-05},
	journal = {arXiv:1606.01868 [cs, stat]},
	author = {Bellemare, Marc G. and Srinivasan, Sriram and Ostrovski, Georg and Schaul, Tom and Saxton, David and Munos, Remi},
	month = nov,
	year = {2016},
	note = {arXiv: 1606.01868},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{largescalecuriosity,
	title = {Large-{Scale} {Study} of {Curiosity}-{Driven} {Learning}},
	url = {http://arxiv.org/abs/1808.04355},
	urldate = {2021-06-05},
	journal = {arXiv:1808.04355 [cs, stat]},
	author = {Burda, Yuri and Edwards, Harri and Pathak, Deepak and Storkey, Amos and Darrell, Trevor and Efros, Alexei A.},
	month = aug,
	year = {2018},
	note = {arXiv: 1808.04355},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Robotics, Statistics - Machine Learning},
}

@article{nevergiveup,
	title = {Never {Give} {Up}: {Learning} {Directed} {Exploration} {Strategies}},
	shorttitle = {Never {Give} {Up}},
	url = {http://arxiv.org/abs/2002.06038},
	urldate = {2021-06-05},
	journal = {arXiv:2002.06038 [cs, stat]},
	author = {Badia, Adrià Puigdomènech and Sprechmann, Pablo and Vitvitskyi, Alex and Guo, Daniel and Piot, Bilal and Kapturowski, Steven and Tieleman, Olivier and Arjovsky, Martín and Pritzel, Alexander and Bolt, Andew and Blundell, Charles},
	month = feb,
	year = {2020},
	note = {arXiv: 2002.06038},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{instability_2019,
	title = {Deterministic {Implementations} for {Reproducibility} in {Deep} {Reinforcement} {Learning}},
	url = {http://arxiv.org/abs/1809.05676},
	abstract = {While deep reinforcement learning (DRL) has led to numerous successes in recent years, reproducing these successes can be extremely challenging. One reproducibility challenge particularly relevant to DRL is nondeterminism in the training process, which can substantially affect the results. Motivated by this challenge, we study the positive impacts of deterministic implementations in eliminating nondeterminism in training. To do so, we consider the particular case of the deep Q-learning algorithm, for which we produce a deterministic implementation by identifying and controlling all sources of nondeterminism in the training process. One by one, we then allow individual sources of nondeterminism to affect our otherwise deterministic implementation, and measure the impact of each source on the variance in performance. We find that individual sources of nondeterminism can substantially impact the performance of agent, illustrating the benefits of deterministic implementations. In addition, we also discuss the important role of deterministic implementations in achieving exact replicability of results.},
	urldate = {2021-06-07},
	journal = {arXiv:1809.05676 [cs]},
	author = {Nagarajan, Prabhat and Warnell, Garrett and Stone, Peter},
	month = jun,
	year = {2019},
	note = {arXiv: 1809.05676},
	keywords = {Computer Science - Artificial Intelligence},
	annote = {Comment: 17 Pages}
}

@article{alphastar,
	title = {Grandmaster level in {StarCraft} {II} using multi-agent reinforcement learning},
	volume = {575},
	copyright = {2019 The Author(s), under exclusive licence to Springer Nature Limited},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/s41586-019-1724-z},
	doi = {10.1038/s41586-019-1724-z},
	abstract = {Many real-world applications require artificial agents to compete and coordinate with other agents in complex environments. As a stepping stone to this goal, the domain of StarCraft has emerged as an important challenge for artificial intelligence research, owing to its iconic and enduring status among the most difficult professional esports and its relevance to the real world in terms of its raw complexity and multi-agent challenges. Over the course of a decade and numerous competitions1–3, the strongest agents have simplified important aspects of the game, utilized superhuman capabilities, or employed hand-crafted sub-systems4. Despite these advantages, no previous agent has come close to matching the overall skill of top StarCraft players. We chose to address the challenge of StarCraft using general-purpose learning methods that are in principle applicable to other complex domains: a multi-agent reinforcement learning algorithm that uses data from both human and agent games within a diverse league of continually adapting strategies and counter-strategies, each represented by deep neural networks5,6. We evaluated our agent, AlphaStar, in the full game of StarCraft II, through a series of online games against human players. AlphaStar was rated at Grandmaster level for all three StarCraft races and above 99.8\% of officially ranked human players.},
	language = {en},
	number = {7782},
	urldate = {2021-06-08},
	journal = {Nature},
	author = {Vinyals, Oriol and Babuschkin, Igor and Czarnecki, Wojciech M. and Mathieu, Michaël and Dudzik, Andrew and Chung, Junyoung and Choi, David H. and Powell, Richard and Ewalds, Timo and Georgiev, Petko and Oh, Junhyuk and Horgan, Dan and Kroiss, Manuel and Danihelka, Ivo and Huang, Aja and Sifre, Laurent and Cai, Trevor and Agapiou, John P. and Jaderberg, Max and Vezhnevets, Alexander S. and Leblond, Rémi and Pohlen, Tobias and Dalibard, Valentin and Budden, David and Sulsky, Yury and Molloy, James and Paine, Tom L. and Gulcehre, Caglar and Wang, Ziyu and Pfaff, Tobias and Wu, Yuhuai and Ring, Roman and Yogatama, Dani and Wünsch, Dario and McKinney, Katrina and Smith, Oliver and Schaul, Tom and Lillicrap, Timothy and Kavukcuoglu, Koray and Hassabis, Demis and Apps, Chris and Silver, David},
	month = nov,
	year = {2019},
	note = {Number: 7782
Publisher: Nature Publishing Group},
	pages = {350--354},
}

@article{openai_learning_2019,
	title = {Learning {Dexterous} {In}-{Hand} {Manipulation}},
	url = {http://arxiv.org/abs/1808.00177},
	abstract = {We use reinforcement learning (RL) to learn dexterous in-hand manipulation policies which can perform vision-based object reorientation on a physical Shadow Dexterous Hand. The training is performed in a simulated environment in which we randomize many of the physical properties of the system like friction coefficients and an object's appearance. Our policies transfer to the physical robot despite being trained entirely in simulation. Our method does not rely on any human demonstrations, but many behaviors found in human manipulation emerge naturally, including finger gaiting, multi-finger coordination, and the controlled use of gravity. Our results were obtained using the same distributed RL system that was used to train OpenAI Five. We also include a video of our results: https://youtu.be/jwSbzNHGflM},
	urldate = {2021-06-08},
	journal = {arXiv:1808.00177 [cs, stat]},
	author = {OpenAI and Andrychowicz, Marcin and Baker, Bowen and Chociej, Maciek and Jozefowicz, Rafal and McGrew, Bob and Pachocki, Jakub and Petron, Arthur and Plappert, Matthias and Powell, Glenn and Ray, Alex and Schneider, Jonas and Sidor, Szymon and Tobin, Josh and Welinder, Peter and Weng, Lilian and Zaremba, Wojciech},
	month = jan,
	year = {2019},
	note = {arXiv: 1808.00177},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Robotics, Statistics - Machine Learning},
}

@article{ha_learning_2020,
	title = {Learning to {Walk} in the {Real} {World} with {Minimal} {Human} {Effort}},
	url = {http://arxiv.org/abs/2002.08550},
	abstract = {Reliable and stable locomotion has been one of the most fundamental challenges for legged robots. Deep reinforcement learning (deep RL) has emerged as a promising method for developing such control policies autonomously. In this paper, we develop a system for learning legged locomotion policies with deep RL in the real world with minimal human effort. The key difficulties for on-robot learning systems are automatic data collection and safety. We overcome these two challenges by developing a multi-task learning procedure and a safety-constrained RL framework. We tested our system on the task of learning to walk on three different terrains: flat ground, a soft mattress, and a doormat with crevices. Our system can automatically and efficiently learn locomotion skills on a Minitaur robot with little human intervention. The supplemental video can be found at: {\textbackslash}url\{https://youtu.be/cwyiq6dCgOc\}.},
	urldate = {2021-06-08},
	journal = {arXiv:2002.08550 [cs]},
	author = {Ha, Sehoon and Xu, Peng and Tan, Zhenyu and Levine, Sergey and Tan, Jie},
	month = nov,
	year = {2020},
	note = {arXiv: 2002.08550},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Robotics},
}

@article{popova_deep_2018,
	title = {Deep reinforcement learning for de novo drug design},
	volume = {4},
	copyright = {Copyright © 2018 The Authors, some rights reserved; exclusive licensee American Association for the Advancement of Science. No claim to original U.S. Government Works. Distributed under a Creative Commons Attribution NonCommercial License 4.0 (CC BY-NC).. This is an open-access article distributed under the terms of the Creative Commons Attribution-NonCommercial license, which permits use, distribution, and reproduction in any medium, so long as the resultant use is not for commercial advantage and provided the original work is properly cited.},
	issn = {2375-2548},
	url = {https://advances.sciencemag.org/content/4/7/eaap7885},
	doi = {10.1126/sciadv.aap7885},
	abstract = {We have devised and implemented a novel computational strategy for de novo design of molecules with desired properties termed ReLeaSE (Reinforcement Learning for Structural Evolution). On the basis of deep and reinforcement learning (RL) approaches, ReLeaSE integrates two deep neural networks—generative and predictive—that are trained separately but are used jointly to generate novel targeted chemical libraries. ReLeaSE uses simple representation of molecules by their simplified molecular-input line-entry system (SMILES) strings only. Generative models are trained with a stack-augmented memory network to produce chemically feasible SMILES strings, and predictive models are derived to forecast the desired properties of the de novo–generated compounds. In the first phase of the method, generative and predictive models are trained separately with a supervised learning algorithm. In the second phase, both models are trained jointly with the RL approach to bias the generation of new chemical structures toward those with the desired physical and/or biological properties. In the proof-of-concept study, we have used the ReLeaSE method to design chemical libraries with a bias toward structural complexity or toward compounds with maximal, minimal, or specific range of physical properties, such as melting point or hydrophobicity, or toward compounds with inhibitory activity against Janus protein kinase 2. The approach proposed herein can find a general use for generating targeted chemical libraries of novel compounds optimized for either a single desired property or multiple properties.
We introduce an artificial intelligence approach to de novo design of molecules with desired physical or biological properties.
We introduce an artificial intelligence approach to de novo design of molecules with desired physical or biological properties.},
	language = {en},
	number = {7},
	urldate = {2021-06-09},
	journal = {Science Advances},
	author = {Popova, Mariya and Isayev, Olexandr and Tropsha, Alexander},
	month = jul,
	year = {2018},
	note = {Publisher: American Association for the Advancement of Science
Section: Research Article},
	pages = {eaap7885},
}

@article{zhou_optimizing_2017,
	title = {Optimizing {Chemical} {Reactions} with {Deep} {Reinforcement} {Learning}},
	volume = {3},
	issn = {2374-7943, 2374-7951},
	url = {https://pubs.acs.org/doi/10.1021/acscentsci.7b00492},
	doi = {10.1021/acscentsci.7b00492},
	abstract = {Deep reinforcement learning was employed to optimize chemical reactions. Our model iteratively records the results of a chemical reaction and chooses new experimental conditions to improve the reaction outcome. This model outperformed a state-of-the-art blackbox optimization algorithm by using 71\% fewer steps on both simulations and real reactions. Furthermore, we introduced an eﬃcient exploration strategy by drawing the reaction conditions from certain probability distributions, which resulted in an improvement on regret from 0.062 to 0.039 compared with a deterministic policy. Combining the eﬃcient exploration policy with accelerated microdroplet reactions, optimal reaction conditions were determined in 30 min for the four reactions considered, and a better understanding of the factors that control microdroplet reactions was reached. Moreover, our model showed a better performance after training on reactions with similar or even dissimilar underlying mechanisms, which demonstrates its learning ability.},
	language = {en},
	number = {12},
	urldate = {2021-06-09},
	journal = {ACS Central Science},
	author = {Zhou, Zhenpeng and Li, Xiaocheng and Zare, Richard N.},
	month = dec,
	year = {2017},
	pages = {1337--1344},
}

@article{joseph_neural_2020,
	title = {Neural hierarchical models of ecological populations},
	volume = {23},
	copyright = {© 2020 John Wiley \& Sons Ltd/CNRS},
	issn = {1461-0248},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/ele.13462},
	doi = {10.1111/ele.13462},
	abstract = {Neural networks are increasingly being used in science to infer hidden dynamics of natural systems from noisy observations, a task typically handled by hierarchical models in ecology. This article describes a class of hierarchical models parameterised by neural networks – neural hierarchical models. The derivation of such models analogises the relationship between regression and neural networks. A case study is developed for a neural dynamic occupancy model of North American bird populations, trained on millions of detection/non-detection time series for hundreds of species, providing insights into colonisation and extinction at a continental scale. Flexible models are increasingly needed that scale to large data and represent ecological processes. Neural hierarchical models satisfy this need, providing a bridge between deep learning and ecological modelling that combines the function representation power of neural networks with the inferential capacity of hierarchical models.},
	language = {en},
	number = {4},
	urldate = {2021-06-09},
	journal = {Ecology Letters},
	author = {Joseph, Maxwell B.},
	year = {2020},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/ele.13462},
	keywords = {Deep learning, hierarchical model, neural network, occupancy},
	pages = {734--747},
}

@article{valletta_applications_2017,
	title = {Applications of machine learning in animal behaviour studies},
	volume = {124},
	issn = {0003-3472},
	url = {https://www.sciencedirect.com/science/article/pii/S0003347216303360},
	doi = {10.1016/j.anbehav.2016.12.005},
	abstract = {In many areas of animal behaviour research, improvements in our ability to collect large and detailed data sets are outstripping our ability to analyse them. These diverse, complex and often high-dimensional data sets exhibit nonlinear dependencies and unknown interactions across multiple variables, and may fail to conform to the assumptions of many classical statistical methods. The field of machine learning provides methodologies that are ideally suited to the task of extracting knowledge from these data. In this review, we aim to introduce animal behaviourists unfamiliar with machine learning (ML) to the promise of these techniques for the analysis of complex behavioural data. We start by describing the rationale behind ML and review a number of animal behaviour studies where ML has been successfully deployed. The ML framework is then introduced by presenting several unsupervised and supervised learning methods. Following this overview, we illustrate key ML approaches by developing data analytical pipelines for three different case studies that exemplify the types of behavioural and ecological questions ML can address. The first uses a large number of spectral and morphological characteristics that describe the appearance of pheasant, Phasianus colchicus, eggs to assign them to putative clutches. The second takes a continuous data stream of feeder visits from PIT (passive integrated transponder)-tagged jackdaws, Corvus monedula, and extracts foraging events from it, which permits the construction of social networks. Our final example uses aerial images to train a classifier that detects the presence of wildebeest, Connochaetes taurinus, to count individuals in a population. With the advent of cheaper sensing and tracking technologies an unprecedented amount of data on animal behaviour is becoming available. We believe that ML will play a central role in translating these data into scientific knowledge and become a useful addition to the animal behaviourist's analytical toolkit.},
	language = {en},
	urldate = {2021-06-09},
	journal = {Animal Behaviour},
	author = {Valletta, John Joseph and Torney, Colin and Kings, Michael and Thornton, Alex and Madden, Joah},
	month = feb,
	year = {2017},
	keywords = {animal behaviour data, classification, clustering, dimensionality reduction, machine learning, predictive modelling, random forests, social networks, supervised learning, unsupervised learning},
	pages = {203--220},
}

@inproceedings{LSTM,
	title = {Reinforcement learning by backpropagation through an {LSTM} model/critic},
	doi = {10.1109/ADPRL.2007.368179},
	abstract = {This paper describes backpropagation through an LSTM recurrent neural network model/critic, for reinforcement learning tasks in partially observable domains. This combines the advantage of LSTM's strength at learning long-term temporal dependencies to infer states in partially observable tasks, with the advantage of being able to learn high-dimensional and/or continuous actions with backpropagation's focused credit assignment mechanism},
	booktitle = {2007 {IEEE} {International} {Symposium} on {Approximate} {Dynamic} {Programming} and {Reinforcement} {Learning}},
	author = {Bakker, Bram},
	month = apr,
	year = {2007},
	note = {ISSN: 2325-1867},
	keywords = {Backpropagation, Dynamic programming, Intelligent networks, Intelligent systems, Laboratories, Learning systems, Neural networks, Observability, Recurrent neural networks, State-space methods},
	pages = {127--134},
}

@misc{keras-rl,
    author = {Matthias Plappert},
    title = {keras-rl},
    year = {2016},
    publisher = {GitHub},
    journal = {GitHub repository},
    howpublished = {\url{https://github.com/keras-rl/keras-rl}},
}

@misc{tensorflow-agents,
  title = {{TF-Agents}: A library for Reinforcement Learning in TensorFlow},
  author = {Sergio Guadarrama and Anoop Korattikara and Oscar Ramirez and
     Pablo Castro and Ethan Holly and Sam Fishman and Ke Wang and
     Ekaterina Gonina and Neal Wu and Efi Kokiopoulou and Luciano Sbaiz and
     Jamie Smith and Gábor Bartók and Jesse Berent and Chris Harris and
     Vincent Vanhoucke and Eugene Brevdo},
  howpublished = {\url{https://github.com/tensorflow/agents}},
  url = "https://github.com/tensorflow/agents",
  year = 2018,
  note = "[Online; accessed 25-June-2019]"
}

@misc{sb2,
  author = {Hill, Ashley and Raffin, Antonin and Ernestus, Maximilian and Gleave, Adam and Kanervisto, Anssi and Traore, Rene and Dhariwal, Prafulla and Hesse, Christopher and Klimov, Oleg and Nichol, Alex and Plappert, Matthias and Radford, Alec and Schulman, John and Sidor, Szymon and Wu, Yuhuai},
  title = {Stable Baselines},
  year = {2018},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/hill-a/stable-baselines}},
}

@article{spinningup,
    author = {Achiam, Joshua},
    title = {{Spinning Up in Deep Reinforcement Learning}},
    year = {2018}
}

@incollection{torch,
  title = {PyTorch: An Imperative Style, High-Performance Deep Learning Library},
  author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
  booktitle = {Advances in Neural Information Processing Systems 32},
  editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
  pages = {8024--8035},
  year = {2019},
  publisher = {Curran Associates, Inc.},
  url = {http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf}
}

@misc{rocker,
  title = {Rocker},
  year = {2018},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/rocker-org/rocker}},
}

@misc{tensorflow,
  title = {Tensorflow},
  year = {2018},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/tensorflow/tensorflow}},
}

@misc{reticulate,
  title = {Reticulate},
  year = {2018},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/rstudio/reticulate}},
}

@misc{ramlegacy,
    author = {{RAM Legacy Stock Assessment Database}},
    title = {RAM Legacy Stock Assessment Database v4.491},
    year = {2020},
    doi = {10.5281/zenodo.3676088},
    url = {http://doi.org/10.5281/zenodo.3676088},
}


@article{knape_are_2012,
	title = {Are patterns of density dependence in the {Global} {Population} {Dynamics} {Database} driven by uncertainty about population abundance?},
	volume = {15},
	copyright = {© 2011 Blackwell Publishing Ltd/CNRS},
	issn = {1461-0248},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1461-0248.2011.01702.x},
	doi = {10.1111/j.1461-0248.2011.01702.x},
	abstract = {Ecology Letters (2011) 14: 17–23 Abstract Density dependence in population growth rates is of immense importance to ecological theory and application, but is difficult to estimate. The Global Population Dynamics Database (GPDD), one of the largest collections of population time series available, has been extensively used to study cross-taxa patterns in density dependence. A major difficulty with assessing density dependence from time series is that uncertainty in population abundance estimates can cause strong bias in both tests and estimates of strength. We analyse 627 data sets in the GPDD using Gompertz population models and account for uncertainty via the Kalman filter. Results suggest that at least 45\% of the time series display density dependence, but that it is weak and difficult to detect for a large fraction. When uncertainty is ignored, magnitude of and evidence for density dependence is strong, illustrating that uncertainty in abundance estimates qualitatively changes conclusions about density dependence drawn from the GPDD.},
	language = {en},
	number = {1},
	urldate = {2021-06-09},
	journal = {Ecology Letters},
	author = {Knape, Jonas and Valpine, Perry de},
	year = {2012},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1461-0248.2011.01702.x},
	keywords = {Density dependence, GPDD, observation error, time series},
	pages = {17--23},
}


@article{ricard_examining_2012,
	title = {Examining the knowledge base and status of commercially exploited marine species with the {RAM} {Legacy} {Stock} {Assessment} {Database}},
	volume = {13},
	copyright = {© 2011 Blackwell Publishing Ltd},
	issn = {1467-2979},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1467-2979.2011.00435.x},
	doi = {10.1111/j.1467-2979.2011.00435.x},
	abstract = {Meta-analyses of stock assessments can provide novel insight into marine population dynamics and the status of fished species, but the world’s main stock assessment database (the Myers Stock-Recruitment Database) is now outdated. To facilitate new analyses, we developed a new database, the RAM Legacy Stock Assessment Database, for commercially exploited marine fishes and invertebrates. Time series of total biomass, spawner biomass, recruits, fishing mortality and catch/landings form the core of the database. Assessments were assembled from 21 national and international management agencies for a total of 331 stocks (295 fish stocks representing 46 families and 36 invertebrate stocks representing 12 families), including nine of the world’s 10 largest fisheries. Stock assessments were available from 27 large marine ecosystems, the Caspian Sea and four High Seas regions, and include the Atlantic, Pacific, Indian, Arctic and Antarctic Oceans. Most assessments came from the USA, Europe, Canada, New Zealand and Australia. Assessed marine stocks represent a small proportion of harvested fish taxa (16\%), and an even smaller proportion of marine fish biodiversity (1\%), but provide high-quality data for intensively studied stocks. The database provides new insight into the status of exploited populations: 58\% of stocks with reference points (n = 214) were estimated to be below the biomass resulting in maximum sustainable yield (BMSY) and 30\% had exploitation levels above the exploitation rate resulting in maximum sustainable yield (UMSY). We anticipate that the database will facilitate new research in population dynamics and fishery management, and we encourage further data contributions from stock assessment scientists.},
	language = {en},
	number = {4},
	urldate = {2021-06-09},
	journal = {Fish and Fisheries},
	author = {Ricard, Daniel and Minto, Cóilín and Jensen, Olaf P. and Baum, Julia K.},
	year = {2012},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1467-2979.2011.00435.x},
	keywords = {Marine fisheries, meta-analysis, overfishing, population dynamics models, relational database, stock assessment},
	pages = {380--398},
}


@techreport{chapman_promoting_2021,
	type = {preprint},
	title = {Promoting equity in scientific recommendations for high seas governance},
	url = {https://osf.io/jhbuz},
	abstract = {In the coming months, international negotiations under the 1982 United Nations Convention on the Law of the Sea (UNCLOS) will enter their fourth and final session to establish a legally binding agreement for the conservation and sustainable use of marine biodiversity in one of our largest global commons, areas beyond national jurisdiction (BBNJ). In this context, scientists have proposed data-driven optimization algorithms (hereafter algorithmic approaches) to assist in maximizing the protection of BBNJ while considering specific economic costs and risks. Algorithmic approaches to global environmental issues are attractive and useful because of their capacity to integrate a wide scope of data and complex objectives into definitive policy suggestions. However, data-driven scientific recommendations at a global scale largely consider the costs and benefits of conservation action to only a subset of human actors, raising critical questions of equity and environmental justice that we must address as a scientific community. Here, we 1) highlight equity oversights in algorithmic approaches to BBNJ protection, including current disparities in both the data inputs and optimization targets of these approaches; and 2) propose ways in which current algorithmic approaches can be integrated into a broader and more inclusive prioritization process for the conservation and sustainable use of BBNJ. A more equitable path forward in scientific recommendations to BBNJ protection will require not only stepping away from rigid data-driven algorithmic approaches and towards approaches that integrate established social science methodologies, but also critical and continued engagement with environmental justice principles. By integrating powerful algorithmic approaches with tools from the conservation social sciences that allow for the consideration of diverse interests in this global treaty, scientists can help ensure that our greatest global commons is managed in a more equitable manner, not just for the values and benefit of a select powerful few.},
	urldate = {2021-06-09},
	institution = {EcoEvoRxiv},
	author = {Chapman, Melissa and Oestreich, William and Frawley, Timothy H. and Boettiger, Carl and Diver, Sibyl and Santos, Bianca and Scoville, Caleb and Armstrong, Katrina and Blondin, Hannah and Chand, Kevin and Haulsee, Danielle and Knight, Christopher and Crowder, Larry},
	month = feb,
	year = {2021},
	doi = {10.32942/osf.io/jhbuz},
}



@article{scoville_algorithmic_2021,
	title = {Algorithmic conservation in a changing climate},
	volume = {51},
	issn = {18773435},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1877343521000191},
	doi = {10.1016/j.cosust.2021.01.009},
	language = {en},
	urldate = {2021-06-09},
	journal = {Current Opinion in Environmental Sustainability},
	author = {Scoville, Caleb and Chapman, Melissa and Amironesei, Razvan and Boettiger, Carl},
	month = aug,
	year = {2021},
	pages = {30--35},
}


@article{punt_management_2016,
	title = {Management strategy evaluation: best practices},
	volume = {17},
	issn = {14672960},
	shorttitle = {Management strategy evaluation},
	url = {http://doi.wiley.com/10.1111/faf.12104},
	doi = {10.1111/faf.12104},
	language = {en},
	number = {2},
	urldate = {2021-06-09},
	journal = {Fish and Fisheries},
	author = {Punt, André E and Butterworth, Doug S and de Moor, Carryn L and De Oliveira, José A A and Haddon, Malcolm},
	month = jun,
	year = {2016},
	pages = {303--334},
}


@article{ferrer-mestres_k-n-momdps_2021,
	title = {K-{N}-{MOMDPs}: {Towards} {Interpretable} {Solutions} for {Adaptive} {Management}},
	volume = {35},
	copyright = {Copyright (c) 2021 Association for the Advancement of Artificial Intelligence},
	issn = {2374-3468},
	shorttitle = {K-{N}-{MOMDPs}},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/17735},
	language = {en},
	number = {17},
	urldate = {2021-06-13},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Ferrer-Mestres, Jonathan and Dietterich, Thomas G. and Buffet, Olivier and Chades, Iadine},
	month = may,
	year = {2021},
	note = {Number: 17},
	keywords = {Environmental Sustainability},
	pages = {14775--14784},
	file = {Full Text PDF:/Users/marcus/Zotero/storage/Z8422AZ6/Ferrer-Mestres et al. - 2021 - K-N-MOMDPs Towards Interpretable Solutions for Ad.pdf:application/pdf;Snapshot:/Users/marcus/Zotero/storage/87C8NB2Q/17735.html:text/html},
}


@article{weber_imagination-augmented_2018,
	title = {Imagination-{Augmented} {Agents} for {Deep} {Reinforcement} {Learning}},
	url = {http://arxiv.org/abs/1707.06203},
	abstract = {We introduce Imagination-Augmented Agents (I2As), a novel architecture for deep reinforcement learning combining model-free and model-based aspects. In contrast to most existing model-based reinforcement learning and planning methods, which prescribe how a model should be used to arrive at a policy, I2As learn to interpret predictions from a learned environment model to construct implicit plans in arbitrary ways, by using the predictions as additional context in deep policy networks. I2As show improved data efficiency, performance, and robustness to model misspecification compared to several baselines.},
	urldate = {2021-06-13},
	journal = {arXiv:1707.06203 [cs, stat]},
	author = {Weber, Théophane and Racanière, Sébastien and Reichert, David P. and Buesing, Lars and Guez, Arthur and Rezende, Danilo Jimenez and Badia, Adria Puigdomènech and Vinyals, Oriol and Heess, Nicolas and Li, Yujia and Pascanu, Razvan and Battaglia, Peter and Hassabis, Demis and Silver, David and Wierstra, Daan},
	month = feb,
	year = {2018},
	note = {arXiv: 1707.06203},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
}


@article{hafner_learning_2019,
	title = {Learning {Latent} {Dynamics} for {Planning} from {Pixels}},
	url = {http://arxiv.org/abs/1811.04551},
	abstract = {Planning has been very successful for control tasks with known environment dynamics. To leverage planning in unknown environments, the agent needs to learn the dynamics from interactions with the world. However, learning dynamics models that are accurate enough for planning has been a long-standing challenge, especially in image-based domains. We propose the Deep Planning Network (PlaNet), a purely model-based agent that learns the environment dynamics from images and chooses actions through fast online planning in latent space. To achieve high performance, the dynamics model must accurately predict the rewards ahead for multiple time steps. We approach this using a latent dynamics model with both deterministic and stochastic transition components. Moreover, we propose a multi-step variational inference objective that we name latent overshooting. Using only pixel observations, our agent solves continuous control tasks with contact dynamics, partial observability, and sparse rewards, which exceed the difficulty of tasks that were previously solved by planning with learned models. PlaNet uses substantially fewer episodes and reaches final performance close to and sometimes higher than strong model-free algorithms.},
	urldate = {2021-08-10},
	journal = {arXiv:1811.04551 [cs, stat]},
	author = {Hafner, Danijar and Lillicrap, Timothy and Fischer, Ian and Villegas, Ruben and Ha, David and Lee, Honglak and Davidson, James},
	month = jun,
	year = {2019},
	note = {arXiv: 1811.04551},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: 20 pages, 12 figures, 1 table},
	file = {arXiv Fulltext PDF:/Users/marcus/Zotero/storage/WWALEGDT/Hafner et al. - 2019 - Learning Latent Dynamics for Planning from Pixels.pdf:application/pdf;arXiv.org Snapshot:/Users/marcus/Zotero/storage/MWNQZKU2/1811.html:text/html},
}


@article{yamada_evolution_2020,
	title = {Evolution of a {Complex} {Predator}-{Prey} {Ecosystem} on {Large}-scale {Multi}-{Agent} {Deep} {Reinforcement} {Learning}},
	url = {http://arxiv.org/abs/2002.03267},
	abstract = {Simulation of population dynamics is a central research theme in computational biology, which contributes to understanding the interactions between predators and preys. Conventional mathematical tools of this theme, however, are incapable of accounting for several important attributes of such systems, such as the intelligent and adaptive behavior exhibited by individual agents. This unrealistic setting is often insufficient to simulate properties of population dynamics found in the real-world. In this work, we leverage multi-agent deep reinforcement learning, and we propose a new model of large-scale predator-prey ecosystems. Using different variants of our proposed environment, we show that multi-agent simulations can exhibit key real-world dynamical properties. To obtain this behavior, we firstly define a mating mechanism such that existing agents reproduce new individuals bound by the conditions of the environment. Furthermore, we incorporate a real-time evolutionary algorithm and show that reinforcement learning enhances the evolution of the agents' physical properties such as speed, attack and resilience against attacks.},
	urldate = {2021-08-11},
	journal = {arXiv:2002.03267 [cs]},
	author = {Yamada, Jun and Shawe-Taylor, John and Fountas, Zafeirios},
	month = feb,
	year = {2020},
	note = {arXiv: 2002.03267},
	keywords = {Computer Science - Multiagent Systems},
	annote = {Comment: 9 pages, 13 figures},
}


@article{wang_reinforcement_2020,
	title = {A reinforcement learning-based predator-prey model},
	volume = {42},
	issn = {1476945X},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1476945X1930039X},
	doi = {10.1016/j.ecocom.2020.100815},
	language = {en},
	urldate = {2021-08-11},
	journal = {Ecological Complexity},
	author = {Wang, Xueting and Cheng, Jun and Wang, Lei},
	month = mar,
	year = {2020},
	pages = {100815},
}


@article{chades_primer_2021,
	title = {A primer on {Partially} {Observable} {Markov} {Decision} {Processes} ({POMDPs})},
	issn = {2041-210X, 2041-210X},
	url = {https://onlinelibrary.wiley.com/doi/10.1111/2041-210X.13692},
	doi = {10.1111/2041-210X.13692},
	language = {en},
	urldate = {2021-08-11},
	journal = {Methods in Ecology and Evolution},
	author = {Chades, Iadine and Pascal, Luz V. and Nicol, Sam and Fletcher, Cameron S. and Ferrer Mestres, Jonathan},
	month = aug,
	year = {2021},
	pages = {2041--210X.13692},
}


@article{frankenhuis_enriching_2019,
	title = {Enriching behavioral ecology with reinforcement learning methods},
	volume = {161},
	issn = {03766357},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0376635717303637},
	doi = {10.1016/j.beproc.2018.01.008},
	language = {en},
	urldate = {2021-09-28},
	journal = {Behavioural Processes},
	author = {Frankenhuis, Willem E. and Panchanathan, Karthik and Barto, Andrew G.},
	month = apr,
	year = {2019},
	pages = {94--100},
	file = {Full Text:/Users/marcus/Zotero/storage/AJ9QRHXC/Frankenhuis et al. - 2019 - Enriching behavioral ecology with reinforcement le.pdf:application/pdf},
}


@article{perolat_multi-agent_2017,
	title = {A multi-agent reinforcement learning model of common-pool resource appropriation},
	url = {http://arxiv.org/abs/1707.06600},
	abstract = {Humanity faces numerous problems of common-pool resource appropriation. This class of multi-agent social dilemma includes the problems of ensuring sustainable use of fresh water, common fisheries, grazing pastures, and irrigation systems. Abstract models of common-pool resource appropriation based on non-cooperative game theory predict that self-interested agents will generally fail to find socially positive equilibria---a phenomenon called the tragedy of the commons. However, in reality, human societies are sometimes able to discover and implement stable cooperative solutions. Decades of behavioral game theory research have sought to uncover aspects of human behavior that make this possible. Most of that work was based on laboratory experiments where participants only make a single choice: how much to appropriate. Recognizing the importance of spatial and temporal resource dynamics, a recent trend has been toward experiments in more complex real-time video game-like environments. However, standard methods of non-cooperative game theory can no longer be used to generate predictions for this case. Here we show that deep reinforcement learning can be used instead. To that end, we study the emergent behavior of groups of independently learning agents in a partially observed Markov game modeling common-pool resource appropriation. Our experiments highlight the importance of trial-and-error learning in common-pool resource appropriation and shed light on the relationship between exclusion, sustainability, and inequality.},
	urldate = {2021-09-28},
	journal = {arXiv:1707.06600 [cs, q-bio]},
	author = {Perolat, Julien and Leibo, Joel Z. and Zambaldi, Vinicius and Beattie, Charles and Tuyls, Karl and Graepel, Thore},
	month = sep,
	year = {2017},
	note = {arXiv: 1707.06600},
	keywords = {Computer Science - Neural and Evolutionary Computing, Computer Science - Multiagent Systems, Quantitative Biology - Populations and Evolution},
	annote = {Comment: 15 pages, 11 figures},
}


@article{kirk_survey_2022,
	title = {A {Survey} of {Generalisation} in {Deep} {Reinforcement} {Learning}},
	url = {http://arxiv.org/abs/2111.09794},
	abstract = {The study of generalisation in deep Reinforcement Learning (RL) aims to produce RL algorithms whose policies generalise well to novel unseen situations at deployment time, avoiding overfitting to their training environments. Tackling this is vital if we are to deploy reinforcement learning algorithms in real world scenarios, where the environment will be diverse, dynamic and unpredictable. This survey is an overview of this nascent field. We provide a unifying formalism and terminology for discussing different generalisation problems, building upon previous works. We go on to categorise existing benchmarks for generalisation, as well as current methods for tackling the generalisation problem. Finally, we provide a critical discussion of the current state of the field, including recommendations for future work. Among other conclusions, we argue that taking a purely procedural content generation approach to benchmark design is not conducive to progress in generalisation, we suggest fast online adaptation and tackling RL-specific problems as some areas for future work on methods for generalisation, and we recommend building benchmarks in underexplored problem settings such as offline RL generalisation and reward-function variation.},
	urldate = {2022-01-31},
	journal = {arXiv:2111.09794 [cs]},
	author = {Kirk, Robert and Zhang, Amy and Grefenstette, Edward and Rocktäschel, Tim},
	month = jan,
	year = {2022},
	note = {arXiv: 2111.09794},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	annote = {Comment: Updated versions: fixed typos, added benchmarks and methods, including adjusted Adapting Online section},
}

