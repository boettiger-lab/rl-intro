---
title: "Appendix B: Examples of Deep RL in Ecological Decision Problems"
authors:
  - name: Marcus Lapeyrolerie
    department: Department of Environmental Science, Policy, and Management
    affiliation: University of California, Berkeley
    location: Berkeley, California
  - name: Melissa Chapman
    department: Department of Environmental Science, Policy, and Management
    affiliation: University of California, Berkeley
    location: Berkeley, California
  - name: Kari Norman
    department: Department of Environmental Science, Policy, and Management
    affiliation: University of California, Berkeley
    location: Berkeley, California
  - name: Carl Boettiger
    department: Department of Environmental Science, Policy, and Management
    affiliation: University of California, Berkeley
    location: Berkeley, California
    email: cboettig@berkeley.edu


keywords:
  - blah
  - blee
  - bloo
  - these are optional and can be removed
bibliography: ../manuscript/references.bib

header-includes:
  - \usepackage{amsmath}
  - \usepackage{amssymb}

output: 
  rticles::arxiv_article:
    keep_tex: true
---

```{r}
renv::restore()
```

```{r knitr, include = FALSE}
knitr::opts_chunk$set(echo=FALSE, message = FALSE, warning = FALSE,
                      fig.width = 7, fig.height = 4, cache = FALSE)
ggplot2::theme_set(ggplot2::theme_bw())

scale_colour_discrete = function(...) ggthemes::scale_colour_solarized()
scale_fill_discrete = function(...) ggthemes::scale_fill_solarized()
pal = ggthemes::solarized_pal()(6)
txtcolor = "#586e75"
```

```{r setup, cache = FALSE, message=FALSE}
# R dependencies
library(tidyverse)
library(patchwork)
library(reticulate)

## Python dependencies loaded via R
sb3         = import ("stable_baselines3")
gym         = import ("gym")
gym_fishing = import("gym_fishing")
gym_conservation = import("gym_conservation")

## reproducible settings
np = import("numpy")
seed = 24L # integer
np$random$seed(seed)

# Optionally set to "" to force CPU-evaluation if needing perfect reproducibility
Sys.setenv("CUDA_VISIBLE_DEVICES"="")
set.seed(seed)
```

```{r }
## initialize the environment
env = gym$make("fishing-v1", sigma = 0.1)
```


```{r}
a2c = sb3$A2C$load("../python/cache/fishing_a2c")
a2c_sims = env$simulate(a2c, reps = 100L)
a2c_policy = env$policyfn(a2c, reps = 5L)
```



```{r}
ppo = sb3$A2C$load("../python/cache/fishing_ppo")
ppo_sims = env$simulate(ppo, reps = 100L)
ppo_policy = env$policyfn(ppo, reps = 5L)
```


```{r}
# Simulate management under the trained agent
td3 = sb3$TD3$load("cache/td3")
td3_sims = env$simulate(td3, reps = 500L)
td3_policy = env$policyfn(td3, reps = 5L)
```


```{r}
# Simulate under the optimal solution (given the model)
opt = gym_fishing$models$escapement(env)
opt_sims = env$simulate(opt, reps = 100L)
opt_policy = env$policyfn(opt)
```


```{r}
sims_df = bind_rows(td3_sims, a2c_sims, ppo_sims, opt_sims, .id = "model") %>%
  mutate(model = c("TD3", "A2C", "PPO", "optimal")[as.integer(model)])

policy_df = bind_rows(td3_policy, a2c_policy, opt_policy, .id = "model") %>%
  mutate(model = c("TD3", "A2C", "PPO", "optimal")[as.integer(model)])

gamma = 1 #discount
reward_df = sims_df %>% 
  group_by(rep, model) %>%
  mutate(cum_reward = cumsum(reward * gamma^time)) %>%
  group_by(time, model) %>%
  summarise(mean_reward = mean(cum_reward), 
            sd = sd(cum_reward), .groups = "drop")

```

```{r}
ymin = function(x) last(x[(ntile(x, 20)==1)])
ymax = function(x) last(x[(ntile(x, 20)==19)])

fig_sims = 
sims_df %>% 
  group_by(time, model) %>% 
  summarise(ymin = ymin(state),
            ymax = ymax(state),
            state = mean(state), .groups = "drop") %>%
  ggplot(aes(time, state, ymin = ymin, ymax = ymax, fill=model)) + 
  geom_ribbon(alpha= 0.3) + geom_line(aes(col = model))
```


```{r}
fig_policy = 
  policy_df %>% ggplot(aes(state, action, 
                           group=interaction(rep, model),
                           col = model)) + 
  geom_line(show.legend = FALSE) + 
  coord_cartesian(xlim = c(0, 1.3), ylim=c(0,0.9)) 
```

```{r}

fig_reward = reward_df %>% 
  ggplot(aes(time, mean_reward)) + 
  geom_ribbon(aes(ymin = mean_reward - 2*sd, 
                  ymax = mean_reward + 2*sd, fill = model),
              alpha=0.25, show.legend = FALSE) + 
  geom_line(aes(col = model), show.legend = FALSE) + 
  ylab("reward")
```


```{r fig1}
fig_sims / ( fig_policy + fig_reward)
```






# Conservation

```{r}
env = gym$make("conservation-v6")
```


```{r}
model = sb3$TD3$load("cache/td3-conservation")
TD3_sims = env$simulate(model, reps = 100L)
TD3_policy = env$policyfn(model, reps = 5L)
```


```{r}
a2c = sb3$A2C$load("../python/cache/conservation_a2c")
a2c_sims = env$simulate(a2c, reps = 100L)
a2c_policy = env$policyfn(a2c, reps = 5L)
```



```{r}
ppo = sb3$A2C$load("../python/cache/conservation_ppo")
ppo_sims = env$simulate(ppo, reps = 100L)
ppo_policy = env$policyfn(ppo, reps = 5L)
```



```{r}
# Simulate under the steady-state solution (given the model)
K = 1.5
alpha = 0.001
opt = gym_conservation$models$fixed_action(env, fixed_action = alpha * 100 * 2 * K )
opt_sims = env$simulate(opt, reps = 100L)
opt_policy = env$policyfn(opt)
```

We gather together the results under the RL agent and steady-state policy as before,

```{r}
sims_df = bind_rows(TD3_sims, a2c_sims, ppo_sims, opt_sims, .id = "model") %>%
  mutate(model = c("TD3", "A2C", "PPO", "steady-state")[as.integer(model)])

policy_df = bind_rows(TD3_policy, a2c_policy, ppo_policy, opt_policy, .id = "model") %>%
  mutate(model = c("TD3", "A2C", "PPO", "steady-state")[as.integer(model)])

gamma = 1 #discount
reward_df = sims_df %>% 
  group_by(rep, model) %>%
  mutate(cum_reward = cumsum(reward * gamma^time)) %>%
  group_by(time, model) %>%
  summarise(mean_reward = mean(cum_reward), 
            sd = sd(cum_reward), .groups = "drop")

```

```{r}
fig_sims = 
  sims_df %>%
   group_by(time, model) %>% 
   summarise(ymin = ymin(state),
            ymax = ymax(state),
            state = mean(state), .groups = "drop") %>%
  ggplot(aes(time, state, ymin = ymin, ymax = ymax, fill=model)) + 
   geom_ribbon(alpha= 0.3) + geom_line(aes(col = model))

fig_policy = 
  policy_df %>% ggplot(aes(state, action, 
                           group=interaction(rep, model),
                           col = model)) + 
  geom_line(show.legend = FALSE) 

fig_reward = reward_df %>% 
  ggplot(aes(time, mean_reward)) + 
  geom_ribbon(aes(ymin = mean_reward - 2*sd, 
                  ymax = mean_reward + 2*sd, fill = model),
              alpha=0.25, show.legend = FALSE) + 
  geom_line(aes(col = model), show.legend = FALSE) + 
  ylab("reward")

fig_sims / ( fig_policy + fig_reward)
```



