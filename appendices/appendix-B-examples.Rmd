---
title: "Appendix B: Examples of Deep RL in Ecological Decision Problems"
authors:
  - name: Marcus Lapeyrolerie
    department: Department of Environmental Science, Policy, and Management
    affiliation: University of California, Berkeley
    location: Berkeley, California
  - name: Melissa Chapman
    department: Department of Environmental Science, Policy, and Management
    affiliation: University of California, Berkeley
    location: Berkeley, California
  - name: Kari Norman
    department: Department of Environmental Science, Policy, and Management
    affiliation: University of California, Berkeley
    location: Berkeley, California
  - name: Carl Boettiger
    department: Department of Environmental Science, Policy, and Management
    affiliation: University of California, Berkeley
    location: Berkeley, California
    email: cboettig@berkeley.edu


bibliography: ../manuscript/references.bib

header-includes:
  - \usepackage{amsmath}
  - \usepackage{amssymb}

output: 
  rticles::arxiv_article:
    keep_tex: true
---

# Overview

In this appendix, we provide examples with annotated code illustrating how to use recent deep RL algorithms
to train RL agents to make decisions in two distinct ecological management scenarios: setting a harvest quota in a fishery, and managing conservation effort in the face of ecological tipping points.
The approach illustrated here should be sufficient to reproduce the results presented in the main text,
and should serve as an effective starting point for researchers seeking to apply deep RL to these simulation environments, 
to other simulation environments we have built and continue to develop, or to their own environments.
(Appendix C details the process of defining one's own `gym` environment from an ecological model). 
This overview is neither a complete introduction to deep RL or a comprehensive analysis of RL solutions to these environments.



```{r knitr, include = FALSE}
knitr::opts_chunk$set(echo=TRUE, message = FALSE, warning = FALSE,
                      fig.width = 7, fig.height = 4, cache = FALSE)
ggplot2::theme_set(ggplot2::theme_bw())

scale_colour_discrete = function(...) ggthemes::scale_colour_solarized()
scale_fill_discrete = function(...) ggthemes::scale_fill_solarized()
pal = ggthemes::solarized_pal()(8)
txtcolor = "#586e75"
```


# Deep Reinforcement Learning in R

Although all the necessary tooling for deep RL is most extensively implemented in Python, the R language is more familiar to most ecologists.
Fortunately, modern bindings such as the `reticulate` package [@reticulate] make it straightforward to use these tools without ever leaving the R interface.
In this appendix, we detail this "pure R" approach, as well as a "pure Python" approach.

In the R based-approach, R functions take responsibility from the user for translating commands into Python code before it is executed, an approach commonly referred to as meta-programming.
This still requires a local Python installation, which can be installed directly from R using the command `install_miniconda()` from the `reticulate` package in R.
Alternately, users may prefer running the analysis inside a docker container.  
The Rocker Project [@rocker] provides pre-built docker containers which include the necessary R and Python environments, as well as CUDA libraries required to take advantage of GPU-based acceleration on suitable architectures.
This can be particularly useful for users running ML algorithms on remote servers.

By using a cache of pre-trained agents distributed with this appendix, it should be possible to efficiently reproduce the results shown here on any recent laptop with the versions of software indicated.
Reproducing the training of RL agents is more computationally intensive, and a more powerful multi-core CPU or CUDA-enabled GPU is recommended.
Note that hardware differences are also a source of instability in the reproduciblity of RL training (see main text and Appendix A).

In this appendix, we illustrate how to apply a deep reinfocement learning framework to train previously published RL algorithms on the novel decision environments as illustrated in the main text.
Appendix C discusses the construction of such environments in code. 

# Deep Reinforcement Learning Frameworks

Rather than implement deep RL algorithms from scratch, we illustrate the use of one of the leading software frameworks for research and application of deep RL methods.
At the time of writing, several major frameworks exist which provide reference implementations of the leading deep RL algorithms (see table 1 and Appendix A).
While we do not seek to provide a comprehensive review of available frameworks, some familiarity with current frameworks can be helpful.
Like many machine learning libraries, these frameworks are themselves each built around one of two popular machine learning libraries, PyTorch [@torch] or Tensorflow [@tensorflow], and all are based in Python.
Existing frameworks we evaluated include Keras-RL [@keras-rl], Tensorflow Agents [@tensorflow-agents],  OpenAI Spinning Up [@spinningup] OpenAI Baselines, Stable-Baselines2 [@sb2], and Stable-Baselines3 [@sb3]. 
Keras-RL saw widespread early adoption, but is built on Tensorflow version 1.x. It is incompatible with Tensorflow 2.x and not actively maintained.
Tensorflow Agents is developed by the Tensorflow team, a recent and actively developed framework with support for both Tensorflow 1.x and 2.x and good support for low-level customization of RL algorithms.
However, higher-level interfaces and high-level documentation are still relatively limited.
OpenAI's SpinningUp is an education-targeted framework useful for developers wanting to become more familiar with the internal methods of RL algorithms.  
OpenAI's Baselines is primarily Tensorflow 1.x-based implementation of many recently published RL algorithms.
Researchers at Ensta Paris Tech first created Stable Baselines as a fork of the OpenAI implementation, rigorously addressing numerous issues in documentation, testing, and coding style that have helped make their fork see even greater adoption. 
Stable-Baselines3 is the most recent version (Feb 2021), a ground-up rewrite which switches to a PyTorch-based implementation and further strengthens internal checks such as static types.
The examples here all use the Stable-Baselines3 framework, though researchers in this area should expect frameworks to continue to emerge and evolve. 
Grand challenge problems will likely require significant development beyond the current algorithms and capabilities available in existing frameworks.


# Getting started

Clone the repository <https://github.com/boettiger-lab/rl-intro>, e.g. using the New Project->From Version Control->Git menu in RStudio.

From the project directory, we can then install all the necessary dependencies using `renv`, which helps ensure a reproducible environment of fixed versions of R packages and Python modules.

```{r message=FALSE}
#install.packages("renv")
#renv::restore()
```


Python users can install the dependencies listed in the repository's `requirements.txt` and run the corresponding python scripts found in the `python/` sub-directory instead. 
Meanwhile, the RMarkdown source-code for this file can be found in the `appendices` directory of the project repository.
Once the packages have installed, we are ready to load the necessary libraries.  Note that the `import` function from `reticulate` package acts much like the `library` command in R, though it does not attach the package function to the global namespace.  To make it more convenient to access those functions, we can assign a shorthand name.  

```{r setup, cache = FALSE, message=FALSE}
# R dependencies
library(tidyverse)
library(patchwork)
library(reticulate)

## Python dependencies loaded via R
sb3      = import("stable_baselines3")
torch    = import("torch")
gym      = import("gym")
gym_fish = import("gym_fishing")
gym_cons = import("gym_conservation")


source("../R/plotting-helpers.R")
```

Numerical reproducibility can be challenging in machine learning problems, particularly when using GPU-based acceleration. In addition to setting a random seed in our Python environment, we can optionally disable GPU use to improve reproducibility by setting the `CUDA_VISIBLE_DEVICES` to a blank value.  

```{r}
## reproducible settings
np = import("numpy")
seed = 24L # integer
np$random$seed(seed)

# Optionally set to "" to force CPU-evaluation if needing perfect reproducibility
#Sys.setenv("CUDA_VISIBLE_DEVICES"="")
set.seed(seed)
```

<!-- is this too verbose? --> 
The above code also illustrates a few conventions which may be helpful to bear in mind when using the `reticulate` interface to interact with Python from R: 
it is often necessary for integer values to be explicitly typed as integers by adding a trailing `L` (corresponding to the primitive C type `long` integer).
Python is also more strongly object-oriented than many R packages, where "methods" of an "object" are accessed with the list-subset operator `$` in R (equivalent to the use of `.` in Python).
Lastly, while R can use `<-` or `=` for assignment, Python uses only `=`. For simplicity we will stick with the latter.  
With few exceptions, the R code shown here can be re-written as python code by dropping the `L` and replacing `$` with `.`, see stand-alone python code in the `python/` subdirectory of the repository.


# Finding a known optimal solution using RL


## Sustainable Harvest Quotas

We begin by selecting an environment from `gym_fishing`.  While we use default parameter values for this example, note that is possible to pass alternative parameterizations using additional optional arguments.

```{r }
## initialize the environment
env = gym$make("fishing-v1")
```


## An optimal solution

Recall that under the assumptions of the simple model used in the `fishing-v1` environment, we can determine the optimal harvest policy analytically if the model and parameters are known precisely [@Reed1979].
The optimal strategy is a policy of 'constant escapement', designed to keep the remaining stock size (the population that 'escapes' fishing harvest) at the biomass corresponding to a maximum growth rate, i.e. at $B_{MSY} = K/2$ in this model.
`gym_fishing` defines a collection of non-RL agents in the `models` submodule, including the a human agent that merely asks to enter their desired quota manually. 
The `escapement` model implements the provably optimal constant escapement rule.
A third model, `msy`, implements a policy based on "Maximum Sustainable Yield" policy [@Schaefer1954], which is actually more commonly used as a basis for management than constant escapement, despite only being optimal at the steady state under deterministic dynamics.  

```{r}
# Simulate under the optimal solution (given the model)
opt = gym_fishing$models$escapement(env)
```

Note that the `escapement` function makes very specific assumptions about the environment that it is given.
In particular, it assumes we can compute $B_{MSY}$ directly from the internal model of stock recruitment contained in the environment.
In contrast, the RL methods make no such assumption of being able to access that internal model directly.


```{r}
opt_sims = env$simulate(opt, reps = 100L)
opt_policy = env$policyfn(opt)
```


We can plot the resulting data.frames using standard `ggplot2` methods shown in the `R/plotting-helpers.R`, or in python using either the `plot` methods defined in `gym_fishing` or standard plotting libraries.

```{r}
algos <- c("Optimal", "PPO", "PPO_tuned")
sims_df <- opt_sims %>% mutate(model = factor("Optimal", algos))
policy_df <- opt_policy  %>% mutate(model = factor("Optimal", algos))

plot_sims(sims_df) / ( plot_policy(policy_df) + plot_reward(sims_df))
```

## RL-based solutions

We will compare this optimal solution with results from the several different RL algorithms.
Unlike the optimal solution, the RL methods never know the underlying growth function used by the simulation (or even if such a function actually exists).
These methods merely seek to learn a strategy for setting harvest quotas which maximizes the cumulative reward the receive from the environment.
At the heart of each deep RL algorithm is a neural network or a collection of neural networks. The RL agent uses these networks to approximate a policy function and/or a value function. The policy function is the agent's mapping of states to actions -- i.e. the agent's strategy. Value functions attempt to evaluate the goodness of a state or state-action pair towards achieving the RL objective of maximizing cumulative rewards. 
Confusingly, the parameters of a RL algorithm that detail the overall learning process are called _hyper-parameters_ while the parameters of a RL algorithm that are learned during training are called parameters. For example, the number of layers in a neural network is a hyper-parameter but a weight in the neural net is referred to as a parameter.
Before training an agent, we must first specify the hyper-parameters; the algorithm's parameters are often randomly initialized.
Each algorithm may have different hyper-parameters. `stable-baselines3` provides default values for all hyper-parameters based on the original papers that introduced the corresponding algorithms.
For example, here we will try the Proximal Policy Optimization (PPO), originally described in @PPO.
Here, we train an agent using the PPO algorithm using default hyper-parameter settings with a multi-layer perceptron ('MlpPolicy') policy network and value network composed of two 64-neuron layers.

```{r}
ppo = sb3$PPO('MlpPolicy', env, seed = seed) 
```


Training is the main computationally intensive process, which can take anywhere from a few minutes to many days, depending on the complexity of the environment, the neural network architecture and the number of training iterations budgeted.
Therefore, we save the trained agent to disk, and only execute the training method (`learn`) if no previously saved agent is found:

```{r}
if(!file.exists("../python/cache/ppo.zip")) {

  # Train the agent for a fixed number of timesteps
  ppo$learn(total_timesteps=300000L)
  
  # Save our trained agent for future use
  ppo$save("../python/cache/ppo")

}
```


Note that while default hyper-parameters provide a useful starting place (particularly when the environment has been suitably normalized, a best-practice we discuss above), better performance can almost always be achieved by _tuning_ the hyper-parameter selection.
This is discussed further below.
Having saved our trained agent to disk (`cache/ppo.zip`), we can then re-load this agent for evaluation or to continue training.
Note that a copy of the trained agents are included in the corresponding GitHub repository.

```{r}
# Simulate management under the trained agent
ppo = sb3$PPO$load("../python/cache/ppo")
```


We can supply an observation to our trained agent and it will return it's proposed action using the `predict` method. 
This is all we need to evaluate or employ the agent on a decision-making task.
Recall that state space and action space in the fishing gym have been re-scaled to a (-1, 1) interval.
Note that this is equivalent to a choice of appropriate units -- we can re-scale the interval without loss of generality.
This is often an important step in the design of an RL environment to facilitate successful training.
Following the `gym` standard, the core methods such as `predict` and `step` operate on the re-scaled units,
so it is necessary to first transform the original units into this re-scaled state space.
For example, if we wish to start a simulation with a stock size of 0.75, we can use the helper
method `get_state()`, to determine the corresponding value in the re-scaled state space.  
Unlike `predict` and `step`, `get_state()` is not a standard method of all gym environments -- typically
a user must first inspect the state space of an environment and choose themselves how to re-scale their
problem into that state space.  

```{r}
## represent the initial state size in the 'rescaled' state space.
state = env$get_state( 0.75 )
state
```


With an initial state in hand, we are ready to simulate management using our agent.
The iteration is simple: we use the agent to predict what action we should take given the current state. Then, we take said action and examine the result to determine the future state. 
Because these methods return additional information as well, a little extra sub-setting is required in R:

```{r}
for(i in 1:10){
  
  out = ppo$predict(state, deterministic=TRUE)
  action = out[[1]]
  result = env$step(action)
  state = result[[1]]

}

```

For convenience, `gym_fishing` defines the helper routine `simulate` to perform the above iteration `reps` number of times. The `simulate` method returns the state, action, and reward resulting from each time step of each replicate:


```{r}
ppo_sims = env$simulate(ppo, reps = 100L)
```


Because the agent typically attempts to retain the fish population near a specific value, simulations with well-trained agents will usually not explore the full range of possible states.
To get a better idea of how the agent behaves across the full state space, it is common to examine the policy function: to do this, we'll plot the action taken at every possible state.
Some agents are non-deterministic, so we may want to use replicate draws at each state to get a better picture of the agent's behavior.
Note that the `policyfn` method is another custom method and not part of the `gym` standard. 
The use of `policyfn` will not make sense for agents that consider the history of many previous states in selecting their action, such as agents which utilize recurrent neural network architectures like LSTMs [@LSTM].

```{r}
ppo_policy = env$policyfn(ppo, reps = 5L)
```


```{r}
# stack result simulations with those above and plot:
sims_df <- ppo_sims %>% 
  mutate(model = factor("PPO", algos)) %>% 
  bind_rows(sims_df)
policy_df <- ppo_policy  %>% 
  mutate(model = factor("PPO", algos))  %>% 
  bind_rows(policy_df)

plot_sims(sims_df) / ( plot_policy(policy_df) + plot_reward(sims_df))
```

Note that the un-tuned version of PPO has found a viable policy which slowly increases its total reward over the course of the simulation,
but nevertheless is substantially sub-optimal in its performance.
Under the management of this agent, stock sizes average much higher than $B_{MSY}$ level of the optimal solution.
The stock sizes also fluctuate as the agent constantly adjusts the harvest quota in response to the previous level.



## Improving performance by tuning:

Good choices for hyper-parameters can be found through a process known as "hyper-parameter tuning", which uses standard function optimization approaches to vary the choice of hyper-parameters to determine which hyper-parameters maximize the agent's performance.
In practice, tuning may attempt to set only a subset of parameters through an optimal search algorithm, while others are chosen manually.
Because each iteration in tuning must retrain the algorithm over thousands or millions of episodes, tuning can easily become very computationally intensive.
For simplicity, here we illustrate how to train PPO using custom hyperparameters.
The PPO algorithm supports training on a vectorized environment, in which the algorithm learns by interacting with `n` independent copies of the environment simultaneously.
The choice of `n` can itself be considered another hyperparameter.
Additional choices for many of the hyperparameters used by the PPO model are specified as additional optional arguments.
In this case, the parameter choices shown were already derived through tuning (see <https://github.com/boettiger-lab/conservation-agents>, or the standard reference examples presented in `stable-baselines3-zoo` [@zoo]).
More generally, these choices could represent any alternative trial of possible hyperparameters.


```{r}
vec_env = sb3$common$env_util$make_vec_env("fishing-v1", n_envs=4L, seed=seed)


# Create a fishing environment
# Create an agent
ppo_tuned = sb3$PPO("MlpPolicy",
                    "env" = vec_env, 
                    "seed" = seed,
                    "batch_size" = 64L,
                    "n_steps" = 32L,
                    "gamma" = 0.9999,
                    "learning_rate" = 0.00037996229760279524,
                    "ent_coef" = 2.392750198613048e-08,
                    "clip_range" = 0.3,
                    "n_epochs" = 10L,
                    "gae_lambda" = 0.92,
                    "max_grad_norm" = 0.7,
                    "vf_coef" = 0.4863551514846155,
                    "policy_kwargs" = list(
                      "net_arch" = c(64L, 64L),
                      "activation_fn" = torch$nn$ReLU
                     )
                   )

```

We train this new PPO-based agent for 300,000 steps, as before:

```{r }
if(!file.exists("../python/cache/ppo_tuned.zip")){
  
  ppo_tuned$learn(total_timesteps=300000L)
  ppo_tuned$save("../python/cache/ppo_tuned")
  
}
```

With our trained agent, we can now simulate replicate scenarios or scan across states to determine the implicit policy function:

```{r}
# Simulate management under the trained agent
ppo_tuned = sb3$PPO$load("../python/cache/ppo_tuned")
ppo_tuned_sims = env$simulate(ppo_tuned, reps = 100L)
ppo_tuned_policy = env$policyfn(ppo_tuned, reps = 5L)
```

 
```{r}
# stack result simulations with those above and plot:
sims_df <- ppo_tuned_sims %>% 
  mutate(model = factor("PPO_tuned", algos)) %>% 
  bind_rows(sims_df)
policy_df <- ppo_tuned_policy  %>% 
  mutate(model = factor("PPO_tuned", algos))  %>% 
  bind_rows(policy_df)

plot_sims(sims_df) / ( plot_policy(policy_df) + plot_reward(sims_df))
```


The tuned solution is still not optimal, but it is very close.
Moreover, the RL algorithm has been able to learn this nearly-optimal policy with no prior knowledge about the shape or nature of the population growth function used in the environment. 
  
We preserve a copy of each data frames in local `.csv` text files for maximum cross-platform compatibility.

```{r}
write_csv(sims_df, "../manuscript/figs/sims_df.csv")
write_csv(policy_df, "../manuscript/figs/policy_df.csv")
```


<!-- Remaining is just plotting code, repeated in the main .Rmd.  Should we omit it from the appendix? -->


## Applying an RL agent to real data


We use historical data from Argentine Hake to illustrate how an RL agent might be applied in practice.

Historical biomass and catch data for Argentine Hake can be found in the R.A. Myers Legacy Stock Assessment Database [@ramlegacy].
We will use the agent we have just trained using the `gym_fishing` simulator above, 

```{r}
# Simulate management under the trained agent
env = gym$make("fishing-v1", r = 1.0379274,	K = 1.197693, sigma = 0.1121662)
agent = sb3$PPO$load("../python/cache/ppo_tuned")

```



```{r}
hake = read_csv("../data/hake.csv")
x0 = hake %>%
 filter(year == min(year)) %>%
  pull(scaled_biomass)
Tmax = 15
years =1986:2000

state = action = numeric(Tmax)
state[1] = x0
action[1] = NA

```


Our hindcast considers the scenario of managing catch quotas using TD3, beginning in 1986. 
We compare both the anticipated harvest and resulting stock biomass to the historically recorded harvest and biomass.
This shows that quotas set by TD3 would have been lower than historical values throughout the late 80s and early 90s, which
saw a sharp decline in estimated biomass of the Argentine Hake.
For the same or higher estimated stock sizes from the historical record, we see that TD3 would recommend a lower quota.
We also see that under TD3 the stock recovers sufficiently to predict higher quotas by the second half of the 90s than were
being set in the now-depleted stock.  
Whether the TD3 quotas would have truly been sufficient to permit recovery in stock sizes shown in the simulation clearly depends on the accuracy of the underlying simulation.
However, the fact that TD3 would set a lower quota than the historical management given the same or even higher estimate of stock size is independent of the Hake simulation.
Combined with the observed historical declines, this provides some evidence that the RL agent could have decreased or avoided the over-harvesting in this case. 


```{r}
env$reset()
## represent the initial state size in the 'rescaled' state space.
state = env$get_state( x0 )

for(i in 2:Tmax){
  
  # RL-recommended harvest action:
  out = agent$predict(state, determinstic=TRUE)
  action = out[[1]]
  
  # Record state and resulting action
  state[i] = env$get_fish_population(state)
  action[i] = env$get_quota(action)
  
  # Implement RL-recommended harvest. 
  result = env$step(action)
  state = result[[1]]
  
  #state = env$get_state(hake$scaled_biomass[i+1])
}

df = bind_rows(
  tibble(year = years, state = state, action = action, model = "(tuned) PPO"),
  tibble(year = years, state = hake$scaled_biomass, action = hake$scaled_catch, model = "historical"))

df %>% ggplot(aes(year, state, col=model)) + geom_line() + geom_point(aes(year, action, col=model))

```




# Ecological tipping points

Our second example utilizes our `gym_conservation` to provide the necessary environment to simulate an ecosystem approaching a tipping point.

## Tipping point model

The tipping point model is based on the consumer-resource model of @May1977, which creates alternative stable states, 
which we subject to log-normal environmental noise:

\[
\mu_{t} = X_t + X_t r \left(1 - \frac{X_t}{K} \right) - \frac{a_t X_t^q}{X_t^q + b^q}
\]
\[
X_{t+1} \sim \text{lognormal}(\mu_t, \sigma)
\]

where we take $r=0.7$, $K=1.2$, $q=3$, $b=0.15$, $a_0 = 0.19$ and $\sigma = 0.2$ 
Slow change over time in the parameter $a_t$ represents a process of environmental degradation, modeled as a constant increment $a_{t+1} = a_t + \alpha$, where we will take $\alpha = 0.001$.
This model supports the dynamics of a fold bifurcation, widely used to model critical transitions in both theory and empirical manipulation in systems from microbes [@Dai2012] to lakes [@Carptenter2011] to the planet biosphere [@Barnosky2014].  



```{r message=FALSE, fig.cap="Bifurcation diagram for tipping point scenario. The ecosystem begins in the desirable 'high' state under an evironmental parameter (e.g. global mean temperature, arbitrary units) of 0.19.  In the absence of conservation action, the environment worsens (e.g. rising mean temperature) as the parameter increases.  This results in only a slow degredation of the stable state, until the parameter crosses the tipping point threshold at about 0.215, where the upper stable branch is anihilated in a fold bifurcation and the system rapidly transitions to lower stable branch, around state of 0.1.   Recovery to the upper branch requires a much greater conservation investment, reducing the parameter all the way to 0.165 where the reverse bifurcation will carry it back to the upper stable branch."}
bifur_df = read_csv("../manuscript/figs/bifur.csv", col_types = "dccd")
bifur_df %>%
  ggplot(aes(parameter, state, lty=equilibrium, group = group)) + 
  geom_line()

```


We assume the benefit provided by the ecosystem state is assumed to be directly proportional to the state itself, $b x_t$.  
We further assume that each year the manager has the option to slow or reverse the environmental degradation by taking action $A_t$, such that under management, the resulting environment in the next time step is given by

\[
a_{t+1} = a_t + \alpha - A_t
\]

We assume the cost associated with that action to be proportional to the square of the action, such that large actions are proportionally more costly than small ones.
Consequently, the utility at time $t$ is given by the sum of costs and benefits:

\[U(X_t, A_t) = b X_t - c A_t ^2\]

where we will take \(b = 1\) and \(c=1\).  
Our implementation in `gym_conservation` allows the user to consider alternative parameter choices, alternative models for the ecological dynamics, and alternate types of actions, such as manipulating the ecosystem state directly rather than manipulating the environmental parameter. 
For some such scenarios the optimal solution is known or can be determined by stochastic dynamic programming, while for others, including the scenario of focus here, the optimal solution is unknown.


For simplicity of illustration, we will train only a TD3-based agent on this scenario.


```{r}
# Python users see python/conservation_TD3.py
env = gym$make("conservation-v6")

noise_std = 0.4805935357322933
OU = sb3$common$noise$OrnsteinUhlenbeckActionNoise
action_noise = OU(mean = np$zeros(1L),  sigma = noise_std * np$ones(1L))

model = sb3$TD3('MlpPolicy', 
            env, 
            verbose = 0, 
            seed = 42L,
            "gamma"= 0.995,
            "learning_rate"=  8.315382409902049e-05,
            "batch_size"= 512L,
            "buffer_size"= 10000L,
            "train_freq"= 1000L,
            "gradient_steps"= 1000L,
            "action_noise"= action_noise,
            "policy_kwargs"= list("net_arch"= c(64L,64L)))

```

As before, we train the agent unless a cached version of the trained agent is already available:

```{r}
if(!file.exists("../python/cache/td3-conservation.zip")){

  model$learn(total_timesteps=3000000L)
  model$save("../python/cache/td3-conservation")
  
}

```


```{r}
# Simulate management under the trained agent
# See https://github.com/boettiger-lab/conservation-agents/blob//conservation/TD3-v6.py

model = sb3$TD3$load("../python/cache/td3-conservation")
TD3_sims = env$simulate(model, reps = 100L)
TD3_policy = env$policyfn(model, reps = 10L)
```


In general, the optimal solution depends on the ecological dynamics, the benefit of the ecosystem services and the costs associated with a management response.
Because the tipping point problem is non-autonomous, we cannot solve for the optimal policy even given the model and objective (utility) function using Markov Decision Process methods.
However, a simple heuristic solution provides a reasonable starting point for comparison.


As discussed in the main text, we have no existing optimal solution to the tipping point problem, and so rely on a common heuristic strategy instead:
select a fixed level of conservation investment that is sufficient to counter-balance any further side towards the tipping point, preserving it in it's current state.
This is implemented using the `fixed_action` method provided in our `gym_conservation` module, which also implements other heursitic models, including a human agent which requires interactive input to select the action each year.

```{r}
# Simulate under the steady-state solution (given the model)
K = 1.5
alpha = 0.001
opt = gym_conservation$models$fixed_action(env, fixed_action = alpha * 100 * 2 * K )
opt_sims = env$simulate(opt, reps = 100L)
opt_policy = env$policyfn(opt)
```


We gather together the results under the RL agent and steady-state policy as before,

```{r}
sims_df = bind_rows(TD3_sims, opt_sims, .id = "model") %>%
  mutate(model = c("TD3", "steady-state")[as.integer(model)])

policy_df = bind_rows(TD3_policy, opt_policy, .id = "model") %>%
  mutate(model = c("TD3", "steady-state")[as.integer(model)])



```

The resulting three data frames contain the necessary data for each of the subplots in figure 3 of the main text.


```{r}
write_csv(sims_df, "../manuscript/figs/tipping_sims_df.csv")
write_csv(policy_df, "../manuscript/figs/tipping_policy_df.csv")
```



### Manuscript Figure 3

```{r fig3}
plot_sims(sims_df) / ( plot_policy(policy_df) + plot_policy(sims_df))
```


Because it is difficult to get a feel for the dynamics of individual replicate simulations from ensemble statistics, we select a few example trajectories to examine directly. 
Of the 100 replicate simulations, we pick 4 examples that dip below a state value of 0.2 for over 15 consecutive timesteps, indicating a transition into the lower basin of attraction. 
Comparing the dynamics under the rule of thumb steady-state strategy to that of the RL-trained agent, it is clear that the RL agent does a better job at both avoiding tipping points and promoting the recovery of those selected trajectories that cross into the lower attractor. 



```{r}
# Some individual replicates, for comparison

## First 4 of the TD3 reps falling below .2 for more than 15 steps
is_low = sims_df %>% 
  filter(model == "TD3") %>% 
  group_by(rep, model) %>% 
  summarize(low = sum(state < .2) > 10) %>% 
  filter(low) %>% head(6)

## First 4 such cases:
sims_df %>% inner_join(is_low) %>%
  ggplot(aes(time, state,  group=interaction(model, rep))) +
  geom_line(color = pal[3], show.legend = FALSE) + facet_wrap(~rep)
```


```{r}
# Some individual replicates, for comparison
is_low = sims_df %>% filter(model == "steady-state") %>%
  group_by(rep, model) %>% summarize(low = sum(state < .2) > 10) %>% 
  filter(low) %>% head(6)
sims_df %>% inner_join(is_low) %>%
  ggplot(aes(time, state, group=interaction(model, rep))) +
  geom_line(color = pal[1], show.legend = FALSE) + facet_wrap(~rep)
```


```{r}
```