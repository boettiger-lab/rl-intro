---
title: "Appendix A: Deep Reinforcement Learning"
authors:
  - name: Marcus Lapeyrolerie
    department: Department of Environmental Science, Policy, and Management
    affiliation: University of California, Berkeley
    location: Berkeley, California
  - name: Melissa Chapman
    department: Department of Environmental Science, Policy, and Management
    affiliation: University of California, Berkeley
    location: Berkeley, California
  - name: Kari Norman
    department: Department of Environmental Science, Policy, and Management
    affiliation: University of California, Berkeley
    location: Berkeley, California
  - name: Carl Boettiger
    department: Department of Environmental Science, Policy, and Management
    affiliation: University of California, Berkeley
    location: Berkeley, California
    email: cboettig@berkeley.edu


keywords:
  - Decision Theory
  - Conservation
  - Artificial Intelligence
  - Machine Learning
  - Tipping Points
  - Fisheries
bibliography: ../manuscript/references.bib

header-includes:
  - \usepackage{amsmath}
  - \usepackage{amssymb}

output: 
  rticles::arxiv_article:
    keep_tex: true
---


In this appendix, we will go into further detail on how deep RL algorithms work.
The aim here is not to give full treatment of all deep RL methods -- @suttonbarto is an excellent resource for theoretical underpinnings while the white papers behind the algorithms go into greater detail on modern approaches --; instead, this section is meant to serve as an abbreviated background view on common, model-free deep RL techniques.
We hope to convey the sense of what the different algorithm classes try to achieve and will point to specific resources for readers looking for a more complete presentation.
We will discuss policy gradients, value-based and actor-critic critic methods which were briefly introduced in the manuscript. 
These sections presume some familiarity with gradient descent optimization -- see @ruder_overview_2017 for a thorough treatment.
Throughout these sections, we'll consider the environment to be an MDP which makes for a cleaner presentation here and is also consistent with the referenced sources.


## Policy Gradients
The most straightforward way to optimize the RL objective is through a policy search, whereby the agent continually updates a policy parameterized by $\theta$, $\pi_\theta$ -- in deep RL, this parameterized policy will be a neural network.
There are both gradient-free and gradient-based policy search methods that have been successful in practice [@salimans_evolution_2017]; but for the sake of brevity, we will focus on gradient-based methods, which are more commonly encountered. 
In policy gradient methods, algorithms are performing gradient ascent on the expected return, $J(\pi_\theta)$, to find the optimal policy parameters.
At each gradient step, the following update is performed,

$$
  \theta_{t+1} = \theta_t + \alpha \nabla_\theta J(\pi_\theta).
$$

Supposing that the agent has interacted with the environment and has collected some trajectories denoted by, $\tau_i$, we can employ the policy gradient theorem -- see @suttonbarto for the proof of the policy gradient theorem -- to estimate $\nabla J(\pi_\theta)$ as:

$$
  \nabla_\theta J(\pi_\theta) \approx \sum_{i,t} \nabla_\theta \log{\pi_\theta (a_{i,t} \mid s_{i,t})} \Big( \sum_{t'=t}^H \gamma^{t'-t} r(s_{i, t'}, a_{i, t'}) \Big).
$$

The intuition behind this approximation is exactly what we want: the agent will increase the probability of repeating the actions for the inputted states proportional to the size of the return.
In practice, however, the empirical return tends to have high variance, resulting in very noisy gradients that impedes learning [@suttonbarto].
To avoid this issue, many policy gradient algorithms replace the empirical return with a return estimator that has lower variance [@suttonbarto].

Putting the 
A sketch of a simple policy gradient algorithm is shown in ALGORITHM.

## Value-based Methods 
Another way to approach the RL problem is by estimating a value function, and then using the value function to retrieve a policy. 
Value functions attempt to quantify the goodness of being in a state or taking an action from a state.
Given that the objective of RL is to maximize the cumulative expected reward, "goodness" refers to how high of a cumulative reward the agent can expect to receive.
For example, the state-action-value or Q function finds the expected return from a state-action pair under a policy, $\pi$,

$$
  Q^\pi(s_t, a_t) = \mathbb{E}_{\tau \sim p_\pi(\tau \mid s_t, a_t)}\Bigg[\sum_{t' = t}^H \gamma^{t' - t} r(s_t, a_t)\Bigg].
$$

There are more value functions that can used in practice [@gaeschulman], but since Q-functions are commonly encountered, we will focus on Q-learning.
From the above equation, we can define the Q function recursively,

$$
  Q^\pi(s_t, a_t) = r (s_t, a_t) + \gamma  \, \mathbb{E}_{s_{t+1} \sim T(s_{t+1} | s_t, a_t), \, a_{t+1} \sim \pi(a_{t+1}| s_{t+1})}\big[ Q^\pi(s_{t+1}, a_{t+1})\big]
$$

The motivation behind Q-learning is that if we know the Q function under certain conditions -- e.g. that the state-action space is discrete and that all state-action pairs are visited ad infinitum [@suttonbarto 131; @Qproof] --, then we can easily find the optimal action at any state by selecting the action with the highest Q-value: $a_t^* = \underset{a_t}{\text{argmax}} \, Q (s_t, a_t)$.
Using this optimal policy, we can write an equation for the optimal Q-function from the previous definitions,

$$
  Q^*(s_t, a_t) = r (s_t, a_t) + \gamma  \, \mathbb{E}_{s_{t+1} \sim T(s_{t+1} | s_t, a_t)}\big[ \underset{a_{t+1}}{\text{max}} \,\,Q^*(s_{t+1}, a_{t+1})\big]
$$

The objective for deep Q-learning is to find an approximate Q-function that satisfies the above equation, where we represent our approximate Q-function, $Q_\phi$, with a neural network that has the parameters of $\phi$.
Subtracting the left hand and right hand side of the recursive Q-function definition, we can define the temporal difference error, $\mathcal{E}$, for a state-action pair as 

$$
  \mathcal{E} = r(s_t, a_t) + \gamma \, \underset{a}{\text{max}} \, Q_\phi(s_{t+1}, a) - Q_\phi(s_t, a_t).
$$

The goal is to find a $Q_\phi$ that sets the TD error to zero across all state-action pairs. 
The general process to achieve this objective is to sample trajectories from the environment, evaluate the TD Error across different state-action-reward transitions -- with samples indexed by $i$ --, and then perform the following gradient descent on the parameters of the Q-function neural network:

$$
  \phi_{t+1} = \phi_t - \alpha \nabla_\phi \, \sum_i \mathcal{E}_i^2 
$$

After the Q-function neural network or Q-network has been fitted, the agent will select actions by inputting the current state into the Q network and then identifying the state-action pair with the highest estimated Q value.
The benefit of using neural networks to approximate Q-functions is that neural networks are able to generalize Q functions over unseen state-action pairs, whereas classic methods like TD-learning are not able to generalize [@CITEmnihhumancontrol?].
But there a host of problems with getting neural-network-based value-learning agents to work in practice; see @mnihhumancontrol and @ddqn for more insight on these issues.   
A notable concern with these methods is that they tend to suffer from bias which is introduced by fitting a value network to a target that incorporates an estimated value function [@ddqn]. 
In review, a sketch of simple deep Q learning algorithm is shown in ALGORITHM.

## Actor-Critic
Actor-critic algorithms integrate the main components from both value-learning and policy gradient based methods.
The "actor" attempts to learn the policy, and the "critic" attempts to learn a value function.
While the general algorithm for the critic is exactly the same as mentioned in the value-learning section, the actor differs from policy gradient algorithms by incorporating the critic in its gradient ascent step.
Instead of estimating the gradient of the return as in eq ..., the gradient of the return is estimated as 

$$
  \nabla_\theta J(\pi_\theta) \approx \sum_{i,t} \nabla_\theta \log{\pi_\theta (a_{i,t} \mid s_{i,t})} \, \hat{Q}(s_{i,t}, a_{i, t}).
$$

where $\hat{Q}$ is a value function estimated by the critic.
The benefit of actor-critic algorithms is that they can balance the issues with bias and variance that are common with value-learning and policy gradient methods respectively [@suttonbarto].
Consequently, actor-critic algorithms have achieved many of the state of the art results in model-free deep RL [@SAC; @TD3].  
A simple sketch of an actor critic algorithm is shown in ALGORITHM.

## Exploration 

Exploration is a central component of any deep RL algorithm, and there are a variety of exploration algorithms that an agent can employ.
In the sections above, we procedurally mentioned that the agent must interact with the environment and then use its experiences to maximize rewards.
Yet, how the agent explores is very important because if an agent fails to explore the state-action space well, then the agent will ultimately learn a bad policy.
In this section, we will go over some common exploration algorithms and try to provide some practical insight. 

### Epsilon-Greedy Algorithm

The Epsilon-Greedy algorithm takes a very simple approach towards balancing exploration and exploitation. 
For some $\epsilon \in (0, 1)$ and typically close to 0, an Epsilon-Greedy agent exploits the best available action with probability $1-\epsilon$ and explores a random action with probability $\epsilon$.
While simplistic, this algorithm has been effective on a range of problems notably @mnihhumanlevelcontrol.

### Boltzmann Exploration

For algorithms that learn a value function, agents can select actions by sampling from a Boltzmann or Softmax distribution that is constructed over the value function.
So if the agent is doing Q-learning, the probability for the agent to select the action, $a \in \mathcal A$, would be:

$$
P(a) = \frac{e^{\frac{Q(s_t, a)}{\tau}}}{\sum_{a'} e^{\frac{Q(s_t, a')}{\tau}}}
$$
where $\tau \in (0, \infty)$ is called the temperature parameter and controls how much the agent will weight high Q scores over low Q scores.
The advantage of using Boltzmann exploration is that since the probability of selecting an action is proportional to how good the agent thinks that action is, the agent can explore the space more efficiently than with Epsilon-Greedy exploration.
This is because an Epsilon-Greedy agent will explore all actions equally including low reward yielding actions; in contrast, with Boltzmann exploration, an agent will spend less time on these unpromising actions.

### Noise-Based Exploration

Injecting noise into an agent's action, observation or parameter space is another effective exploration method.
With noise addition, there are a lot design choices that can be made.
For instance, noise can be sampled from different distributions [@parameter_space], scaled adaptively based on the policy[@parameter_space], or the noise can be sampled according to the state that the agent observes [@gsde].
There are a variety of reasons why one would want to use these variations of noise injection, which are well discussed in @parameter_space and @gsde.

### Entropy-based Exploration

An increasingly common way to achieve exploration is by adding a function of the entropy term, $H(\pi(a|s))$, to the loss function of the policy network.
While this introduces some bias as the agent no longer exclusively maximizes cumulative rewards, entropy-based exploration has been effective in practice at improving exploration, resulting in better agent performance.
@understandingtheimpactofentropy gives a very complete discussion on the benefits of using entropy.

There is a multitude of exploration algorithms that we have not touched on, so it is important to say that the algorithms mentionned above are generally suited for dense reward or "easy-exploration" problems.
For "hard-exploration" problems -- for instance, environments that have very few state-action pairs with significant rewards, like a maze, are seen as hard-exploration problems --, more intricate and bespoke exploration algorithms tend to outperform.
We will not go over these algorithms for the sake of brevity but a good example of an algorithm for hard-exploration problems is @go-explore.







----

